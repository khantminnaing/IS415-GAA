---
title: "In-Class Exercise 09"
description: |
 Geographically Weighted Predictive Models
author:
  - name: Khant Min Naing
    url: https://www.linkedin.com/in/khantminnaing/
date: 03-18-2024
date-modified: "last-modified"
categories:
  - Hands-On Exercise
  - R
  - sf
  - GWmodel
  - SpatialML
output:
  distill::distill_article:
    code_folding: false
    toc: true
    self_contained: false
---

# Geographical Weighted Predictive Models

## 1.0 Importing Packages

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML, 
               tmap, rsample, tidymodels, tidyverse,
               gtsummary, rpart, rpart.plot, ggstatsplot,
               performance)
```

::: {.callout-tip title="Reflection"}
`SpaialML` package focuses only on random forest models and not other ML algorithms.

`tidymodels` framework is a collection of packages for modeling and machine learning such as `recipes` for data pre-processing, `tune` for tuning models, `yardstick` for evaluating model performance, among others.

`rpart` and `rpart.plot` packages will be used to demonstrate recursive partitioning, a fundamental concept behind random forest modelling.
:::

## 2.0 Importing Datasets

We will read the datasets into R environment.

```{r}
rs_sf <- read_rds("../data/rds/HDB_resale.rds")
```

Next, the code chunk below is used tor reveal the properties of `rs_sf` object.

```{r}
rs_sf
```

::: {.callout-tip title="Reflection"}
`rs_sf` is in simple feature data frame format and has a total of 15901 observations with 18 variables.
:::

## 3.0 Data Sampling

```{r}
set.seed(1234)
resale_split <- initial_split(
  rs_sf,
  prop = 5/10,)
train_sf <- training(resale_split)
test_sf <- testing(resale_split)
```

::: {.callout-tip title="Reflection"}
`set.seed(1234)` sets the seed of R's random number generator, which is useful for creating simulations or random objects that can be reproduced.

`initial_split` function from the `rsample` package is used to split the `rs_sf` dataset into two parts. The prop = 5/10 argument indicates that the data is being split into two equal halves, with 50% of the data going into each set.

`initial_split` function use random sampling approach. `rsample` package can be used for other sampling patterns such as stratified sampling, group sampling and time-based sampling. More details can be read here: https://rsample.tidymodels.org/articles/Common_Patterns.html
:::

We will save the `train_sf` and `test_sf` into rds for later use.

```{r}
write_rds(train_sf, "../data/rds/train_sf.rds")
write_rds(test_sf, "../data/rds/test_sf.rds")
train_sf <- read_rds("../data/rds/train_sf.rds")
test_sf <- read_rds("../data/rds/test_sf.rds")
```

```{r}
train_df <- train_sf %>%
  st_drop_geometry() %>%
  as.data.frame()

test_df <- test_sf %>%
  st_drop_geometry() %>%
  as.data.frame()
```

::: {.callout-tip title="Reflection"}
It is important to check which data class is compatible for the chosen analysis. Not all machine learning algorithms can handle all types of data. For example, many algorithms cannot process spatial data directly, which is why we dropped the geometry column. Also some algorithms cannot handle `tibble` data type and hence we convert it to data.frame.
:::

## 4.0 Computing Correlation Matrix

To check for multicollinearity, we compute a correlation matrix using the ggcorrmat() function from the ggstatsplot package. This function creates a correlation matrix plot, which is a graphical representation of the correlation matrix. This correlation matrix will give us a visual overview of how the predictors in our dataset are related to each other. If we see high correlation coefficients (close to 1 or -1), we may need to address multicollinearity before proceeding with our analysis.

```{r}
#| fig-width: 10
#| fig-height: 10
ggstatsplot::ggcorrmat(
  data = train_df[, 2:17],
  matrix.type = "upper",
  type = "parametric",
  tr = 0.2,
  partial = FALSE,
  k = 2L,
  sig.level = 0.05,
  conf.level = 0.95,
  bf.prior = 0.707,
  ggcorrplot.args = list(
     tl.cex = 10,
     pch.cex = 5,
     lab_size = 3
  )) + 
  ggplot2::theme(
    axis.text.x = ggplot2::element_text(
      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = "cm")
    )
  )
```

::: {.callout-tip title="Reflection"}
The correlation matrix above shows that all the correlation values are below 0.65. Hence, there is no sign of multicollinearity.
:::

## 5.0 Building and Fitting MLR Model

In this section, we will be building a non-spatial multiple linear regression model. This type of model is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal is to model the relationship between the explanatory and response variables.

```{r}
rs_mlr <- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +
         STOREY_ORDER + REMAINING_LEASE_MTHS +
         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
         PROX_MRT + PROX_PARK + PROX_MALL + PROX_CHAS +
         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
         WITHIN_1KM_PRISCH,
         data=train_df)
summary(rs_mlr)
```

```{r}
tbl_regression(rs_mlr,
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

## 6.0 Revising MLR Model

From the results in previous section, `PROX_CHAS` is not statistically significant. Hence, we will remove the `PROX_CHAS` column from the training and testing sets.

```{r}
train_df <- train_df %>%
  select(-c(PROX_CHAS))
train_sf <- train_sf %>%
  select(-c(PROX_CHAS))
test_df <- test_df %>%
  select(-c(PROX_CHAS))
test_sf <- test_sf %>%
  select(-c(PROX_CHAS))
```

After removal, we will re-run the MLR model again.

```{r}
rs_mlr <- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +
         STOREY_ORDER + REMAINING_LEASE_MTHS +
         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
         PROX_MRT + PROX_PARK + PROX_MALL + 
         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
         WITHIN_1KM_PRISCH,
         data=train_df)
summary(rs_mlr)
```

```{r}
class(rs_mlr)
```

::: {.callout-tip title="Reflection"}
The MLR model we fitted returns an object of class `lm`. The generic accessor functions `coefficients`, `effects`, `fitted.values` and `residuals` extract various useful features of the `lm` class object.
:::

## 7.0 Extracting Coordinates Data

In this step, we extract the X and Y coordinates from the full, training, and test datasets. The `st_coordinates()` function from the `sf` package is used to perform this extraction.

```{r}
coords <- st_coordinates(rs_sf)
coords_train <- st_coordinates(train_sf)
coords_test <- st_coordinates(test_sf)
```

::: {.callout-tip title="Reflection"}
If we inspect the documentation (https://search.r-project.org/CRAN/refmans/SpatialML/html/grf.bw.html) for bandwidth calculation of geographical random forest using `grf.bw()`, it expects a separate input called `coords`, which represent a numeric matrix or data frame of two columns giving the X,Y coordinates of the observations. These coordinates values are used to calculate spatial weight matrix.

In `GWmodel` package for GWR modelling, we just have to convert it to `sp` object and the algorithms automatically extract the coordinates, eliminating the need for manual input.
:::

## 8.0 Recursive Partitioning

```{r}
set.seed(1234)
rs_rp <- rpart(
  formula = RESALE_PRICE ~ FLOOR_AREA_SQM +
         STOREY_ORDER + REMAINING_LEASE_MTHS +
         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
         PROX_MRT + PROX_PARK + PROX_MALL + 
         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
         WITHIN_1KM_PRISCH,
  data = train_df)
rs_rp
```

To visualise how recursive partitioning works, we will pass the model to `rpart.plot()` function.

```{r}
rpart.plot(rs_rp)
```

::: {.callout-tip title="Reflection"}
Recursive partitioning can use the same variables more than once in different parts of the tree. This capability can uncover complex interdependencies between sets of variables. That is why, we see variables such as `PROX_CBD` and `REMAINING_LEASE_MTHS` in different levels of the trees.
:::

## 9.0 Non-Spatial Random Forest Modelling

```{r}
#| eval: false
set.seed(1234)
rs_rf <- ranger(
  RESALE_PRICE  ~ FLOOR_AREA_SQM +
                  STOREY_ORDER + 
               REMAINING_LEASE_MTHS +
                  PROX_CBD + 
               PROX_ELDERLYCARE + 
               PROX_HAWKER +
                  PROX_MRT + 
               PROX_PARK + 
               PROX_MALL + 
                  PROX_SUPERMARKET + 
               WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + 
               WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_df,
  importance = "impurity")

write_rds(rs_rf, "../data/models/rs_rf.rds")
```

::: {.callout-tip title="Reflection"}
The `ranger` package in R is a fast implementation of Random Forests, particularly suited to high dimensional data. By using ranger as a base, we can take advantage of its speed and functionality when calibrating your models.

When it comes to spatial analysis, packages like `SpatialML` use `ranger` as a dependency. This means that the functions in `SpatialML` are built upon the functions in ranger. Using `ranger` as a base model allows for seamless integration and readily calibration of base model to a spatial model.
:::

-   The "impurity" measure is the Gini index for classification, the variance of the responses for regression and the sum of statistics for survival.

```{r}
rs_rf <- read_rds("../data/models/rs_rf.rds")
rs_rf
```

## 10.0 Variable Importance

Variable importance or feature importance scores are indicative of how "important" the variable is to our model.

By using `impurity` importance argument in our ranger function earlier, `rs_rf` has contains the generated variable.importance. Now we will extract variable.importance and save it into vi.

```{r}
vi <- as.data.frame(rs_rf$variable.importance)
vi$variables <- rownames(vi)
vi <- vi%>%
  rename(vi = "rs_rf$variable.importance")
```

Plot the graph.
```{r}
ggplot(data = vi,
       aes(x = vi,
           y = reorder(variables, vi),
       fill = variables)) +
  geom_bar(stat="identity", show.legend = FALSE)
```
::: {.callout-tip title="Reflection"}
`reorder(variables, vi)` function is used to reorder the levels of variables based on the values of `vi`, so that the bars will be sorted in the plot. 

Since there is no sign of quasi-complete or complete separation, we can proceed to calibrate our bandwidth.
:::

## 11.0 Fitting Geographical Random Forest

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + 
                       STOREY_ORDER +
                       REMAINING_LEASE_MTHS + 
                       PROX_CBD + 
                       PROX_ELDERLYCARE +
                       PROX_HAWKER + 
                       PROX_MRT + 
                       PROX_PARK + 
                       PROX_MALL +
                       PROX_SUPERMARKET + 
                       WITHIN_350M_KINDERGARTEN +
                       WITHIN_350M_CHILDCARE + 
                       WITHIN_350M_BUS +
                       WITHIN_1KM_PRISCH,
                     dframe=train_df, 
                     bw = 55,
                     step = 1,
                     nthreads = 16,
                     forest = FALSE,
                     weighted = TRUE,
                     kernel="adaptive",
                     coords=coords_train)
```

```{r}
#| eval: false
rs_grf <- read_rds("../data/models/rs_grf.rds")
```

```{r}
test_df <- cbind(test_sf, coords_test) %>%
  st_drop_geometry()
```

## 12.0 Predicting with test data

Next, predict.grf() of spatialML will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
grf_pred <- read_rds("../data/models/grf_pred.rds")
grf_pred_df <- as.data.frame(grf_pred)
```

```{r}
test_pred <- test_df %>%
  select(RESALE_PRICE) %>%
  cbind(grf_pred_df)
```

```{r}
rf_pred <- predict(rs_rf, test_df)
rf_pred_df <- as.data.frame(rf_pred$predictions) %>% rename(rf_pred = "rf_pred$predictions")
```

```{r}
mlr_pred <- predict(rs_mlr, test_df)
mlr_pred_df <- as.data.frame(mlr_pred) %>% rename(mlr_pred = "mlr_pred")
```

```{r}
test_pred <- cbind(test_pred,rf_pred_df)
test_pred <- cbind(test_pred,mlr_pred_df)
```

## 13.0 Performance Evaluation with RMSE

```{r}
yardstick::rmse(test_pred,
                RESALE_PRICE,
                grf_pred)
```

```{r}
yardstick::rmse(test_pred,
                RESALE_PRICE,
                rf_pred)
```

```{r}
yardstick::rmse(test_pred,
                RESALE_PRICE,
                mlr_pred)
```

```{r}
mc <- test_pred %>%
  pivot_longer(cols = c(2:4),
               names_to = "models",
               values_to = "predicted")
```

```{r}
```


Finally, we can visualize the actual resale price and the predicted resale price using a scatterplot. This can help us understand how well our model is performing.


```{r}
ggplot(data = mc,
       aes(x = predicted,
           y = RESALE_PRICE)) +
  geom_point()
```
