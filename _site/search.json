[
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#overview",
    "href": "In-class_Ex/In-class_Ex02.html#overview",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "title": "In-Class Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow for reading and writing Apache Parquet files\nlubridate for tackling with temporal data (dates and times)\ntidyverse for manipulating and wrangling data, as well as, implementing data science functions\ntmap for creating and visualizing thematic maps\nsf for handling geospatial data.\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "href": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "title": "In-Class Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia.\n\nApart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\n\n\n3.1 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format.\n\nFirstly, we will use read_parquet function from arrow package\n\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet')\ndf_1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet')\n\n\nNext, we will use head() function to quickly scan through the data columns and values.\n\n\nhead(df)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)\n\n\n\n3.2 Extracting Trip Starting Locations and Temporal Data Values\nAfter loading the Grab-Posisi dataset, we will extract features that we want to use for analysis. Firstly, we will extract trip starting locations for all trajectories in the dataset and save it into a new df called origin_df.\nAlso, we are interested to derive useful temporal data such as day of the week, hour, and yy-mm-dd. To do so, we will use the following functions from lubridate package, and add the newly derived values as new columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n3.3 Extracting Trip Ending Locations and Temporal Data Values\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, for destination_df, we use arrange(desc()) argument to sort the timestamps in descending order\n\n\n\n\n3.4 Saving R Objects in RDS Format\nRDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow us to preserve the state of an object between R sessions. Saving R object as an RDS file in R can be useful for sharing our work with others, replicating our analysis, or simply storing our work for later use.\n\nwrite_rds(origin_df, \"../data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"../data/rds/destination_df.rds\")\n\n\n\n3.4 Importing RDS Objects\n\norigin_df &lt;- read_rds(\"../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../IS415-GAA/data/rds/destination_df.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHands-On Exercise 01\n\n\n\n\n\nGeospatial Data Wrangling with R!\n\n\n\n\n\n\nJan 7, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 02\n\n\n\n\n\nThematic Mapping and GeoVisualisation with R\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 03\n\n\n\n\n\nSpatial Point Pattern Analysis\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 08\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nJan 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 01\n\n\n\n\n\nApplication of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html",
    "href": "Hands-on_Ex/hands_on02.html",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#overview",
    "href": "Hands-on_Ex/hands_on02.html#overview",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-packages",
    "href": "Hands-on_Ex/hands_on02.html#importing-packages",
    "title": "Hands-On Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nSince readr, tidyr and dplyr are part of tidyverse package, we will only need to install and import tidyverse.\n\npacman::p_load(sf, tmap, tidyverse)\ndevtools::install_github(\"thomasp85/patchwork\")\n\nSkipping install of 'patchwork' from a github remote, the SHA1 (d9437579) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(patchwork)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "href": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "title": "Hands-On Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use two datasets as follows:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\nFor geospatial data, we will use st_read() function of sf package to import shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"~/IS415-GAA/data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "title": "Hands-On Exercise 02",
    "section": "4.0 Data Preparation and Wrangling",
    "text": "4.0 Data Preparation and Wrangling\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n4.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n4.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nHence, we will standard the data values in these two fields.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 02",
    "section": "5.0 Choropleth Mapping Geospatial Data Using tmap",
    "text": "5.0 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n5.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\",\n    fill.palette =\"Plasma\")\n\n\n\n\n\n\n5.2 Plotting a choropleth map quickly by using qtm()\nHowever, in real-life application, the quick choropleth map produced in the previous session may not be sufficient enough to properly visualize geospatial data. However, tmap packages allow us to customise and control how we design our choropleth maps. We will exploit tmap’s drawing elements to create a high quality cartographic choropleth map that includes more accurate and informative information.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Plasma\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nNext, we will breakdown the different tmpa functions used to plot the additional elements in the map above.\n\n\n5.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e.DEPENDENCY)\n\n\n\n\n\n5.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n5.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "title": "Hands-On Exercise 02",
    "section": "6.0 Data Classification Methods of tmap",
    "text": "6.0 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher, and\njenks.\n\n\n6.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNext, we will try equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nNext, we will try other data classification methods.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"Plasma\",\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\nAlso, we can try exploring using the same classification methods, but with different numbers of classes. As an example, we will use kmeans clustering method with different class sizes (2,6,10,20)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          palette = \"Plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          palette = \"Plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          palette = \"Plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          palette = \"Plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\noriginal &lt;- ggplot(data=mpsz_pop2020, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\n#Try to remove outliers\nmpsz_pop2020_no_outlier &lt;- subset(mpsz_pop2020, mpsz_pop2020$DEPENDENCY &lt;3)\n\nfiltered &lt;- ggplot(data=mpsz_pop2020_no_outlier, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\noriginal + filtered\n\n\n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 1.00. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we use the above-mentioned breaks?\nThe reason behind choosing those break points is mainly stemmed from the 1st quantile and 3rd quantile of the datasets. While the minimum value is 0.10 and maximum value is 19.0, the 1st quantile (the value under which 25% of data points are found) is 0.7147 and the 3rd quantile (the value under which 75% of data points are found) is 0.8763. Using these two values, we may assume that the dataset might have outliers on the right end, and the majority of the dataset might be scattered in the range of 0.7147 and 0.8763. Hence, we use the mentioned break points.\nOtherwise, we can use non-heuristic approach in this case as well. We can easily plot the data to see the distribution first (see above). As we assumed earlier, you can clearly see the outliers on the right-side of the histogram. After removing the outliers (temporarily), we can see the new plot (see above). Majority of the datasets are scattered within the range of 0.6 - 1.0. This is why we break the datasets into 0.6, 0.7, 0.8 and 0.9 respectively so that there is balanced quantity of data points in each break.\n\n\nUsing this information, we will now proceed to plot the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"Plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Sunset\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "href": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "title": "Hands-On Exercise 02",
    "section": "7.0 Controlling and Customizing Map Layots",
    "text": "7.0 Controlling and Customizing Map Layots\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"DarkSunset\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Sunset\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n7.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Sunset\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-On Exercise 02",
    "section": "8.0 Drawing Small Multiple Choropleth Maps",
    "text": "8.0 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Purp\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Burg\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-On Exercise 02",
    "section": "9.0 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "9.0 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\nFor example, we have select the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#references",
    "href": "Hands-on_Ex/hands_on02.html#references",
    "title": "Hands-On Exercise 02",
    "section": "10. References",
    "text": "10. References\nTutorial provided by Professor Kam Tin Seong©, Singapore Management University\nReference: https://r4gdsa.netlify.app/chap02.html\n\n10.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n10.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n10.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html",
    "href": "Hands-on_Ex/hands_on1.html",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#overview",
    "href": "Hands-on_Ex/hands_on1.html#overview",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#setup",
    "href": "Hands-on_Ex/hands_on1.html#setup",
    "title": "Hands-On Exercise 01",
    "section": "2.0 Setup",
    "text": "2.0 Setup\n\n2.1 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, I extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n\n2.2 Installing R-Packages\nIn this exercise, I will be using two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nI install the required packages using the code chunk below.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "3.0 Importing Geospatial Data into R",
    "text": "3.0 Importing Geospatial Data into R\nIn this section, I will import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n3.1 Importing polygon feature data in shapefile format\nDataset used: MP14_SUBZONE_WEB_PL File format: shapefile Data frame type: polygon feature\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our mpsz simple feature data frame, there are 323 multipolygon features, 15 fields and is in the svy21 projected coordinates system.\n\n\n3.2 Importing polyline feature data in shapefile form\nDataset used: CyclingPathGazette File format: shapefile Data frame type: line feature\n\ncyclingpath = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                      layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our cyclingpath linestring feature data frame, there are 1625 linestring features, 2 fields and is in the svy21 projected coordinates system.\n\n\n3.3 Importing GIS data in kml format\nDataset used: pre-schools-location-kml File format: kml Data frame type: point feature\n\npreschool = st_read(\"~/IS415-GAA/data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we can see that in our preschool point feature data frame, there are 1359 linestring features, 2 fields and is in the wgs84 projected coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "4.0 Importing + Converting Aspatial Data into R",
    "text": "4.0 Importing + Converting Aspatial Data into R\nFor aspatial data, such as the listings Airbnb datset, there’s an extra step in the importing process. We’ll import it into a tibble data frame, then convert it into a simple feature data frame.\n\n4.1 Importing aspatial data\nSince our listings data set is in a csv file format, we’ll use the read_csv() function from the readr package, like so:\n\nlistings &lt;- read_csv(\"~/IS415-GAA/data/aspatial/listings.csv\")\n\nRows: 3457 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(listings) \n\nRows: 3,457\nColumns: 18\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ latitude                       &lt;dbl&gt; 1.34537, 1.34754, 1.34531, 1.29015, 1.2…\n$ longitude                      &lt;dbl&gt; 103.9589, 103.9596, 103.9610, 103.8081,…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n\n\nFrom the output message, we can see that in our listing tibble data frame, there are 4252 rows and 16 columns (not features and fields like in our simple data feature frame!) Take note of the latitude and longitude fields - we’ll be using them in the next phase.\n\nAssumption: The data is in the wgs84 Geographic Coordinate System on account of its latitude/longtitude fields.\n\n\n\n4.2 Converting aspatial data\nNow, let’s convert our listing tibble data frame into a by using the st_as_sf() function from the sf package.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThis gives us the new simple feature data frame, listings_sf:\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\n\nNote that a new column called geometry has been added! In addition, longtitude and latitude have both been dropped."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-On Exercise 01",
    "section": "5.0 Checking the Content of A Simple Feature Data Frame",
    "text": "5.0 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "title": "Hands-On Exercise 01",
    "section": "6.0 Plotting the Geospatial Data",
    "text": "6.0 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. I use plot() to quickly plot a sf object as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "href": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "title": "Hands-On Exercise 01",
    "section": "7.0 Working with Projection",
    "text": "7.0 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, I project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nTo check the coordinate system of mpsz simple feature data frame, I use st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in SVY21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for SVY21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nI take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (25089.46 31299.16 0)\n\n\nPOINT Z (27189.07 32792.54 0)\n\n\nPOINT Z (28844.56 36773.76 0)\n\n\nPOINT Z (24821.92 46303.16 0)\n\n\nPOINT Z (28637.82 35038.49 0)\n\n\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "title": "Hands-On Exercise 01",
    "section": "8.0 Geoprocessing with sf package",
    "text": "8.0 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n8.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\n8.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNext, I calculate the density of pre-school by planning subzone.\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 01",
    "section": "9.0 Exploratory Data Analysis (EDA)",
    "text": "9.0 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\nUsing ggplot2 method, I plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html",
    "href": "Hands-on_Ex/hands_on08.html",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#overview",
    "href": "Hands-on_Ex/hands_on08.html#overview",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "href": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "title": "Hands-On Exercise 08",
    "section": "2.0 Importing Datasets and Packages",
    "text": "2.0 Importing Datasets and Packages\nFirstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "href": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "title": "Hands-On Exercise 08",
    "section": "3.0 Data Wrangling",
    "text": "3.0 Data Wrangling\n\n3.1 Geospatial Data Wrangling\nWe use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n3.1 Aspatial Data Wrangling\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 08",
    "section": "4.0 Exploratory Data Analysis (EDA)",
    "text": "4.0 Exploratory Data Analysis (EDA)\n\n4.1 EDA Using Statistical Graphics\nWe can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n4.2 EDA Using Multiple Histogram Plots Distribution of Variables\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n4.3 Drawing Statistical Point Map\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 08",
    "section": "5.0 Hedonic Pricing Modelling in R",
    "text": "5.0 Hedonic Pricing Modelling in R\nIn this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n5.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n5.2 Multiple Linear Regression Method\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n5.3 Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\n\n5.4 Checking for Multicollinearity\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n5.5 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n5.6 Test for Normality Assumption\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the Kolmogorov-Smirnov test\n\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n5.7 Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "href": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "title": "Hands-On Exercise 08",
    "section": "6.0 Building Hedonic Pricing Model using GWmodel",
    "text": "6.0 Building Hedonic Pricing Model using GWmodel\nAfter exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n6.1 Computing Bandwidth\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n6.2 Building Basic GWModel with Fixed and Adaptive Bandwidth\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-01-17 14:25:47.109064 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-01-17 14:25:48.670706 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-01-17 14:25:48.671437 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-01-17 14:25:50.329209 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n6.3 Visualisaing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n{= 1) +}   tm_view(set.zoom.limits = c(11,14))\nNext, we will visualise the coefficient estimates\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=\"plasma\",\n          size = 0.3,\n          alpha = 0.5)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=\"plasma\",\n          size = 0.3,\n          alpha = 0.5)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I am Khant Min Naing, an undergraduate student at Singapore Management University. This course website is developed to document my learning journey at IS415: Geospatial Analytics and Applications under Professor Kam Tin Seong. Follow my journey as I venture into the world of big data, geospatial analysis and urban planning.\nConnect with me!"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html",
    "href": "Hands-on_Ex/hands_on03.html",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This hands-on exercise will explore the basic concepts and methods of Spatial Point Pattern Analysis.\nParticularly, we will explore using spatstat, an R package specially designed for Spatial Point Pattern Analysis.\n\n\nToday, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?\n\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, devtools,sp)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"~/IS415-GAA/data/aspatial/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  2290 points\nAverage intensity 2.875673e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    3    4    1    7    7    1    1    1    2    1    1    1    1    2 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    4    1    1    1    1    1    5    1    2    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1    2    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   5    1    1    2    1    1    1    1    1    1    1    2    1    1    1    4 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1   10    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    4    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1   10 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n  10   10    1    1    1    1    1    1    1    1    1    1    1    1    3    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    2    1   10    1    1    1    1    1    1    2    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    3    1    1    1    1    1    3    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    2    1    1    3    1    1    1    2    1    2    2    2    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    1    1    2    1    1    1    1    1    2    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    2    1    1    1    1    1    1    3    1    1    1    4    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    1    1    1    1    1    1    1    1    1    2    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    3    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    2    7    1    3    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   2    1    1    1    1    1    1    1    3    2    1    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    2    1    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    1    1    1    1    1    2    3    2    1    2    1    1    1    1    5 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    3    1    1    1    1    1    1    5    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    4    1    1    1    1    1    1    1    1    3    1    1    2 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    2    1    1    1    1    1    1    3    1    1    1    1    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    2    1    1    1    1    1    2    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1   10    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   2    1    1    1    2    1    3    1    1    1    1    1    1    1    1    2 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   2    1    1    3    1    1    1    1    1    1    3    1    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    3    1    1    1    3    1    3    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    2    2    2    1    1    2    3    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    2    1    1    1    1    3    1    1    3    1    1    1    1    2 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    2    1    2    1   10    1    4    2    2    1    1    1    1    4    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    3    1    1    1    1    4    1    2    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    3    1    1    1    1    1    2    1    1    1    2    2    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    4    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   2    1    1    1    1    1    1    1    1    1    1    1   10    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    3    3    3    3    1    1    1    1    1    1    1    3    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   3    1    1    1    1    1    1    1    1    3    1    3    1    1    1    3 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   2    2    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   2    1    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    4    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    3    1    3    3    3    3    1    1    1    1    3    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    3    1    2    1    1    1    1    1    3    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   3    1    3    1    3    1    1    1    1    1    1    1    1    1    2    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    2    3    1    1    1    1    1   10    1    2    4    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    4    1    7    1    1    1    1    3    1    1    1    1    1    3 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   3    1    1    1    1    3    1    1    1    3    1    3    1    1    1    3 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   3    1    1    1    1    2    1    1    1    1    3    1    1    3    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    3    3    1    1    3    1    2    1    3    1    3    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    3    1    1    1    1    3    1    1    1    1    3    1    3 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    1    1    3    1    1    3    1    1    1    1    2    1    1    1    3 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    3    1    1    1    1    1    1    3    3    3    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    2    1    1    3    1    1    1    1    1    1    1    3    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   3    3    3    3    3    1    1    1    3    1    4    3    1    3    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   3    4    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    4    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    3    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    3    1    1    1    1    1    1    1   10    1    1    1    1    1    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    3    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    2    2    3    1    1    1    7    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    1    1    1    2    1    1    1    1    4    2    3    2    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    2    2    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    3    3    2    2    2    2    2    2    2    2    2    3    3    3 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    2    3    2    3    2    3    2    2    2    2    2    2    2    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    2    1    1    1    1    1    1    3    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    2    1    1    1    1    5    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    7    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   3    1    1    5    1    3    2    3    3    3    3    2    2    4    3    2 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   2    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    1    5    1    1    3    1    1    1    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    2 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   2    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   2    2    2    3    2    2    2    2    2    2    2    4    2    2    2    2 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   2    4    3    2    2    2    2    3    2    2    2    2    2    2    2    2 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   2    2    4    2    2    2    2    2    2    2    1    2    2    2    2    2 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   3    2    2    2    2    2    2    2    2    2    3    2    2    2    2    2 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   2    2    2    2    2    2    2    5    2    2    2    7    2    2    2    2 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   2    2    2    2    2    2    7    2    4    2    2    2    2    2    2    2 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   2    2    2    2    2    2    2    2    3    2    2    2    2    2    1    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   2    2    2    3    2    2    2    2    3    2    2    2    2    3    2    2 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    2    2    2    3    2    2    3    3    3    3    3    2    3    2    3 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   3    3    3    2    2    3    3    2    3    3    2    2    2    2    2    3 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   2    3    3    3    3    2    2    2    2    2    3    2    2    2    3    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   3    3    2    2    2    2    3    3    3    3    3    3    3    2    2    3 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   2    2    2    2    3    3    3    3    2    2    3    3    2    2    3    2 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    3    2    2    3    3    2    2    3    4    3    3    2    3    2    2 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   3    2    2    2    2    3    2    2    2    7    2    1    2    7    2    2 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   4    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 \n   2    2    2    2    2    2    5    2    2    2    2    2    4    1    1    1 \n1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 \n   1    1    1    1    1    1    1    1    1    3    3    3    3    1    3    1 \n1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 \n   3    3    3    3    3    3    3    3    3    3    3    3    3    1    3    3 \n1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 \n   3    3    3    3    3    3    3    3    3    3    3    1    1    3    3    3 \n2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 \n   2    3    3    3    3    3    4    3    2    3    3    3    3    3    3    3 \n2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 \n   3    3    3    3    3    3    3    3    2    3    3    3    2    2    2    2 \n2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 \n   3    2    2    2    2    2    2    2    4    2    2    2    2    1    2    2 \n2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 \n   2    2    2    2    2    3    2    3    2    2    2    3    3    2    2    2 \n2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 \n   3    2    3    2    2    2    2    2    2    3    2    2    3    2    2    2 \n2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 \n   2    2    3    2    2    2    2    2    2    2    2    2    3    2    2    2 \n2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 \n   2    2    2    2    2    2    4    2    7    2    2    2    1    2    2    2 \n2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 \n   2    1    2    2    2    2    2    7    2    4    2    2    2    2    2    2 \n2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 \n   2    2    2    2    2    2    2    2    2    3    2    2    2    2    2    2 \n2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 \n   1    2    2    2    2    3    2    2    3    1    2    2    2    2    2    2 \n2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 \n   2    2    1    3    2    2    2    3    2    2    2    2    2    2    2    2 \n2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 \n   2    2    2    2    2    4    2    7    2    7    2    2    4    2    2    2 \n2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 \n   2    2    3    2    2    2    2    2    2    2    2    2    2    4    2    2 \n2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 \n   2    2    2    2    3    2    2    2    2    2    2    2    2    2    2    2 \n2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 \n   2    2    2    2    2    4    2    2    2    2    2    2    2    2    2    4 \n2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 \n   2    2    2    2    2    2    2    3    3    2    2    2    2    2    2    2 \n2289 2290 \n   2    2 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 885\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  2290 points\nAverage intensity 3.156983e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)\n\n\n\n\n\n\n\n\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n281.8312 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.111666 1.347496 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.2109048 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf[mpsz_sf$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz_sf[mpsz_sf$PLN_AREA_N == \"TAMPINES\",]\nbp = mpsz_sf[mpsz_sf$PLN_AREA_N == \"BUKIT PANJANG\",]\njw = mpsz_sf[mpsz_sf$PLN_AREA_N == \"JURONG WEST\",]\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.41081, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.63429, p-value = 1.645e-07\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#overview",
    "href": "Hands-on_Ex/hands_on03.html#overview",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Today, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"~/IS415-GAA/data/aspatial/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "2.0 Literature Review of Spatial Point Pattern Analysis",
    "text": "2.0 Literature Review of Spatial Point Pattern Analysis\nSpatial point pattern analysis is concerned with description, statistical characterization, modeling and visulisation of point patterns over space and making inference about the process that could have generated an observed pattern (Boots & Getis, 1988,Rey et al., 2023; Pebesma & Bivand, 2023). According to this theory, empirical spatial distribution of points in our daily life are not controlled by sampling, but a result of an underlying geographically-continuous process (Rey et al., 2023). For example, an COVID-19 cluster did not happen by chance, but due to a spatial process of close-contact infection.\nWhen analysing real-world spatial points, it is important to analyse whether the observed spatial points are randomly distributed or patterned due to a process or interaction (Floch et al., 2018). In “complete random” distribution, points are located everywhere with the same probability and independently of each other. On the other hand, the spatial points can be clustered or dispersed due to an underlying point process. However, it is challenging to use heuristic observation and intuitive interpretation to detect whether a spatial point pattern exists (Baddeley et al., 2015; Floch et al., 2018). Hence, spatial point pattern analysis can be used to detect the spatial concentration or dispersion phenomena.\n\nWhen analysing and interpreting the properties of a point pattern, these properties can be categorized into two: (a) first-order properties and (b) second-order properties (Yuan et al., 2020; Gimond, 2023). First-order properties concern with the characteristics of individual point locations and their variations of their density across space (Gimond, 2023). Under this conception, observations vary from point to point due to changes in the underlying property. Second-order properties focus on not only individual points, but also the interactions between points and their influences on one another (Gimond, 2023). Under this conception, observations vary from place to place due to interaction effects between observations. First-order properties of point patterns are mostly addressed by density-based techniques, such as kernel density, whereas, distance-based techniques, such nearest neighbour index and K-functions, are often used to analyse second-order properties since they take into account the distance between point pairs (Yuan et al., 2020; Gimond, 2023)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "3.0 Importing Packages",
    "text": "3.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow : for reading and writing Apache Parquet files\nlubridate : for tackling with temporal data (dates and times)\nspatstat: A package for statistical analysis of spatial data, specifically Spatial Point Pattern Analysis. This package was provided by Baddeley, Turner and Ruback (2015) and gives a comprehensive list of functions to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nrgdal: Used to import geospatial data and output as spatial class objects from sp package\nraster : reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools, ggplot2, ggthemes, plotly: Packages used to plot interactive visualisations summary statistics and KDE layers\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf,st,spatstat)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "4.0 Importing Datasets into R Environment",
    "text": "4.0 Importing Datasets into R Environment\n\n4.1 Datasets\nIn this study, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia. Apart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\nMoreover, we will also use OpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nTo extract the Singapore boundary, we will use Master Plan 2019 Subzone Boundary (No Sea), provided by data.gov.sg.\n\n\n4.2 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format. The Singapore portion of the dataset is packaged into a total of 10 Parquet files.\nFirstly, we will use read_parquet function from arrow package, which allows us to read Parquet files into R environment as a data frame (more specifically, a tibble).\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet',as_data_frame = TRUE)\ndf1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet',as_data_frame = TRUE)\ndf2 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00002.snappy.parquet',as_data_frame = TRUE)\ndf3 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00003.snappy.parquet',as_data_frame = TRUE)\ndf4 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00004.snappy.parquet',as_data_frame = TRUE)\ndf5 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00005.snappy.parquet',as_data_frame = TRUE)\ndf6 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00006.snappy.parquet',as_data_frame = TRUE)\ndf7 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00007.snappy.parquet',as_data_frame = TRUE)\ndf8 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00008.snappy.parquet',as_data_frame = TRUE)\ndf9 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00009.snappy.parquet',as_data_frame = TRUE)\n\nTo consolidate all trajectory instances into a single dataframe, we’ll vertically bind all 10 imported dataframes using bind_rows() function from tidyverse package.\n\ndf_trajectories &lt;- bind_rows(df,df1,df2,df3,df4,df5,df6,df7,df8,df9)\n\nTo get a quick overview of the dataset, we’ll first check the number of trajectory instances using dim() function. Then, we’ll use head() function to quickly scan through the data columns and values\n\ndim(df_trajectories)\n\n[1] 30329685        9\n\nhead(df_trajectories)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 30329685 trajectory instances, each with a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf_trajectories$pingtimestamp &lt;- as_datetime(df_trajectories$pingtimestamp)\n\n\n\n4.3 Importing OpenStreetMap road data for Malaysia, Singapore and Brunei\nThe gis_osm_roads_free_1 dataset, which we downloaded from OpenStreetMap, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nosm_road_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"gis_osm_roads_free_1\") %&gt;% st_transform(crs = 3414)\n\nReading layer `gis_osm_roads_free_1' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 1764130 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 99.66041 ymin: 0.8021131 xmax: 119.2601 ymax: 7.514393\nGeodetic CRS:  WGS 84\n\n\nAfter importing the dataset, we will have a quick look at summary of osm_road_sf.\n\nsummary(osm_road_sf)\n\n    osm_id               code         fclass              name          \n Length:1764130     Min.   :5111   Length:1764130     Length:1764130    \n Class :character   1st Qu.:5122   Class :character   Class :character  \n Mode  :character   Median :5141   Mode  :character   Mode  :character  \n                    Mean   :5132                                        \n                    3rd Qu.:5141                                        \n                    Max.   :5199                                        \n     ref               oneway             maxspeed           layer        \n Length:1764130     Length:1764130     Min.   :  0.000   Min.   :-5.0000  \n Class :character   Class :character   1st Qu.:  0.000   1st Qu.: 0.0000  \n Mode  :character   Mode  :character   Median :  0.000   Median : 0.0000  \n                                       Mean   :  3.146   Mean   : 0.0299  \n                                       3rd Qu.:  0.000   3rd Qu.: 0.0000  \n                                       Max.   :110.000   Max.   : 5.0000  \n    bridge             tunnel                   geometry      \n Length:1764130     Length:1764130     LINESTRING   :1764130  \n Class :character   Class :character   epsg:3414    :      0  \n Mode  :character   Mode  :character   +proj=tmer...:      0  \n                                                              \n                                                              \n                                                              \n\n\nFrom the code output, we observed that there are a total of 1764130 observations. This is due to the fact that OpenStreetMap package we downloaded include data for not only Singapore, but also Malaysia and Brunei.\n\n\n4.4 Importing Singapore Master Plan Planning Subzone boundary data\nThe MP14_SUBZONE_WEB_PL dataset, which we downloaded from data.gov.sg, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we use %&gt;% operator is used to pipe the output of st_read() to the st_transform() function. Since the dataset we are using is the Singapore boundary, we need to assign the standard coordinate reference system for Singapore, which is SVY21 (EPSG:3414). st_transform() function transforms the coordinate reference system of the sf object to 3414.\n\n\nAfter importing the dataset, we will plot it to see how it looks. The plot() function is used to plot the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object.\n\nplot(st_geometry(mpsz_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01.html#references",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "References",
    "text": "References\n1. Rey, S.J., Arribas-Bel, D., Wolf, L.J.: Point Pattern Analysis. In: Geographic Data Science with python. pp. 185–219. CRC Press, Boca Raton etc. (2023)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "5.0 Data Wrangling",
    "text": "5.0 Data Wrangling\nData wrangling is the process of converting and transforming raw data into a usable form and is carried out prior to conducting any data analysis.\n\n5.1 Extracting Trip Starting Locations and Temporal Data Values from Grab-Posisi dataset\nFirstly, we will extract trip starting locations for all unqiue trajectories in the dataset and store them to a new df named origin_df. We are also interested in obtaining valuable temporal data such as the day of the week, the hour, and the date (yy-mm-dd). To do so, we will use the following functions from lubridate package, and add the newly derived values as columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n5.2 Extracting Trip Ending Locations and Temporal Data Values from Grab-Posisi dataset\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, this default order is applied to origin_df. However, for destination_df, the arrange(desc()) argument is used to modify the default order to descending.\n\n\n\n\n5.3 Converting to sf tibble data.frame\n\norigin_sf &lt;- st_as_sf(origin_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\ndestination_sf &lt;- st_as_sf(destination_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n5.4 Creating a CoastalLine of Singapore\nThe original mpsz_sf dataset we imported include information of all URA Master Plan planning area boundaries. However, for this analysis, we only need the national-level boundary of Singapore. Hence, we will need to union all the subzone boundaries to one single polygon boundary. Also, Grab ride-hailing service is only available on the main Singapore islands. Hence, we will need to remove outer islands which Grab service is not available. In particular, we will remove the following planning subzones: NORTH-EASTERN ISLANDS, SOUTHERN GROUP, SUDONG & SEMAKAU.\nWe can remove these subzones using the subset() function. The subset() function is used to extract rows from a data frame that meet certain conditions.\n\nnortheasten.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouthern.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\")\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\")\n\nouterislands &lt;- dplyr::bind_rows(list(northeasten.islands,southern.islands,sudong,semakau))\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we first created four new data frames called northeasten.islands, southern.islands, sudong, and semakau by selecting rows from mpsz_sf where the value in the SUBZONE_N column matches the corresponding value.\nAfter that, we used bind_rows() function from the dplyr package to combine these four data frames into a single data frame called outerislands.\n\n\nAfter importing the dataset, we will plot it to see how it looks.\n\nplot(st_geometry(outerislands))\n\n\n\n\nAs mentioned earlier, we only need to get national-level boundary of Singapore, without outer islands. To do so, we will need to process the mpsz_sf layer to achieve the outcome. - We will first use st_union() function from the sf package to combine the geometries of mpsz_sf and outerislands sf objects into a single geometry each. - Next, we will use st_difference() function then removes the overlapping areas between the two geometries. - Finally, we will store the non-overlapping areas into a new sf objected called sg_sf.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outerislands))\n\nTo assess whether the geometry of the newly created sg_sf matches our intended outcome, we will plot it out.\n\nplot(st_geometry(sg_sf))\n\n\n\n\n\n\n5.5 Extracting Road Layers within Singapore\nAs we have seen in Section 4.3., osm_road_sf dataset includes road networks from not only Singapore, but also Malaysia and Brunei. However, our analysis is focused on Singapore. Hence, we will need to remove unecessary data rows. To do so, we will\n\n#sg_road_sf &lt;- st_difference(osm_road_sf,sg_sf)\n\n\n#plot(sg_road_sf)\n\n\n\n5.5 Converting the Simple Features to Planar Point Pattern Object\nIn order to use the capabilities of spatstat packahe, a spatial dataset should be converted into an object of class planar point pattern ppp (Baddeley et al., 2015). A point pattern object contains the spatial coordinates of the points, the marks attached to the points (if any), the window in which the points were observed, and the name of the unit of length for the spatial coordinates. s. Thus, a single object of class ppp contains all the information required to perform spatial point pattern analysis.\nIn previous section, we have created sf objects of Grab trajectory origin and destination points. Now, we will convert them into ppp objects using as.ppp() function from spatstat package.\n\norigin_ppp &lt;- as.ppp(st_coordinates(origin_sf), st_bbox(origin_sf))\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe code chunk above converts the origin_sf object to a point pattern object of class ppp. st_coordinates() function is used to extract the coordinates of the origin_sf object and st_bbox() function is used to extract the bounding box of the origin_sf object. The resulting object origin_ppp is a point pattern object of class ppp.\n\n\n\ndestination_ppp &lt;- as.ppp(st_coordinates(destination_sf), st_bbox(destination_sf))\n\npar(mar = c(0,0,1,0))\nplot(destination_ppp)\n\n\n\n\n\n\n5.6 Handling Data Errors\nBefore going striaght into analysis, we will need to a quick look at the summary statistics of the newly created ppp objects. This is an important step to ensure that the data is free of errors and that a reliable analysis can be performed.\n\n5.6.1 Data Error Handling for origin_ppp\nWe will use summary() function to get summary information of origin_ppp object.\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\nWe can also check if there is any duplicated points in origin_ppp object using any(duplicated() function.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordaintes in the origin_ppp object.\n\nWhy do we need to check duplication?\nWhen analyzing spatial point processes, it is important to avoid duplication of points. This is because statistical methodology for spatial point processes is based largely on the assumption that processes are simple, i.e., that points of the process can never be coincident. When the data have coincident points, some statistical procedures designed for simple point processes will be severely affected (Baddeley et al., 2015).\n\n\n\n5.6.2 Data Error Handling for destination_ppp\nWe will use summary() function to get summary information of destination_ppp object.\n\nsummary(destination_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\nWe can also check if there is any duplicated points in destination_ppp object using any(duplicated() function.\n\nany(duplicated(destination_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordinates in the destination_ppp object.\n\n\n\n5.7 Creating Observation Windows\nMany data types in spatstat require us to specify the region of space inside which the data were observed. This is the observation window and it is represented by an object of class owin. In this analysis, our study area is Singapore, hence we will use Singapore boundary as the observation window for spatial point pattern analysis.\nIn Section 5.4, we have already created the sg_sf object, which represents the Singapore boundary (without outer islands). To convert this sf object to owin object, we will use as.owin() function from spatstat package.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot.owin(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of sg_owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459\n\n\n\n\n5.8 Combining ppp objects and owin object\nIn section 5.5, we have created two ppp objects - origin_ppp and destination_ppp, each representing the spatial points of Grab trajectory origin and destination. In section 5.7, we have created a owin object called sg_owin, which represent the observation window of our analysis.\nThe observation window sg_owin and the point pattern origin_ppp or destination_ppp can be combined, so that the custom window replaces the default ractangular extent (as seen in section 5.5).\n\norigin_ppp_sg = origin_ppp[sg_owin]\ndestination_ppp_sg = destination_ppp[sg_owin]\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp_sg)\n\n\n\nplot(destination_ppp_sg)\n\n\n\n\nWe will use summary() function to get summary information of the newly created origin_ppp_sg object and destination_ppp_sg object.\n\nsummary(origin_ppp_sg)\n\nPlanar point pattern:  28000 points\nAverage intensity 3.981136e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459\n\nsummary(destination_ppp_sg)\n\nPlanar point pattern:  27997 points\nAverage intensity 3.980709e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "6.0 Exploratory Spatial Data Analysis",
    "text": "6.0 Exploratory Spatial Data Analysis\n\n6.1 Visualising Frequency Distribution\n\nggplot(data=origin_df, \n       aes(x=weekday)) + \n  geom_bar()\n\n\n\nggplot(data=origin_df, \n       aes(x=day)) + \n  geom_bar()\n\n\n\n\n\n\n6.2 Creating Point Symbol Maps\n\ntmap_mode(\"view\")\ntm_shape(origin_sf) +\n    tm_dots(alpha=0.4, \n          size=0.05)\n\ntm_shape(destination_sf) +\n    tm_dots(alpha=0.4, \n          size=0.05)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-packages",
    "href": "Hands-on_Ex/hands_on03.html#importing-packages",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, devtools,sp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "href": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "After data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n281.8312 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.111666 1.347496 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.2109048 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "childcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  2290 points\nAverage intensity 2.875673e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    3    4    1    7    7    1    1    1    2    1    1    1    1    2 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    4    1    1    1    1    1    5    1    2    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1    2    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   5    1    1    2    1    1    1    1    1    1    1    2    1    1    1    4 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1   10    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    4    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1   10 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n  10   10    1    1    1    1    1    1    1    1    1    1    1    1    3    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    2    1   10    1    1    1    1    1    1    2    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    3    1    1    1    1    1    3    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    2    1    1    3    1    1    1    2    1    2    2    2    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    1    1    2    1    1    1    1    1    2    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    2    1    1    1    1    1    1    3    1    1    1    4    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    1    1    1    1    1    1    1    1    1    2    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    3    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    2    7    1    3    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   2    1    1    1    1    1    1    1    3    2    1    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    2    1    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    1    1    1    1    1    2    3    2    1    2    1    1    1    1    5 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    3    1    1    1    1    1    1    5    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    4    1    1    1    1    1    1    1    1    3    1    1    2 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    2    1    1    1    1    1    1    3    1    1    1    1    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    2    1    1    1    1    1    2    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1   10    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   2    1    1    1    2    1    3    1    1    1    1    1    1    1    1    2 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   2    1    1    3    1    1    1    1    1    1    3    1    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    3    1    1    1    3    1    3    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    2    2    2    1    1    2    3    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    2    1    1    1    1    3    1    1    3    1    1    1    1    2 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    2    1    2    1   10    1    4    2    2    1    1    1    1    4    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    3    1    1    1    1    4    1    2    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    3    1    1    1    1    1    2    1    1    1    2    2    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    4    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   2    1    1    1    1    1    1    1    1    1    1    1   10    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    3    3    3    3    1    1    1    1    1    1    1    3    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   3    1    1    1    1    1    1    1    1    3    1    3    1    1    1    3 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   2    2    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   2    1    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    4    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    3    1    3    3    3    3    1    1    1    1    3    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    3    1    2    1    1    1    1    1    3    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   3    1    3    1    3    1    1    1    1    1    1    1    1    1    2    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    2    3    1    1    1    1    1   10    1    2    4    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    4    1    7    1    1    1    1    3    1    1    1    1    1    3 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   3    1    1    1    1    3    1    1    1    3    1    3    1    1    1    3 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   3    1    1    1    1    2    1    1    1    1    3    1    1    3    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    3    3    1    1    3    1    2    1    3    1    3    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    3    1    1    1    1    3    1    1    1    1    3    1    3 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    1    1    3    1    1    3    1    1    1    1    2    1    1    1    3 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    3    1    1    1    1    1    1    3    3    3    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    2    1    1    3    1    1    1    1    1    1    1    3    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   3    3    3    3    3    1    1    1    3    1    4    3    1    3    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   3    4    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    4    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    3    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    3    1    1    1    1    1    1    1   10    1    1    1    1    1    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    3    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    2    2    3    1    1    1    7    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    1    1    1    2    1    1    1    1    4    2    3    2    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    2    2    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    3    3    2    2    2    2    2    2    2    2    2    3    3    3 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    2    3    2    3    2    3    2    2    2    2    2    2    2    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    2    1    1    1    1    1    1    3    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    2    1    1    1    1    5    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    7    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   3    1    1    5    1    3    2    3    3    3    3    2    2    4    3    2 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   2    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    1    5    1    1    3    1    1    1    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    2 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   2    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   2    2    2    3    2    2    2    2    2    2    2    4    2    2    2    2 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   2    4    3    2    2    2    2    3    2    2    2    2    2    2    2    2 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   2    2    4    2    2    2    2    2    2    2    1    2    2    2    2    2 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   3    2    2    2    2    2    2    2    2    2    3    2    2    2    2    2 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   2    2    2    2    2    2    2    5    2    2    2    7    2    2    2    2 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   2    2    2    2    2    2    7    2    4    2    2    2    2    2    2    2 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   2    2    2    2    2    2    2    2    3    2    2    2    2    2    1    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   2    2    2    3    2    2    2    2    3    2    2    2    2    3    2    2 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    2    2    2    3    2    2    3    3    3    3    3    2    3    2    3 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   3    3    3    2    2    3    3    2    3    3    2    2    2    2    2    3 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   2    3    3    3    3    2    2    2    2    2    3    2    2    2    3    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   3    3    2    2    2    2    3    3    3    3    3    3    3    2    2    3 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   2    2    2    2    3    3    3    3    2    2    3    3    2    2    3    2 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    3    2    2    3    3    2    2    3    4    3    3    2    3    2    2 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   3    2    2    2    2    3    2    2    2    7    2    1    2    7    2    2 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   4    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 \n   2    2    2    2    2    2    5    2    2    2    2    2    4    1    1    1 \n1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 \n   1    1    1    1    1    1    1    1    1    3    3    3    3    1    3    1 \n1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 \n   3    3    3    3    3    3    3    3    3    3    3    3    3    1    3    3 \n1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 \n   3    3    3    3    3    3    3    3    3    3    3    1    1    3    3    3 \n2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 \n   2    3    3    3    3    3    4    3    2    3    3    3    3    3    3    3 \n2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 \n   3    3    3    3    3    3    3    3    2    3    3    3    2    2    2    2 \n2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 \n   3    2    2    2    2    2    2    2    4    2    2    2    2    1    2    2 \n2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 \n   2    2    2    2    2    3    2    3    2    2    2    3    3    2    2    2 \n2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 \n   3    2    3    2    2    2    2    2    2    3    2    2    3    2    2    2 \n2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 \n   2    2    3    2    2    2    2    2    2    2    2    2    3    2    2    2 \n2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 \n   2    2    2    2    2    2    4    2    7    2    2    2    1    2    2    2 \n2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 \n   2    1    2    2    2    2    2    7    2    4    2    2    2    2    2    2 \n2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 \n   2    2    2    2    2    2    2    2    2    3    2    2    2    2    2    2 \n2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 \n   1    2    2    2    2    3    2    2    3    1    2    2    2    2    2    2 \n2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 \n   2    2    1    3    2    2    2    3    2    2    2    2    2    2    2    2 \n2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 \n   2    2    2    2    2    4    2    7    2    7    2    2    4    2    2    2 \n2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 \n   2    2    3    2    2    2    2    2    2    2    2    2    2    4    2    2 \n2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 \n   2    2    2    2    3    2    2    2    2    2    2    2    2    2    2    2 \n2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 \n   2    2    2    2    2    4    2    2    2    2    2    2    2    2    2    4 \n2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 \n   2    2    2    2    2    2    2    3    3    2    2    2    2    2    2    2 \n2289 2290 \n   2    2 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 885\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  2290 points\nAverage intensity 3.156983e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf[mpsz_sf$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz_sf[mpsz_sf$PLN_AREA_N == \"TAMPINES\",]\nbp = mpsz_sf[mpsz_sf$PLN_AREA_N == \"BUKIT PANJANG\",]\njw = mpsz_sf[mpsz_sf$PLN_AREA_N == \"JURONG WEST\",]\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.41081, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.63429, p-value = 1.645e-07\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  }
]