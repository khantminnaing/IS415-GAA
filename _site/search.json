[
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "title": "In-Class Exercise 03 NKDE",
    "section": "2.0 Installing and launching the R package",
    "text": "2.0 Installing and launching the R package\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nrgdal, which provides bindings to the ‘Geospatial’ Data Abstraction Library (GDAL) (&gt;= 1.11.4) and access to projection/transformation operations from the PROJ library. In this exercise, rgdal will be used to import geospatial data in R and store as sp objects.\nsp, which provides classes and methods for dealing with spatial data in R. In this exercise, it will be used to manage SpatialPointsDataFrame and SpatiaLinesDataFrame, and for performing projection transformation.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, spNetwork, tmap, classInt, virdis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "title": "In-Class Exercise 03 NKDE",
    "section": "3.0 Importing Data to R Environment",
    "text": "3.0 Importing Data to R Environment\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format.\n\nnetwork &lt;- st_read(dsn=\"../data/geospatial\", \n                   layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\nchildcare &lt;- st_read(dsn=\"../data/geospatial\",\n                     layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\nNext, we will plot the childcare locations (as points) and road network (as lines) as follows.\n\ntmap_mode('view')\ntm_shape(childcare)+\n  tm_dots(col='orange')+\n  tm_shape(network)+\n  tm_lines()\n\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "title": "In-Class Exercise 03 NKDE",
    "section": "4.0 Network Constrained KDE (NetKDE) Analysis",
    "text": "4.0 Network Constrained KDE (NetKDE) Analysis\n\n4.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nlixels &lt;- lixelize_lines(network, 750, mindist = 375)\n\n\n\n4.2 Generating Line Centers\nNext, we will used lines_center() of spNetwork to generate a SpatialPointsDataFrame (i.e. samples) with line centre points.\n\nsamples &lt;- lines_center(lixels)\n\n\n\n4.3 Performing NKDE\nOnce we have obtained all the datasets required, we can perform NKDE by using nkde() function of spNetwork.\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1,nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n4.4 Visualisation\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nNext, we will rescale the density values to help with better mapping results\n\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\", palette=\"plasma\")+\ntm_shape(childcare)+\n  tm_dots()\n\n\n\n\n\ntmap_mode('plot')\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)\nIn practical use, we can use these results to effectively identify areas where new pedestrian roads can be built."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05.html",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "A collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Local spatial autocorrelation statics can be decomposed from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*. In this exercise, we will explore sfdep package in R-environment, with a case study on gross domestic product per captial (GDPPC) of Hunan.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\npacman::p_load(sf, tmap, sfdep, tmap, tidyverse)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan_GDPPC &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\") \n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan_GDPPC &lt;- left_join(hunan_GDPPC,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\ntmap_mode('plot')\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\",\n          style =\"quantile\",\n          palette = \"plasma\",\n          title= \"GDPPC\") + \n  tm_layout(main.title = \"Distribution of GDP per capita by County, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size=2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nNow, we will derive contiguity weight matrix using Queen’s method. To achieve this, we will use st_contiguity and st_weights functions, respectively\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nThe neighbor list is created by st_contiguity()\nThe weight list is created by st_weights()\nThe style = \"W\" argument indicates that the weights should be row-standardized, which is a common choice in spatial analysis\nThe argument .before = 1 indicates that the new variables should be added as the first columns of the data frame.\n\n\n\n\n\n\nWe will now compute global moran’s I values using global_moran_test() function.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nWe will test the Moran’s I values using Monte Carlo simulations with nsim=99. In sfdep, we can do it by using global_moran_perm() function.\n\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe p-value is &lt; 2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#overview",
    "href": "In-class_Ex/In-class_Ex05.html#overview",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "A collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Local spatial autocorrelation statics can be decomposed from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*. In this exercise, we will explore sfdep package in R-environment, with a case study on gross domestic product per captial (GDPPC) of Hunan."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#overview-1",
    "href": "In-class_Ex/In-class_Ex05.html#overview-1",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\npacman::p_load(sf, tmap, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex05.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan_GDPPC &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\") \n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex05.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan_GDPPC &lt;- left_join(hunan_GDPPC,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\ntmap_mode('plot')\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\",\n          style =\"quantile\",\n          palette = \"plasma\",\n          title= \"GDPPC\") + \n  tm_layout(main.title = \"Distribution of GDP per capita by County, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size=2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nNow, we will derive contiguity weight matrix using Queen’s method. To achieve this, we will use st_contiguity and st_weights functions, respectively\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nThe neighbor list is created by st_contiguity()\nThe weight list is created by st_weights()\nThe style = \"W\" argument indicates that the weights should be row-standardized, which is a common choice in spatial analysis\nThe argument .before = 1 indicates that the new variables should be added as the first columns of the data frame.\n\n\n\n\n\n\nWe will now compute global moran’s I values using global_moran_test() function.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nWe will test the Moran’s I values using Monte Carlo simulations with nsim=99. In sfdep, we can do it by using global_moran_perm() function.\n\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe p-value is &lt; 2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03.html",
    "title": "In-Class Exercise 03",
    "section": "",
    "text": "pacman::p_load(arrow,lubridate,tidyverse,tmap,sf)\n\n\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 03",
    "section": "3.0 Importing Datasets to R Environment",
    "text": "3.0 Importing Datasets to R Environment\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n3.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 03",
    "section": "4.0 Geospatial Data Wrangling",
    "text": "4.0 Geospatial Data Wrangling\n\n4.1. Assigning Stanadrd Coordinate Systems\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n4.2 Creating Coastal Outline\n\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\nplot(sg_sf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "2.0 Literature Review of Spatial Point Pattern Analysis",
    "text": "2.0 Literature Review of Spatial Point Pattern Analysis\nSpatial point pattern analysis is concerned with description, statistical characterization, modeling and visulisation of point patterns over space and making inference about the process that could have generated an observed pattern (Boots & Getis, 1988 ,Rey et al., 2023; Pebesma & Bivand, 2023). According to this theory, empirical spatial distribution of points in our daily life are not controlled by sampling, but a result of an underlying geographically-continuous process (Rey et al., 2023). For example, an COVID-19 cluster did not happen by chance, but due to a spatial process of close-contact infection.\nWhen analysing real-world spatial points, it is important to analyse whether the observed spatial points are randomly distributed or patterned due to a process or interaction (Floch et al., 2018). In “complete random” distribution, points are located everywhere with the same probability and independently of each other. On the other hand, the spatial points can be clustered or dispersed due to an underlying point process. However, it is challenging to use heuristic observation and intuitive interpretation to detect whether a spatial point pattern exists (Baddeley et al., 2015; Floch et al., 2018). Hence, spatial point pattern analysis can be used to detect the spatial concentration or dispersion phenomena.\n\nWhen analysing and interpreting the properties of a point pattern, these properties can be categorized into two: (a) first-order properties and (b) second-order properties (Yuan et al., 2020; Gimond, 2023). First-order properties concern with the characteristics of individual point locations and their variations of their density across space (Gimond, 2023). Under this conception, observations vary from point to point due to changes in the underlying property. Second-order properties focus on not only individual points, but also the interactions between points and their influences on one another (Gimond, 2023). Under this conception, observations vary from place to place due to interaction effects between observations. First-order properties of point patterns are mostly addressed by density-based techniques, such as quadrat analysis and kernel density estimation, whereas, distance-based techniques, such nearest neighbour index and K-functions, are often used to analyse second-order properties since they take into account the distance between point pairs (Yuan et al., 2020; Gimond, 2023)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "3.0 Importing Packages",
    "text": "3.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nsf : provides a standardised way to encode spatial vector data in R environment, facilitating spatial data operations and analysis.\nst : creats simple features from numeric vectors, matrices, or lists, enabling the representation and manipulation of spatial structures in R.\narrow : for reading and writing Apache Parquet files, a columnar storage file format optimized for use with big data processing frameworks.\nlubridate : for tackling with temporal data (dates and times), providing tools to parse, manipulate, and do arithmetic with date-time objects.\nspatstat: A package for statistical analysis of spatial data, specifically Spatial Point Pattern Analysis. This package was provided by Baddeley, Turner and Ruback (2015) and gives a comprehensive list of functions to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\ntidyverse : a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structure.\nraster : reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\ntmap: Packages used for creating static and interactive visualisations summary statistics and KDE layers.\nspatstat : for spatial statistics with a strong focus on analysing spatial point patterns.\nspNetwork : provides several functions to perform spatial analysis on network, including network kernel density estimation and point pattern analysis.\nclassInt : for choosing univariate class intervals for mapping or other graphics purposes.\nviridis : for providing viridis color palette designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency.\ngifski : for converting images to GIF animations, useful for creating dynamic visualizations and presentations.\n\n\npacman::p_load(sf,st,arrow,lubridate,tidyverse,raster,tmap,ggplot2, patchwork,spatstat,spNetwork,classInt,viridis,gifski)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "4.0 Importing Datasets into R Environment",
    "text": "4.0 Importing Datasets into R Environment\n\n4.1 Datasets\nIn this study, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia. Apart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\nMoreover, we will also use OpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nTo extract the Singapore boundary, we will use Master Plan 2019 Subzone Boundary (No Sea), provided by data.gov.sg.\n\n\n4.2 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format. The Singapore portion of the dataset is packaged into a total of 10 Parquet files.\nFirstly, we will use read_parquet function from arrow package, which allows us to read Parquet files into R environment as a data frame (more specifically, a tibble).\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet',as_data_frame = TRUE)\ndf1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet',as_data_frame = TRUE)\ndf2 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00002.snappy.parquet',as_data_frame = TRUE)\ndf3 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00003.snappy.parquet',as_data_frame = TRUE)\ndf4 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00004.snappy.parquet',as_data_frame = TRUE)\ndf5 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00005.snappy.parquet',as_data_frame = TRUE)\ndf6 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00006.snappy.parquet',as_data_frame = TRUE)\ndf7 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00007.snappy.parquet',as_data_frame = TRUE)\ndf8 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00008.snappy.parquet',as_data_frame = TRUE)\ndf9 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00009.snappy.parquet',as_data_frame = TRUE)\n\nTo consolidate all trajectory instances into a single dataframe, we’ll vertically bind all 10 imported dataframes using bind_rows() function from tidyverse package.\n\ndf_trajectories &lt;- bind_rows(df,df1,df2,df3,df4,df5,df6,df7,df8,df9)\n\nTo get a quick overview of the dataset, we’ll first check the number of trajectory instances using dim() function. Then, we’ll use head() function to quickly scan through the data columns and values\n\ndim(df_trajectories)\n\n[1] 30329685        9\n\nhead(df_trajectories)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 30329685 trajectory instances, each with a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\nOperating System used for Data Recording\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf_trajectories$pingtimestamp &lt;- as_datetime(df_trajectories$pingtimestamp)\n\n\n\n4.3 Importing OpenStreetMap road data for Malaysia, Singapore and Brunei\nThe gis_osm_roads_free_1 dataset, which we downloaded from OpenStreetMap, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nosm_road_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"gis_osm_roads_free_1\") %&gt;% st_transform(crs = 3414)\n\n\n\n4.4 Importing Singapore Master Plan Planning Subzone boundary data\nThe MP14_SUBZONE_WEB_PL dataset, which we downloaded from data.gov.sg, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;% st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we use %&gt;% operator is used to pipe the output of st_read() to the st_transform() function. Since the dataset we are using is the Singapore boundary, we need to assign the standard coordinate reference system for Singapore, which is SVY21 (EPSG:3414). st_transform() function transforms the coordinate reference system of the sf object to 3414.\n\n\nAfter importing the dataset, we will plot it to see how it looks. The plot() function is used to plot the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(mpsz_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "5.0 Data Wrangling",
    "text": "5.0 Data Wrangling\nData wrangling is the process of converting and transforming raw data into a usable form and is carried out prior to conducting any data analysis. In this section, we will carry out different data wrangling steps to prepare our datasets ready for analysis.\n\n5.1 Extracting Trip Starting Locations and Temporal Data Values from Grab-Posisi dataset\nFirstly, we will extract trip starting locations for all unqiue trajectories in the dataset and store them to a new df named origin_df. We are also interested in obtaining valuable temporal data such as the day of the week, the hour, and the date (yy-mm-dd). To do so, we will use the following functions from lubridate package, and add the newly derived values as columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         pickup_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n5.2 Extracting Trip Ending Locations and Temporal Data Values from Grab-Posisi dataset\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         dropoff_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, this default order is applied to origin_df. However, for destination_df, the arrange(desc()) argument is used to modify the default order to descending.\n\n\n\n\n5.3 Converting to sf tibble data.frame\norigin_df & destination_df that we created earlier are of class “DataFrame”. However, when carrying out point pattern analysis, we need to convert them to sf object to allow for spatial operations. The sf package introduces the sf data frame, which is a tibble (a modern reimagining of the data frame) that has some special characteristics for handling spatial data. Hence, we will convert origin_df & destination_df into sf tibble data.frame using rowlng and rowlat columns as inputs for coordinates. Subsequently, we will assign appropriate coordinate reference system for Singapore, SVY21 (EPSG:3414).\n\norigin_sf &lt;- st_as_sf(origin_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\ndestination_sf &lt;- st_as_sf(destination_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n5.4 Creating a CoastalLine of Singapore\nThe original mpsz_sf dataset we imported include information of all URA Master Plan planning area boundaries. However, for this analysis, we only need the national-level boundary of Singapore. Hence, we will need to union all the subzone boundaries to one single polygon boundary. Also, Grab ride-hailing service is only available on the main Singapore islands. Hence, we will need to remove outer islands which Grab service is not available. In particular, we will remove the following planning subzones: NORTH-EASTERN ISLANDS, SOUTHERN GROUP, SUDONG & SEMAKAU.\nWe can remove these subzones using the subset() function. The subset() function is used to extract rows from a data frame that meet certain conditions.\n\nnortheasten.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouthern.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\")\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\")\n\nouterislands &lt;- dplyr::bind_rows(list(northeasten.islands,southern.islands,sudong,semakau))\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we first created four new data frames called northeasten.islands, southern.islands, sudong, and semakau by selecting rows from mpsz_sf where the value in the SUBZONE_N column matches the corresponding value.\nAfter that, we used bind_rows() function from the dplyr package to combine these four data frames into a single data frame called outerislands.\n\n\nAfter importing the dataset, we will plot it to see how it looks.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(outerislands))\n\n\n\n\nAs mentioned earlier, we only need to get national-level boundary of Singapore, without outer islands. To do so, we will need to process the mpsz_sf layer to achieve the outcome. - We will first use st_union() function from the sf package to combine the geometries of mpsz_sf and outerislands sf objects into a single geometry each. - Next, we will use st_difference() function then removes the overlapping areas between the two geometries. - Finally, we will store the non-overlapping areas into a new sf objected called sg_sf.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outerislands))\n\nTo assess whether the geometry of the newly created sg_sf matches our intended outcome, we will plot it out.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(sg_sf))\n\n\n\n\n\n\n5.5 Extracting Road Layers within Singapore\nAs we have seen in Section 4.3., osm_road_sf dataset includes road networks from not only Singapore, but also Malaysia and Brunei. However, our analysis is focused on Singapore. Hence, we will need to remove unecessary data rows. To do so, we will\n\nsg_road_sf &lt;- st_intersection(osm_road_sf,lsg_sf)\n\nNext, we will look at the classification of road networks as provided by OpenStreetMap.\n\nunique(sg_road_sf$fclass)\n\n [1] \"primary\"        \"residential\"    \"tertiary\"       \"footway\"       \n [5] \"service\"        \"secondary\"      \"motorway\"       \"motorway_link\" \n [9] \"trunk\"          \"trunk_link\"     \"primary_link\"   \"pedestrian\"    \n[13] \"living_street\"  \"unclassified\"   \"steps\"          \"track_grade2\"  \n[17] \"track\"          \"secondary_link\" \"cycleway\"       \"path\"          \n[21] \"tertiary_link\"  \"track_grade1\"   \"track_grade3\"   \"unknown\"       \n[25] \"track_grade5\"   \"bridleway\"      \"track_grade4\"  \n\n\nLooking at the road classification, it is observed that not all categories are relevant to our analysis, which is primarily concerned with driving networks where taxis can facilitate pick-ups or drop-offs. Hence, we will implement a filtering process on the dataset to exclude road segments that fall outside the scope of our analysis.\nFirstly we will specify the road classes that we want to retain.\n\ndriving_classes &lt;- c(\"primary\", \"primary_link\", \"residential\", \"secondary\", \"secondary_link\", \"service\", \"tertiary\", \"tertiary_link\") \n\nNext, we will filter sg_road_sf object to remove all the rows that does not have our desired f_class attribute value.\n\nsg_driving_sf &lt;- sg_road_sf %&gt;%\n  filter(fclass %in% driving_classes)\nunique(sg_driving_sf$fclass)\n\n[1] \"primary\"        \"residential\"    \"tertiary\"       \"service\"       \n[5] \"secondary\"      \"primary_link\"   \"secondary_link\" \"tertiary_link\" \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhy motorway and motorway_link classes are not included?\nAccording to OpenStreetMap, fclass motorway refers to expressway. In Singapore, stopping or parking a vehicle on an expressway is illegal under the Road Traffic Act. Hence, motorway (and motorway_link) are not relevant for network constraint kernel density estimation (NKDE) analysis that we will carry out later.\n\n\nNow that we have filterd out the dataset, we will now plot to see the driving road network of Singapore using tmap.\n\ntmap_mode(\"plot\")\n\ntm_shape(sg_sf) +\n  tm_polygons() +\ntm_shape(sg_driving_sf) +\n  tm_lines(col=\"fclass\", palette =\"viridis\") +\n  tm_layout(main.title = \"Road Network in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.outside = TRUE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.5 Converting the Simple Features to Planar Point Pattern Object\nIn order to use the capabilities of spatstat packahe, a spatial dataset should be converted into an object of class planar point pattern ppp (Baddeley et al., 2015). A point pattern object contains the spatial coordinates of the points, the marks attached to the points (if any), the window in which the points were observed, and the name of the unit of length for the spatial coordinates. s. Thus, a single object of class ppp contains all the information required to perform spatial point pattern analysis.\nIn previous section, we have created sf objects of Grab trajectory origin and destination points. Now, we will convert them into ppp objects using as.ppp() function from spatstat package.\n\norigin_ppp &lt;- as.ppp(st_coordinates(origin_sf), st_bbox(origin_sf))\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe code chunk above converts the origin_sf object to a point pattern object of class ppp. st_coordinates() function is used to extract the coordinates of the origin_sf object and st_bbox() function is used to extract the bounding box of the origin_sf object. The resulting object origin_ppp is a point pattern object of class ppp.\n\n\n\ndestination_ppp &lt;- as.ppp(st_coordinates(destination_sf), st_bbox(destination_sf))\n\npar(mar = c(0,0,1,0))\nplot(destination_ppp)\n\n\n\n\n\n\n5.6 Handling Data Errors\nBefore going striaght into analysis, we will need to a quick look at the summary statistics of the newly created ppp objects. This is an important step to ensure that the data is free of errors and that a reliable analysis can be performed.\n\n5.6.1 Data Error Handling for origin_ppp\nWe will use summary() function to get summary information of origin_ppp object.\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\nWe can also check if there is any duplicated points in origin_ppp object using any(duplicated() function.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordaintes in the origin_ppp object.\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we need to check duplication?\nWhen analyzing spatial point processes, it is important to avoid duplication of points. This is because statistical methodology for spatial point processes is based largely on the assumption that processes are simple, i.e., that points of the process can never be coincident. When the data have coincident points, some statistical procedures designed for simple point processes will be severely affected (Baddeley et al., 2015).\n\n\n\n\n5.6.2 Data Error Handling for destination_ppp\nWe will use summary() function to get summary information of destination_ppp object.\n\nsummary(destination_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\nWe can also check if there is any duplicated points in destination_ppp object using any(duplicated() function.\n\nany(duplicated(destination_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordinates in the destination_ppp object.\n\n\n\n5.7 Creating Observation Windows\nMany data types in spatstat require us to specify the region of space inside which the data were observed. This is the observation window and it is represented by an object of class owin. In this analysis, our study area is Singapore, hence we will use Singapore boundary as the observation window for spatial point pattern analysis.\nIn Section 5.4, we have already created the sg_sf object, which represents the Singapore boundary (without outer islands). To convert this sf object to owin object, we will use as.owin() function from spatstat package.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot.owin(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of sg_owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46\n\n\n\n\n5.8 Combining ppp objects and owin object\nIn section 5.5, we have created two ppp objects - origin_ppp and destination_ppp, each representing the spatial points of Grab trajectory origin and destination. In section 5.7, we have created a owin object called sg_owin, which represent the observation window of our analysis.\nThe observation window sg_owin and the point pattern origin_ppp or destination_ppp can be combined, so that the custom window replaces the default ractangular extent (as seen in section 5.5).\n\norigin_ppp_sg = origin_ppp[sg_owin]\ndestination_ppp_sg = destination_ppp[sg_owin]\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp_sg)\n\n\n\nplot(destination_ppp_sg)\n\n\n\n\nWe will use summary() function to get summary information of the newly created origin_ppp_sg object and destination_ppp_sg object.\n\nsummary(origin_ppp_sg)\n\nPlanar point pattern:  28000 points\nAverage intensity 3.965129e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46\n\nsummary(destination_ppp_sg)\n\nPlanar point pattern:  27997 points\nAverage intensity 3.964704e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "6.0 Exploratory Spatial Data Analysis",
    "text": "6.0 Exploratory Spatial Data Analysis\n\n6.1 Measuring Central Tendency\nDescriptive statistics are used in point pattern analysis to summarise a point pattern’s basic properties, such as its central tendency and dispersion. The mean centre and the median centre are two often employed metrics for central tendency (Gimond, 2019).\n\n6.1.1 Mean Center\nMean center is the arithmetic average of the (x, y) coordinates of all point in the study area. Similar to mean in statistical analysis, mean center is influenced to a greater degree by the outliers. (Yuan et al.,2020)\n\norigin_xy &lt;- st_coordinates(origin_sf)\norigin_mc &lt;- apply(origin_xy, 2, mean)\n\ndestination_xy &lt;- st_coordinates(destination_sf)\ndestination_mc &lt;- apply(destination_xy, 2, mean)\n\norigin_mc\n\n       X        Y \n28490.57 36939.04 \n\ndestination_mc\n\n       X        Y \n28870.96 36590.49 \n\n\nThe results show that the origin and destination mean centres are, respectively, (28490.57, 36939.04) and (28870.96, 36590.49). The two mean centres appear to be situated in close proximity to one another.\n\n\n6.1.2 Median Center\nMedian center is the location that minimizes the sum of distances required to travel to all points within an observation window. It can be calculated using an iterative procedure first presented by Kulin and Kuenne (1962). The procedure begins at a predetermined point, such as the median center, as the initial point. Then, the algorithm updates the median center’s new coordinates (x’, y’) continually until the optimal value is reached. The median center, as opposed to the mean center, offers a more reliable indicator of central tendency as it is unaffected by outliers (Yuan et al., 2020).\n\norigin_medc &lt;- apply(origin_xy, 2, median)\n\ndestination_medc &lt;- apply(destination_xy, 2, median)\n\norigin_medc\n\n       X        Y \n28553.17 36179.05 \n\ndestination_medc\n\n       X        Y \n28855.04 35883.86 \n\n\nBased on the results, the median centres of origin and destination are, respectively, (28553.17, 36179.05) and (28855.04, 35883.86). The two median centres appear to be situated in close proximity to one another.\nMoreover, mean centers and median centers for each origin and destination points are similar. This may imply that the distribution of the data is relatively balanced and there is not a significant difference in the spatial patterns between the origin and destination points. Additionally, this indicates that both the mean center and median center are effective measures for analyzing the central tendency of the data in this context.\n\n\n6.1.3 Plotting Mean and Median Centers\nWe can try to plot both results obtained from previous section on the same plane for better comparison of the mean center and median center.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(origin_medc[1], origin_medc[2]), pch='*', col='purple', cex=3)\n\n\n\n\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='yellow', cex=3)\npoints(cbind(destination_medc[1], destination_medc[2]), pch='*', col='orange', cex=3)\n\n\n\n\n\n\n\n6.2 Measuring Dispersion\n\n6.2.1 Standard Distance\nStandard distances are defined similarly to standard deviations. This indicator measures how dispersed a group of points is around its mean center (Gimond, 2023).\n\norigin_sd &lt;- sqrt(sum((origin_xy[,1] - origin_mc[1])^2 + (origin_xy[,2] - origin_mc[2])^2) / nrow(origin_xy))\n\ndestination_sd &lt;- sqrt(sum((destination_xy[,1] - destination_mc[1])^2 + (destination_xy[,2] - destination_mc[2])^2) / nrow(destination_xy))\n\norigin_sd\n\n[1] 10187.88\n\ndestination_sd\n\n[1] 9545.69\n\n\nFrom the results, the origin and destination standard distances are 10187.88 and 9545.69, respectively. Hence, it appears that origin points are more dispersed than the origin points.\n\n\n\n\n\n\nReflection\n\n\n\nHowever, it would be challenging to discern why the origin points are more dispersed without further analysis. Further analysis would be needed to determine the factors contributing to the increased dispersion of destination points. Since it is out of scope for this exercise, we will not explore any further.\n\n\n\n\n6.2.2 Plotting Standard Distances\nIn this section, we will create bearing circles of origin and destination points using the standard distance values we have calculated earlier. This can provide visual representation of their dispersion and make intuitive comparison between them.\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey', main=\"standard distance of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- origin_mc[1] + origin_sd * cos(bearing)\ncy &lt;- origin_mc[2] + origin_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='red', lwd=2)\n\n\n\n\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey',main=\"standard distance of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ncy &lt;- destination_mc[2] + destination_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='purple', lwd=2)\n\n\n\n\nA better comparison of the standard distances between origin and destination points can also be achieved by trying to plot both results on the same plane.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey',main=\"standard distances of origin_sf & destination_sf\")\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\n\norigin_cx &lt;- origin_mc[1] + origin_sd * cos(bearing)\norigin_cy &lt;- origin_mc[2] + origin_sd * sin(bearing)\n\ndestination_cx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ndestination_cy &lt;- destination_mc[2] + destination_sd * sin(bearing)\n\norigin_circle &lt;- cbind(origin_cx, origin_cy)\ndestination_circle &lt;- cbind(destination_cx, destination_cy)\n\nlines(origin_circle, col='red', lwd=2)\nlines(destination_circle, col='purple', lwd=2)\n\n\n\n\n\n\n\n6.3 Spatial Randomness Test\nClark and Evans (1954) give a very simple test of spatial randomness called Clark and Evans aggregation index (R). It is the ratio of the observed mean nearest neighbour distance in the pattern to that expected for a Poisson point process of the same intensity. R-value &gt;1 suggests ordering, while R-value &lt;1 suggests clustering.\nWe will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\n\n6.3.1 Spatial Randomness Test for Origin Points\nThe test hypotheses are:\n\nH0 = The distribution of trajectory original points are randomly distributed.\nH1= The distribution of trajectory original points are not randomly distributed.\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(origin_ppp_sg,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origin_ppp_sg\nR = 0.27408, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe Clark-Evans test for the origin points shows an R-value of 0.27408, which is less than 1. This indicates a clustered distribution. The p-value is less than 2.2e-16, which is extremely small and less than the significance level of 0.05. This means that we will reject the null hypothesis (H~0) and accept the alternative hypothesis (H~1). Therefore, the statistical inference from this test is that the original points are not randomly distributed but are clustered. This suggests that there may be underlying factors influencing the spatial distribution of these points.\n\n\n6.3.2 Spatial Randomness Test for Destination Points\nThe test hypotheses are:\n\nH0 = The distribution of trajectory destination points are randomly distributed.\nH1= The distribution of trajectory destination points are not randomly distributed.\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(destination_ppp_sg,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  destination_ppp_sg\nR = 0.29484, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe Clark-Evans test for the destination points also shows an R-value of 0.29484, which is less than 1. This indicates a clustered distribution. The p-value is less than 2.2e-16, which is significantly smaller than the significance level of 0.05. Therefore, we reject the null hypothesis (H~0) and accept the alternative hypothesis (H~1). Therefore, the statistical inference from this test is that the destination points are not randomly distributed but are clustered. This suggests that there may be underlying factors influencing the spatial distribution of these points."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "7.0 First-Order Spatial Point Patterns Analysis",
    "text": "7.0 First-Order Spatial Point Patterns Analysis\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package. As we have discussed in Section 2.0., First-order properties concern the characteristics of individual point locations and their variations of their density across space and are mostly addressed by density-based techniques, such as quadrant analysis and kernel density estimation.\nInvestigation of the intensity of a point pattern is one of the first and most important steps in point pattern analysis (Baddeley et al., 2015). If the point process has an intensity function λ(u), this function can be estimated non-parametrically by kernel estimation (Baddeley et al., 2015). Kernel estimation allows for smoothing of the probability density estimation of a random variable (in this analysis a point event) based on kernels as weights.\n\n7.1 Rescaling origin_ppp_sg and destination_ppp_sg\nThe SVY21 Coordinate References System uses meters as the standard unit. Hence, the original_ppp_sg and destination_ppp_sg that we have prepared in the previous sections has “metres” as the unit. However, we will need to convert the measuring unit from metre to kilometeres when calculating the kernel density estimators for entirety of Singapore because kilometers provide a more appropriate scale for analyzing large areas.\n\norigin_ppp_sg.km &lt;- rescale(origin_ppp_sg, 1000, \"km\")\ndestination_ppp_sg.km &lt;- rescale(destination_ppp_sg, 1000, \"km\")\n\n\n\n7.2 Computing Default Kernel Density Estimation\nKernel Destiny Estimation (KDE) generates a surface (raster) representing the estimated distribution of point events over the observation window. Each cell in the KDE layer carries a value representing the estimated density of that location (Wilkin, 2020). Hence, this approach is also known as local density approach. To build the KDE layer, a localised density is calculated for multiple small subsets of the observation window. However, these subsets overlap throughout each iteration, resulting in a moving window defined by a kernel (Wilkin, 2020; Gimond, 2023).\nIn this section, we will focus on destination points as we would like to identify areas that exert a ‘pull’ effect on people, hence resulting in cluster of trajectory destinations. Analyzing the destination of Grab trajectories can provide interesting insights into pull factors within a given area and help identify popular destinations or areas of high mobility demand.\nKernel estimation is implemented in spatstat by the function density.ppp(), a method for the generic command density.\n\npar(mar = c(0,1,1,1))\nkde_default_destination &lt;- density(destination_ppp_sg.km)\nplot(kde_default_destination,main = \"Default Density KDE for Destination Points\")\ncontour(kde_default_destination, add=TRUE)\n\n\n\n\nsigma argument in density() function controls the bandwidth of kernel function. The choice of the bandwidth affects the kernel density estimation strongly. A smaller bandwidth will produce a finer density estimate with all little peaks and valleys. A larger bandwidth will result into a smoother distribution of point densities. Generally speaking, if the bandwidth is too small the estimate is too noisy, while if bandwidth is too high the estimate may miss crucial elements of the point pattern due to over-smoothing (Scott, 2009).\n\n\n\nDensity Estimates with Different Smoothing Bandwidth (Ref, Baddeley et al., 2015)\n\n\nWhen sigma value is not specified, an isotropic Gaussian kernel will be used, with a default value of sigma calculated by a simple rule of thumb that depends only on the size of the window. Hence, the KDE given by default argument may not be what we aim to achieve. Looking at the KDE plot we have created above, there are signs of oversmoothing where only a single spatial cluster in the CBD area being observable. This can severely limit our analysis as potential small-scale clusters and other interesting details are being masked by the oversmoothing effect.\nTo overcome this challenge, we can specify smoothing bandwidth through the argument sigma or kernel function through the argument kernel to compute and plot more intuitive and detailed KDE maps.\n\n\n7.3 Creating KDE Layers with Fixed Bandwidth\n\n7.3.1 Computing Fixed Bandwidths Using Different Bandwidth Selection Methods\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density: In this method, band-width σ is chosen to minimize the mean-square error criterion defined by Diggle (1985). The mean-square error is a measure of the average of the squares of the errors - that is, the average squared difference between the estimated values and the actual value.\nbw.CvL() Cronie and van Lieshout’s Criterion for Bandwidth Selection for Kernel Density: The bandwidth σ is chosen to minimize the discrepancy between the area of the observation window and the sum of reciprocal estimated intensity values at the points of the point process. This method aims to choose a bandwidth that best represents the underlying point process, taking into account both the observed points and the area they occupy.\nbw.scott() Scott’s Rule for Bandwidth Selection for Kernel Density: The bandwidth σ is computed by the rule of thumb of Scott (1992). The bandwidth is proportional to \\(n^{-1/(d-4)}\\) where n is the number of points and d is the number of spatial dimensions. This rule is very fast to compute. It typically produces a larger bandwidth than Diggle’s method. It is useful for estimating gradual trend.\nbw.ppl() Likelihood Cross Validation Bandwidth Selection for Kernel Density: This approach, explained by Loader (1999), uses likelihood cross-validation to determine the bandwidth (σ) by maximizing the point process likelihood. This method is beneficial when the aim is to maximize the likelihood of observing the given data.\n\n\nbw_diggle &lt;- bw.diggle(destination_ppp_sg.km)\nbw_diggle\n\n      sigma \n0.008317447 \n\nbw_CvL &lt;- bw.CvL(destination_ppp_sg.km)\nbw_CvL\n\n   sigma \n3.745658 \n\nbw_scott &lt;- bw.scott(destination_ppp_sg.km)\nbw_scott\n\n  sigma.x   sigma.y \n1.4763217 0.9063352 \n\nbw_ppl &lt;- bw.ppl(destination_ppp_sg.km)\nbw_ppl\n\n    sigma \n0.1913655 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe notice that bw_diggle, bw_CvL and bw_ppl all give a numeric sigma value, whereas bw_scott, by default, provides a separate bandwidth for each coordinate axis. In the code output above, sigma.x = 1.4763217 and sigma.y = 0.9063352 are the estimated bandwidths for the x and y coordinates, respectively. These values represent the amount of smoothing applied in each direction when estimating the kernel density.\nWe can specify isotropic=TRUE argument when calculating bw_scott() method to produce a single value bandwidth.\n\n\n\nbw_scott_single &lt;- bw.scott(destination_ppp_sg.km, isotropic=TRUE)\nbw_scott_single \n\n   sigma \n1.156738 \n\n\nThe optimized bandwidth values generated from above methods belongs to the special class bw.optim. The plot function can be used to see the objective function for the optimisation that leads to the result.\n\npar(mfrow = c(1,2))\nplot(bw_diggle, xlim=c(-0.02,0.05), ylim=c(-60,200))\nplot(bw_CvL)\n\n\n\npar(mfrow = c(1,2))\nplot(bw_scott, main=\"bw_scott\")\nplot(bw_ppl,  xlim=c(-1,5), ylim=c(70000,130000))\n\n\n\n\n\n\n7.3.2 Plotting Fixed-Bandwidth KDE Layers\nIn practice, there are no definite method to choose the KDE bandwidth. Many literature has outlined a diverse range of approaches for KDE bandwidth selection. According to Wolff and Asche (2009), the choice of bandwidth in many existing studies is mostly conducted by visually comparing different bandwidth setting.\nHence, we will now create KDE layers based on each bandwidth selection method and visualize them to have a better comparison of how distinct the resulting KDE layers are.\n\nkde_diggle &lt;- density(destination_ppp_sg.km, bw_diggle)\nkde_CvL &lt;- density(destination_ppp_sg.km, bw_CvL)\nkde_scott &lt;- density(destination_ppp_sg.km, bw_scott)\nkde_ppl &lt;- density(destination_ppp_sg.km, bw_ppl)\n\npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(kde_diggle,main = \"kde_diggle\")\nplot(kde_CvL,main = \"kde_CvL\")\nplot(kde_scott,main = \"kde_scott\")\nplot(kde_ppl,main = \"kde_ppl\")\n\n\n\n\nNext, we will try to plot histograms to compare the distribution of KDE values obtained from density() function using different bandwidth selection methods.\n\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_diggle,main = \"kde_diggle\")\nhist(kde_CvL,main = \"kde_CvL\")\nhist(kde_scott,main = \"kde_scott\")\nhist(kde_ppl,main = \"kde_ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can interpret the outputs as below:\n\nkde_diggle: The sharp peak at the beginning indicates that the Diggle method for bandwidth selection has indentified a high concentration of points in the first bin. The rest of the bins has little to no concentration. This may suggest that one specific area in our observation window has observed a relatively high spatial clustering than the rest of the window.\nkde_CvL: The more balanced distribution suggests that the CvL method for bandwidth selection is identifying a broader range of spatial point concentration. However, the bin sizes are quite small, which smooths out the overall distribution and masks some of the finer details.\nkde_scott: The wider range of values and less sharp peak compared to kde_diggle indicate that the Scott method is capturing a wider range of spatial point concentrations, including both densely concentrated locations and moderately concentrated ones.\nkde_ppl: The result is very similar to the Diggle method, the sharp peak at the beginning suggests a high concentration of points in a specific area, suggest that one specific area in our observation window has observed a relatively high spatial clustering than the rest of the area.\n\n\n\nAnother apporach to compare the KDE layers is to calculate the standard error of each density estimation. $SE is used to extract the standard error of the density estimate from the output of the density() function\n\ndse_diggle &lt;- density(destination_ppp_sg.km, bw_diggle, se=TRUE)$SE\ndse_CvL &lt;- density(destination_ppp_sg.km, bw_CvL, se=TRUE)$SE\ndse_scott &lt;- density(destination_ppp_sg.km, bw_scott, se=TRUE)$SE\ndse_ppl &lt;- density(destination_ppp_sg.km, bw_ppl, se=TRUE)$SE\n\n\npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(dse_diggle,main = \"standard error_diggle\")\nplot(dse_CvL,main = \"standard error_CvL\")\nplot(dse_scott,main = \"standard error_scott\")\nplot(dse_ppl,main = \"standard error_ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe standard error (SE) of the density estimate provides a measure of the uncertainty associated with the density estimate at each point. This can be useful for understanding the variability of density estimates, especially when comparing density estimates obtained using different bandwidths.\nHowever, in many applications of KDE, the focus is often on the shape of the density estimate rather than its absolute value. In such cases, the standard error might not be as relevant. Hence, in this analysis, we will not use standard error as a criteria for choosing the bandwidth.\n\n\n\n\n7.3.3 Choosing Fixed KDE Bandwidth Selection Method\nUpon the exploration of various fixed bandwidth selection methods for computing KDE vales, and subsequent plotting of the respective KDE estimates, their distributions and associated standard errors, we will now select the KDE bandwidth to be used in our analysis. As we have seen in Section 7.3.1.2, each KDE bandwidth method has produced a distinct KDE and there is no definite method to choose the KDE bandwidth.\nWe will proceed to choose bw_scott method for further analysis. This is because:\n\nbw_scott method provides a pair of bandwidth values for each coordinate axis. This allows it to capture the different levels of spatial clustering in each direction more accurately.\nbw_scott method capture the balance between bias and variance the best among all methods. If the bandwidth is too small, the estimate may be too skewed (high variance). The distribution histograms of KDE layers using bw_diggle and bw_ppl tend to indicate such nature. On the other hand, if the bandwidth is too large, the estimate may be oversmoothed, missing crucial elements of the point pattern (high bias). This is what we observed in the distribution histogram of KDE layer using bw_CvL.\n\nSince we have chosen to use bw_scott method, now we will plot the KDE layer using this method for further analysis.\n\npar(mar = c(0,1,1,1))\nbw_fixed_scott &lt;- bw.scott(destination_ppp_sg.km)\nbw_fixed_scott\n\n  sigma.x   sigma.y \n1.4763217 0.9063352 \n\nkde_fixed_scott &lt;- density(destination_ppp_sg.km, bw_fixed_scott)\nplot(kde_fixed_scott,main = \"Fixed-Bandwidth KDE for Grab Destination Points (Using bw_scott)\")\ncontour(kde_fixed_scott, add=TRUE)\n\n\n\n\nHowever, upon visual inspection, there are signs of a certian degree of over-smoothing when we directly use the bandwidth provide by bw_scott method. Automatic bandwidth selection methods provides a starting point for bandwidth selection, and further fine-tuning might be necessary based on the results of the plot we have created above.\nTo do so, we will use rule of thumb adjustment by diving the bandwidth value by 2, to reduce the bandwidth size, and hence possible over-smoothing effect.\n\npar(mar = c(0,1,1,1))\nkde_fixed_scott &lt;- density(destination_ppp_sg.km, sigma=bw_fixed_scott/2)\nplot(kde_fixed_scott,main = \"Fixed-Bandwidth KDE for Grab Destination Points (Using bw_scott)\")\ncontour(kde_fixed_scott, add=TRUE)\n\n\n\n\nLooking at the plot created, it appears that by reducing the bandwidth (thus making the point cluster buffers smaller), the over-smoothing effects have been minimized. However, we still can observe the Grab destination hotspot areas, which we can investigate further in subsequent sections.\n\n\n7.3.4 Kernel Function Selection for Fixed Bandwidth\n\n\n\n\n\nKernel functions are another consideration in KDE computation because they control how we weight points within the bandwidth radius. The default kernel in density.ppp() is the gaussian. alternatives such as epanechnikov, quartic, and disc are also available.\nIn this section, we will explore and experiment with different kernel functions and plot them together to see how it influences our KDE map results.\n\nkde_fixed_scott.gaussian &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_fixed_scott.epanechnikov &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_fixed_scott.quartic &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_fixed_scott.disc &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(kde_fixed_scott.gaussian, main=\"Gaussian\")\nplot(kde_fixed_scott.epanechnikov, main=\"Epanechnikov\")\nplot(kde_fixed_scott.quartic, main=\"Quartic\")\nplot(kde_fixed_scott.disc, main=\"Disc\")\n\n\n\n\nXie & Yan (2008) suggested that the choice of kernel function has minimal impact on density estimation outcomes. Shen et al. (2020) further identified the search bandwidth as a more influential factor in kernel estimation than the selection of different kernel functions. Empirically looking at the KDE maps we have created above using different kernel functions, similar conclusion can be made. Despite the slight variations in smoothness and spread, all four plots show similar patterns of density estimation. This underscores the argument that the choice of kernel function does not significantly impact KDE results. Consequently, we will not focus this aspect in our analysis.\n\n\n\n7.4 Creating KDE Layers with Spatially Adaptive Bandwidth\nFixed bandwidth kernels are commonly employed in statistical literature due to their ease of implementation. However, their application in spatial datasets often yields suboptimal estimations due to a lack of spatial and temporal adaptability (González & Moraga, 2022). Consequently, the more intuitive ‘adaptive smoothing’ approach has emerged. In this technique, the amount of smoothing is inversely related to the density of the points.\nIn spatstat packages, there are three main approaches (Pebesma & Bivand, 2023) in creating KDE layers with spatially adapative bandwidth.\n\nVoronoi-Dirichlet Adaptive Density Estimate: It computes the intensity function estimate of a point pattern dataset by creating tessellations. On default, the input point pattern data is used to construct a Voronoi/Dirichlet tessellation (Barr and Schoenberg, 2010). The intensity estimate at a given location equals the reciprocal of the size of the Voronoi/Dirichlet cell containing that location.\nAdaptive Kernel Density Estimate: It computes an estimate of the intensity function of a point pattern dataset using the partitioning technique of Davies and Baddeley (2018). It dynamically specifies the smoothing bandwidth to be applied to each of the points. The partitioning method of Davies and Baddeley (2018) accelerates this computation by partitioning the range of bandwidths into n-groups intervals, correspondingly subdividing the point patterns into n-groups, sub-patterns according to bandwidth, and applying fixed-bandwidth smoothing to each sub-pattern.\nNearest-Neighbour Adaptive Density Estimate: It computes an estimate of the intensity function of a point pattern dataset using the distance from each spatial location to the kth nearest points (Cressie, 1991: Silverman, 1986;Burman & Nolan, 1989). The default value of k is the square root of the number of points in the dataset. This estimator of intensity is relatively fast to compute and is spatially adaptive. Some studies suggest the use of the nearest neighbor distance as a suitable parameter for determining the bandwidth (Krisp et al., 2009).\n\n\n7.4.1 Voronoi-Dirichlet Adaptive Density Estimate\nThe Dirichlet-Voronoï estimator is computed in spatstat by the function adaptive.density() with argument method=\"voronoi\".\n\nkde_destination_dirichlet_adaptive &lt;- adaptive.density(destination_ppp_sg.km, f=1, method = \"voronoi\")\n\n\npar(mar = c(0,1,1,1))\nplot(kde_destination_dirichlet_adaptive,main = \"Voronoi-Dirichlet Adaptive Density Estimate\")\n\n\n\n\n\n\n7.4.2 Adaptive Kernel Density Estimate\nThe Adaptive Kernel estimator is computed in spatstat by the function adaptive.density() with argument method=\"kernel\".\n\nkde_destination_kernel_adaptive &lt;- adaptive.density(destination_ppp_sg.km, method = \"kernel\")\n\n\npar(mar = c(0,1,1,1))\nplot(kde_destination_kernel_adaptive,main = \"Adaptive Kernel Density Estimate\")\n\n\n\n\n\n\n7.4.3 Nearest-Neighbour Density Estimate\nThe Nearest-Neighbour estimator is computed in spatstat by the function nndensity().\n\nkde_adaptive_nn &lt;- nndensity(destination_ppp_sg.km, k=10)\n\n\npar(mar = c(0,1,1,1))\nplot(kde_adaptive_nn,main = \"Nearest-Neighbour Adaptive Density Estimate\")\n\n\n\n\n\n\n7.4.4 Choosing Adaptive KDE Method\nSimilar to what we did for fixed bandwidth, we can try to plot histograms to compare the distribution of KDE values obtained from density() function using different adaptive bandwidth selection methods.\n\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_destination_dirichlet_adaptive,main = \"Voronoi-Dirichlet Adaptive\")\nhist(kde_destination_kernel_adaptive,main = \"Adaptive Kernel\")\nhist(kde_adaptive_nn,main = \"Nearest-Neighbour Adaptive\")\n\n\n\n\nFrom the outputs, it seems that there is no significance difference between the distribution of KDE values obtained across different methods. All three methods identified a high concentration of points in a specific area. Hence, we will choose to go with Adapative Kernel method because it provides the most\n\n\n\n7.5 Plotting Interactive KDE Maps\n\nraster_kde_fixed_scott &lt;- raster(kde_fixed_scott)\nraster_kde_adaptive_nn &lt;- raster(kde_adaptive_nn)\nraster_kde_adaptive_kernel &lt;- raster(kde_destination_kernel_adaptive)\n\n\nprojection(raster_kde_fixed_scott) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(raster_kde_adaptive_nn) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(raster_kde_adaptive_kernel) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\n\n\ntmap_mode('view')\nkde_fixed_scott &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_fixed_scott) +\n  tm_raster(\"layer\",\n            n = 10,\n            title = \"KDE_Fixed_scott\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_mode('view')\nkde_adaptive_nn &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_adaptive_nn) +\n  tm_raster(\"layer\",\n            n = 7,\n            title = \"KDE_Adaptive_nn\",\n            style = \"pretty\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_mode('view')\nkde_adaptive_kernel &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_adaptive_kernel) +\n  tm_raster(\"layer\",\n            n = 7,\n            title = \"KDE_Adaptive_Kernel\",\n            style = \"pretty\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_arrange(kde_fixed_scott, kde_adaptive_nn, kde_adaptive_kernel ,ncol=1,nrow=3,sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.6 Analysis of Singapore-Level Kernel Density Estimation Maps\nFromThe highest cluster of Grab taxi drop-off points are seen near Changi Airport, where the concentrations reach up to 450.\nThe central and southern parts of Singapore, particularly the Central Business District (CBD) and Marina Bay areas, also exhibit substantial concentrations of Grab taxi drop-off points, peaking at 350. These areas have a high demand for taxi services, possibly due to a high concentration of businesses, tourist attractions.\nInterestingly enough, we found a few residential subzones outside of the CBD region, which is comparatively higher density values compared to the rest of the island. However, because of the smoothing effect, identifying these hotspots using a fixed-bandwidth KDE map is difficult. Hence, we proceeded to perform a cross inspection with adaptive KDE maps for better precision in hotspots identification.\nThrough adaptive nearest neighbour KDE map, we identified distinct hotspots in Jurong West (7000-8000), Woodlands (6000-7000), Tampines (4000-5000), and Toa Payoh (4000-5000). The Adaptive Kernel KDE map complements this finding by accentuating two additional hotspots, Jurong East (500-1000) and Punggol (500-1000).\n\n\n\nSource: URA Master Plan 2019\n\n\nIt is intriguing to see that the six residential subzones we have identified have higher concentrations of drop-off points, despite being predominantly classified as residential areas , according to the Master Plan 2019 (MP19) by Urban Redevelopment Authority Singapore. It will be interesting to discern underlying spatial process and pull factors that lead to this spatial cluster. Therefore, we will attempt to create comparable KDE maps at the planning subzone level.\n\n\n7.7 Planning Area-Level Kernel Density Estimation\nIn this section, we will create planning-area level KDE maps for six planning areas we have identified. In order to create such maps, we will carry out additional data wrangling as required.\nFirstly, we will filter out different planning areas as separate sf objects from mpsz_sf.\n\nwl = mpsz_sf %&gt;% filter(PLN_AREA_N == \"WOODLANDS\")\nje = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG EAST\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\ntn = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\ntp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TOA PAYOH\")\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\n\n\npar(mar = c(1,1,1,0),mfrow=c(2,3))\nplot(st_geometry(wl), main = \"Woodlands\")\nplot(st_geometry(je), main = \"Jurong East\")\nplot(st_geometry(jw), main = \"Jurong West\")\nplot(st_geometry(tn), main = \"Tampines\")\nplot(st_geometry(tp), main = \"Toa Payoh\")\nplot(st_geometry(pg), main = \"Punggol\")\n\n\n\n\nNext, we will create owin objects to represent the observation windows for respective planning area. Once owin objects are created, we will also filter Grab taxi drop-off points in each observation window from the original destination_ppp_sg ppp object.\n\nwl_owin = as.owin(wl)\nje_owin = as.owin(je)\njw_owin = as.owin(jw)\ntn_owin = as.owin(tn)\ntp_owin = as.owin(tp)\npg_owin = as.owin(pg)\n\ndestination_wl_ppp = destination_ppp_sg[wl_owin]\ndestination_je_ppp = destination_ppp_sg[je_owin]\ndestination_jw_ppp = destination_ppp_sg[jw_owin]\ndestination_tn_ppp = destination_ppp_sg[tn_owin]\ndestination_tp_ppp = destination_ppp_sg[tp_owin]\ndestination_pg_ppp = destination_ppp_sg[pg_owin]\n\nNow that we have prepared both owin and ppp objects for each planning area, we are ready to plot KDE maps. Similar to what we have done in previous section, we will try both fixed-bandwitdh and adaptive bandwidth KDE maps.\n\n7.7.1 Planning Area-Level Fixed-Bandwidth KDE Maps\n\nwl_kde_scott &lt;- density(destination_wl_ppp, sigma=bw.scott, main=\"Woodlands\")\nje_kde_scott &lt;- density(destination_je_ppp, sigma=bw.scott, main=\"Jurong East\")\njw_kde_scott &lt;- density(destination_jw_ppp, sigma=bw.scott, main=\"Jurong West\")\ntn_kde_scott &lt;- density(destination_tn_ppp, sigma=bw.scott, main=\"Tampines\")\ntp_kde_scott &lt;- density(destination_tp_ppp, sigma=bw.scott, main=\"Toa Payoh\")\npg_kde_scott &lt;- density(destination_pg_ppp, sigma=bw.scott, main=\"Punggol\")\npar(mar = c(1,1,1,1.5),mfrow = c(3,2))\n\nplot(wl_kde_scott,main = \"Fixed KDE Woodlands\")\ncontour(wl_kde_scott, add=TRUE)\nplot(je_kde_scott,main = \"Fixed KDE Jurong East\")\ncontour(je_kde_scott, add=TRUE)\nplot(jw_kde_scott,main = \"Fixed KDE Jurong West\")\ncontour(jw_kde_scott, add=TRUE)\nplot(tn_kde_scott,main = \"Fixed KDE Tampines\")\ncontour(tn_kde_scott, add=TRUE)\nplot(tp_kde_scott,main = \"Fixed KDE Toa Payoh\")\ncontour(tp_kde_scott, add=TRUE)\nplot(pg_kde_scott,main = \"Fixed KDE Punggol\")\ncontour(pg_kde_scott, add=TRUE)\n\n\n\n\n\n\n7.7.2 Planning Area-Level Adaptive-Bandwidth KDE Maps\n\nwl_kde_adaptive_kernel &lt;- adaptive.density(destination_wl_ppp, method = \"kernel\")\nje_kde_adaptive_kernel &lt;- adaptive.density(destination_je_ppp, method = \"kernel\")\njw_kde_adaptive_kernel &lt;- adaptive.density(destination_jw_ppp, method = \"kernel\")\ntn_kde_adaptive_kernel &lt;- adaptive.density(destination_tn_ppp, method = \"kernel\")\ntp_kde_adaptive_kernel &lt;- adaptive.density(destination_tp_ppp, method = \"kernel\")\npg_kde_adaptive_kernel &lt;- adaptive.density(destination_pg_ppp, method = \"kernel\")\n\npar(mar = c(1,1,1,1.5),mfrow = c(3,2))\n\nplot(je_kde_adaptive_kernel,main = \"Adaptive KDE Woodlands\")\ncontour(je_kde_adaptive_kernel, add=TRUE)\nplot(je_kde_adaptive_kernel,main = \"Adaptive KDE Jurong East\")\ncontour(je_kde_adaptive_kernel, add=TRUE)\nplot(jw_kde_adaptive_kernel,main = \"Adaptive KDE Jurong West\")\ncontour(jw_kde_adaptive_kernel, add=TRUE)\nplot(tn_kde_adaptive_kernel,main = \"Adaptive KDE Tampines\")\ncontour(tn_kde_adaptive_kernel, add=TRUE)\nplot(tp_kde_adaptive_kernel,main = \"Adaptive KDE Toa Payoh\")\ncontour(tp_kde_adaptive_kernel, add=TRUE)\nplot(pg_kde_adaptive_kernel,main = \"Adaptive KDE Punggol\")\ncontour(pg_kde_adaptive_kernel, add=TRUE)\n\n\n\n\n\n\n\n7.8 Analysis of Planning Area-Level Kernel Density Estimation Maps\nObservations from the planning area-level KDE maps reveal that while KDE maps excel in visualizing and analyzing spatial data, their application to micro-level analysis, like planning subzones, has limitations. Firstly, there are issues with over-smoothing (in fixed KDE maps) and undersmoothing (in adaptative KDE maps) which deters us from discerning meaningful insights. Furthermore, KDE values, generated on grid-pixels using Euclidean distance, result in grid blocks that limit our ability to discern fine details and variations within each subzone.\nFor example, let’s look at the picture below, which is a snapshot from KDE adaptive nearest-neighbor map. A hotspot is identified and represented by a concentrated red zone near Braddell Road in Toa Payoh. However, just by looking at this map, it’s really hard to understand more about this hotspot. We know there is a cluster of drop-off points, but why are they there? What are pull factors attracting people to this area? It’s almost impossible to answer these questions just by looking at this KDE map.\n\n\n\n\n\nThis limitation underscores the need for more advanced visualization techniques or analytical methods that can provide alternative approaches to understanding spatial point clusters. With this limitation in mind, we will apply a new analytical method called network constrained kernel density estimation (NKDE) to offer more detailed insights into the spatial distribution of Grab taxi drop-off points through the integration of road networks."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "8.0 Network Constrained Kernel Density Estimation (NKDE)",
    "text": "8.0 Network Constrained Kernel Density Estimation (NKDE)\nIn the real world, point events are often not randomly distributed. Instead, their distribution is constrained by networks. When carrying out spatial point pattern analysis, traditional Kernel Density Estimation (KDE) assumes an infinite, homogeneous, two-dimensional space, an approximation that is not accurate for network-based study areas. In such networks, movement is constrained by multiple one-dimensional lines (Gelb, 2021). Network Constrained Kernel Density Estimation (NKDE), therefore, is a widely used approach to identify the hotspots and evaluate origin-destination points along with the road network (Shen et al. 2020).\nThis approach estimates the intensity of the spatial process solely on the network. The network edges are divided into lixels (one-dimensional pixels), and the centers of the lixels serve as the locations for intensity estimation. Distances between events and sampling points are calculated as the shortest path distances on the network, instead of Euclidean distances. This adjustment slightly modifies the intensity function from the classical KDE function. The adapted formula makes the interpretation straightforward: it “estimates the density over a linear unit” rather than an area unit.\nIn this section, we will follow-up on the six planning areas we identified in Section 7.6 and attempt to create NKDE maps using spNetwork package.\n\n8.1 Extracting Road Networks for Focus Planning Areas\nBefore we carry out the anlysis, we will extract relevant datasets required for calculating NKDE values in each planning area:\n\nroad network\ndestination points\n\n\nwl_network = st_intersection(sg_driving_sf,st_union(wl))\nje_network = st_intersection(sg_driving_sf,st_union(je))\njw_network = st_intersection(sg_driving_sf,st_union(jw))\ntn_network = st_intersection(sg_driving_sf,st_union(tn))\ntp_network = st_intersection(sg_driving_sf,st_union(tp))\npg_network = st_intersection(sg_driving_sf,st_union(pg))\n\n\nwl_destination = st_intersection(destination_sf,st_union(wl))\nje_destination = st_intersection(destination_sf,st_union(je))\njw_destination = st_intersection(destination_sf,st_union(jw))\ntn_destination = st_intersection(destination_sf,st_union(tn))\ntp_destination = st_intersection(destination_sf,st_union(tp))\npg_destination = st_intersection(destination_sf,st_union(pg))\n\n\n\n8.3 Data Preparation\n\nFor illustrative purpose, I’ll use “Punggol” as a example to demonstrate step-by-step data preparation process. Upon completion of this demonstration, maps for the remaining planning areas will be created.\n\nThe spNetwork package contains nkde function that is specifically designed to implement Network Constrained Kernel Density Estimation (NetKDE).\nThe three main inputs are:\n\nlixels: a “SpatialLinesDataFrame”, representing the lines of the network.\nevents: a “SpatialPointsDataframe”, representing the realizations of the spatial process.\nsamples: a “SpatialPointsDataframe” providing the locations where the density must be estimated.\n\n\n8.3.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork.\n\npg_lixels &lt;- lixelize_lines(pg_network, \n                         700, \n                         mindist = 350)\n\nIn this code snippet, the length of a lixel is set to 700m, and the minimum length of a lixel, denoted as mindist, is set to 350m. This implies that during segmentation, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. The segments that are already shorter than the minimum distance are not modified.\n\n\n8.3.2 Generating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points. The points are located at center of the line based on the length of the line.\n\npg_samples &lt;- lines_center(pg_lixels)\n\n\n\n\n8.4 Performing NKDE\n\n8.4.1 Computing Network Constrained Kernel Density Estimation\nThe spNetwork package offers three methods for calculating NKDE values simple, discontinuous and continuous.\n\nSimple NKDE: Simple NKDE method was proposed by Xie and Yan (2008), extending the planar KDE to a network-based model. However, there are issues with this method due to statistical incorrectness. Specifically, at network intersections, the event’s mass is multiplied in each direction, leading to overestimation and inflated density estimates, which can result in misleading interpretations (Gelb, 2021). To address these issues, Okabe et al. (2009) proposed two unbiased estimators: Discontinuous NKDE and Continuous NKDE.\nDiscontinuous NKDE: Discontinuous NKDE aims to resolve density inflation by dividing the mass at intersections according to the number of directions minus one, resulting in an unbiased estimator (Gelb, 2021). However, the discontinuous nature of this method can be counter-intuitive in practical applications.\nContinuous NKDE: Continuous NKDE proposes to address the limitations of both simple and discontinuous NKDE. It adjusts the values of the NKDE at intersections and applies a backward correction to force the density values to be continuous (Gelb, 2021).\n\nIn this analysis, we will compute NKDE values using all three methods and compare the results.\n\npg_nkde_simple &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\npg_nkde_discontinuous &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"discontinuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\npg_nkde_continuous &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"continuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n\n\n\nReflection\n\n\n\nbw argument refers to the bandwidth used for calculating KDE values. Xie and Yan (2008) suggested that narrow bandwidths (between 20 and 250 m) are more appropriate for identifying local effects at smaller scales. Hence, we use 200 here.\nagg argument allows the events to be aggregated, and their weights to be added within a threshold distance. Aggregating events can simplify networks and limit the number of iterations when calculating the NKDE, hence effectively reducing time complexity of computation.\n\n\n\n\n8.4.2 Visualising NKDE Maps Using Different Methods\nBefore we can visualise the NetKDE values, we will insert the computed density values into samples and lixels objects. To enhance the readability of the results, you will first multiply the obtained densities by the total number of drop-off points. This adjustment ensures that the spatial integral equals the number of events. Subsequently, you will multiply this value by 1000 to derive the estimated number of drop-off points per kilometer. This process will provide a more intuitive understanding of the density distribution along the network.\n\npg_samples$nkde_simple &lt;- pg_nkde_simple*nrow(pg_destination)*1000\npg_lixels$nkde_simple &lt;- pg_nkde_simple*nrow(pg_destination)*1000\n\npg_samples$nkde_discontinuous &lt;- pg_nkde_discontinuous*nrow(pg_destination)*1000\npg_lixels$nkde_discontinuous &lt;- pg_nkde_discontinuous*nrow(pg_destination)*1000\n\npg_samples$nkde_continuous &lt;- pg_nkde_continuous*nrow(pg_destination)*1000\npg_lixels$nkde_continuous &lt;- pg_nkde_continuous*nrow(pg_destination)*1000\n\nNow we can plot NKDE maps using tmap.\n\ntmap_mode('view')\npg_nkde_simple_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_simple\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\npg_nkde_discontinuous_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_discontinuous\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\npg_nkde_continuous_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_continuous\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\n\ntmap_arrange(pg_nkde_simple_map, pg_nkde_discontinuous_map, pg_nkde_continuous_map ,ncol=3,nrow=1,sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\nComparing Simple, Discontinuous and Continuous NKDE Maps\n\n\n\n\n\nUpon close examination to the intersection as shown above, we can clearly see the density inflation caused by simple NKDE method, as opposed to discontinuous and continuous methods. A comparison between the Discontinuous and Continuous NKDE maps reveals the disjointed nature of lixels resulting from the Discontinuous method, as indicated by the varying color bands representing each lixel segment. Out of all three maps, continuous NKDE map offers the most intuitive representation.\n\n\n\n\n\n8.5 Plotting NKDE for Different Planning Areas\nIn this section, we will create interactive maps of different planning areas that we have previousely identified as potential hotspots. These maps will incorporate several key elements: network kernel density estimation values (depicted through lixels), Grab drop-off points, and points of interest (POIs). The purpose of including these elements is to identify the pull factors within these areas. Understanding these factors can help in planning and decision-making processes, such as where to locate new facilities or how to improve transportation routes.\n\nPunggolTampinesJurong EastJurong WestWoodlandsToa Payoh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.6 Analysis of Network Constrained Kernal Density Estimation (NKDE) Maps\nOverall, It is intriguing to observe the interplay of similar, yet distinct, pull factors across different planning subzones.\n\nPunggol: In Punggol, all observed clusters are located near schools and parks. The most significant clusters are found in the vicinity of Edgefield Secondary School, Edgefield Primary School, and Punggol Green Primary School. Multiple smaller clusters are observed around neighbourhood parks.\nTampines: In Tampines, a significant cluster is observed at Tanah Merah Country Club and Laguna Country Club, suggesting a high demand for taxi services in these recreational areas. A smaller cluster is also noticeable near Simei Neighbourhood Park.\nJurong East: In Jurong East, a significant cluster is observed leading to IMM, a major outlet shopping center, which seems to attract multiple taxi trajectories. Additional clusters are observed near Yuhua Primary School and Crest Secondary School, as well as various neighbourhood parks.\nJurong West: In Jurong West, two significant clusters are observed, one near Westwood Secondary School and the other leading to Nanyang Technological University, highlighting the importance of educational institutions in this area. Another equally significant cluster is located near Masjid Maarof, one of Singapore’s oldest and most prominent mosques.\nWoodlands: In Woodlands, a significant cluster is observed at Innova Junior College and Singapore Sports School. Additional clusters are found at Woodland Civic Centre, a community centre with library and dining areas, and STELLAR@TE2, a lifestyle shopping mall. We also observed another cluster leads to Singapore Turf Club.\nToa Payoh: In Toa Payoh, a cluster is observed near Toa Payoh Townpark, indicating its popularity as a communal destination. Another cluster is found at St Andrew’s Junior College.\n\nIn conclusion, the results of NKDE analysis of Grab taxi drop-off points in six selected planning areas reveals a nuanced pattern of clusters associated with various types of amenities and institutions. These findings suggest that different planning areas have distinct characteristics that attract people to specific locations within them. Additionally, understanding these pull factors can help inform urban planning and development strategies to create anew or further enhance the attractiveness of these areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "9.0 Temporal Network Kernel Density Estimation (TNKDE)",
    "text": "9.0 Temporal Network Kernel Density Estimation (TNKDE)\nTemporal Network Kernel Density Estimation (TNKDE) is an extension of Network Constrained Kernel Density Estimation (NKDE) whereby the the temporal dimension is integrated to calculating density of events on a network (Gelb & Apparicio, 2023). This means that density estimation can be done along lines of the network and at different times. The spatio-temporal kernel is calculated as the product of the network kernel density and the time kernel density.\nTNKDE can be calculated in calculated in R environment using tnkde() function from spNetwork package.\n\n9.1 Visualising Frequency Distribution\nBefore we calculate TNKDE, we will conduct an exploratory analysis of our data from the Grab Posisi dataset. This will involve visualizing the frequency distribution of both our origin and destination point samples. This step will provide us with a preliminary understanding of our data distribution, which is crucial for effective TNKDE calculation.\nFirstly, we will analyze the frequency distribution of days from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories originating and ending on each day of the week\n\norigin_day &lt;- ggplot(data=origin_sf, \n              aes(x=weekday)) + \n              geom_bar()\n\ndestination_day &lt;-  ggplot(data=destination_sf, \n                    aes(x=weekday)) + \n                    geom_bar()\n\norigin_day + destination_day\n\n\n\n\nAs seen above, each bar graph displays a uniform distribution across all weekdays. The dataset is sampled; hence all days are equally represented in both origin and destination datasets.\n\n\n9.2 Plotting Frequency of Trip Origination By Hour\nNext, we will analyze the frequency distribution of trip origination by hour from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories originating points in each hour.\n\norigin_sf$pickup_hr_num &lt;- as.numeric(origin_sf$pickup_hr)\npickup_hr_num &lt;- origin_sf$pickup_hr_num\n\nggplot(data=origin_sf, \n                    aes(x=pickup_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = pickup_hr_num)\n\n\n\n\nWe observed noticeable peaks at 3, 5, 14, and 15 hours (3 AM, 5 AM, 2 PM and 3 PM respectively). There’s a significant drop in trip originations during 9 hour (9 AM), as well as from 19 to 22 hours (7 PM to 10 PM). These could be quieter periods in the day when fewer trips are originated. It is also interesting to see a sudden increase of trip origination at 23 hour (11 PM).\n\n\n9.3 Plotting Frequency of Trip Destination By Hour\nNext, we will analyze the frequency distribution of trip destination by hour from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories destination points in each hour.\n\ndestination_sf$dropoff_hr_num &lt;- as.numeric(destination_sf$dropoff_hr)\ndropoff_hr_num &lt;- destination_sf$dropoff_hr_num\n\nggplot(data=destination_sf, \n                    aes(x=dropoff_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = dropoff_hr_num)\n\n\n\n\nSomewhat similar to origination points, we observed noticeable peaks at 3, 13, and 14 hours (3 AM, 1 PM, and 2 PM respectively). There’s a significant drop in trip desintation during 9, 18, 21 and 23 hours (9 AM, 6 PM, 9 PM and 11 PM respectively).\n\n\n\n\n\n\nReflection\n\n\n\nWhile the frequency plots of trip origination and destination points based on temporal segments reveal intriguing patterns, deriving meaningful insights from these can be challenging. This is primarily due to the nature of the dataset. As a sample dataset, the data points are randomly collected within a specific time window. Consequently, these data may not accurately represent the true temporal variations in traffic flows. Therefore, while the plots provide a snapshot of the data, they may not fully capture the complexity and variability of real-world traffic patterns.\n\n\n\n\n9.4 Time-Bandwidth Selection\nWe can now calculate the kernel density values of destination points in time for several bandwidths. When calculating TNKDE, two bandwidths are necessary, one for space and one for time. In this section, we will try to explore three different bandwidth selection methods:\n\nbw_bcv: implements biased cross-validation for bandwidth selection in kernel density estimation. The goal of cross-validation in this context is to find the bandwidth that minimizes the estimation error.\nbw_ucv: implements cross-validation for bandwidth selection, but in an unbiased manner.\nbw_SJ: implements the methods of Sheather & Jones (1991) to select the bandwidth using pilot estimation of derivatives.\n\nTo achieve this, we will follow the steps as below:\nWe will first create a vector w of length equal to the number of rows in the destination_sf data frame. Each element of the vector is set to 1. This vector will be used as weights in calculating TKDE.\n\nw &lt;- rep(1,nrow(destination_sf))\n\nWe will then create a sequence of numbers from 0 to the maximum value of the dropoff_hr_num column in the destination_sf data frame, with a step size of 0.5. These are the sample points where the density will be estimated.\n\nsamples &lt;- seq(0, max(destination_sf$dropoff_hr_num), 0.5)\n\nNext, we will calculate bandwidth values for dropoff_hr_num column of destination_sf using 3 bandwidth selection methods discussed above.\n\nbw_bcv &lt;- bw.bcv(destination_sf$dropoff_hr_num)\nbw_ucv &lt;- bw.ucv(destination_sf$dropoff_hr_num)\nbw_SJ &lt;- bw.SJ(destination_sf$dropoff_hr_num)\n\nOnce bandwidth values are calculated, we will proceed to implement TNKDE analysis using tkde() function from spPackage. Then, we will create a dataframe time_kernel_values to store these density values.\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_bcv, kernel_name = \"quartic\"),\n  bw_ucv = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_ucv, kernel_name = \"quartic\"),\n  bw_SJ = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_SJ, kernel_name = \"quartic\"),\n  time = samples\n)\n\nNext, we will use melt function from the reshape2 package to transform time_kernel_values dataframe into a new dataframe called df_time, using time as the identifier variable. We will also convert the variable column of the df_time data frame to a factor, for plotting in ggplot2.\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nFinally, we will use ggplot2 to create a line plot of the TKDE for each bandwidth.\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = dropoff_hr_num) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat does each line plot say about each bandwidth selection approach?\nIn all three plots, the line represents the estimated density of drop-offs at each time point. At a glance, bw_ucv and bw_SJ methods seem to give a more detailed and potentially noisier pattern than bw_bcv. Unlike the other two, bw_bcv approach shows a relatively smooth curve with minor fluctuations. The differences in these plots highlight how the choice of bandwidth can significantly affect the resulting density estimate.\n\n\n\n\n9.5 TNKDE Implementing at Punggol Planning Area\n\n9.5.1 Data Wrangling and Exploratory Analysis\nNow, we will try to construct a TNKDE map for Punggol Planning Area. Before running TNKDE, we will carry our some preliminary data wrangling and exploratory analysis to ensure our dataset is in the appropriate format.\n\npg_dropoff_hr_num &lt;- pg_destination$dropoff_hr_num\n\nggplot(data=pg_destination, \n                    aes(x=pg_dropoff_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = pg_dropoff_hr_num)\n\n\n\n\nFrom the graph above, we observed noticeable peaks at 1, 6, 8, 13, and 20 hours (1 AM, 6 AM, 8 AM, 1 PM, and 8 PM respectively). These peaks suggest that these time intervals might be particularly active or important. To investigate further, we will filter out the destination points that fall within these specific hour intervals and use tmap to visualize these filtered data points.\n\npg_dropoff_1 &lt;- filter(pg_destination, dropoff_hr_num == '1')\npg_dropoff_6 &lt;- filter(pg_destination, dropoff_hr_num == '6')\npg_dropoff_8 &lt;- filter(pg_destination, dropoff_hr_num == '8')\npg_dropoff_13 &lt;- filter(pg_destination, dropoff_hr_num == '13')\npg_dropoff_20 &lt;- filter(pg_destination, dropoff_hr_num == '20')\n\n\ntmap_mode(\"view\")\ntm_basemap(server = \"Esri.WorldTopoMap\") +\ntm_shape(pg_lixels)+\n  tm_lines()+\ntm_shape(pg_dropoff_1)+\n  tm_dots(size=0.01) +\n  tm_shape(pg_dropoff_6)+\n  tm_dots(size=0.01) +\n    tm_shape(pg_dropoff_8)+\n  tm_dots(size=0.01) +\n    tm_shape(pg_dropoff_13)+\n  tm_dots(size=0.01) +  \n  tm_shape(pg_dropoff_20)+\n  tm_dots(size=0.01)\n\n\n\n\n\n\n\n\n9.5.2 Bandwidth Selection\nOnce, we have applied visualization techniques to briefly analyse the datasets, we will proceed with implementing TKNDE for Punggol planning area. For the steps, we will repeat the procedures in Section 9.4 for this implementation.\n\nw_pg &lt;- rep(1,nrow(pg_destination))\nsamples_pg &lt;- seq(0, max(pg_destination$dropoff_hr_num), 0.5)\n\nbw_bcv &lt;- bw.bcv(pg_destination$dropoff_hr_num)\nbw_ucv &lt;- bw.ucv(pg_destination$dropoff_hr_num)\nbw_SJ &lt;- bw.SJ(pg_destination$dropoff_hr_num)\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_bcv, kernel_name = \"quartic\"),\n  bw_ucv = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_ucv, kernel_name = \"quartic\"),\n  bw_SJ = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_SJ, kernel_name = \"quartic\"),\n  time = samples_pg\n)\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = dropoff_hr_num) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\n\n\nAs we notice that the bandwidths plotted above only accounts for temporal aspect. However, both spatial and temporal dimensions are required for implementation of TNKDE. To achieve this, the network and temporal bandwidths can be selected with the leave-one-out-cross validation method (Gelb & Apparicio, 2023). To do so, we can use bws_tnkde_cv_likelihood_calc() function to select the most appropriate bandwidths for the network and time dimensions. It computes the cross-validation likelihood for various bandwidths, enabling a data-driven selection of the most suitable bandwidths for both dimensions.\n\ncv_scores &lt;- bws_tnkde_cv_likelihood_calc(\n  bw_net_range = c(100,1000),\n  bw_net_step = 100,\n  bw_time_range = c(3,24),\n  bw_time_step = 3,\n  lines = pg_network,\n  events = pg_destination,\n  time_field = \"dropoff_hr_num\",\n  w = rep(1, nrow(pg_destination)),\n  kernel_name = \"quartic\",\n  method = \"discontinuous\", \n  diggle_correction = FALSE,\n  study_area = NULL,\n  max_depth = 10,\n  digits = 2,\n  tol = 0.1,\n  agg = 15,\n  sparse=TRUE,\n  grid_shape=c(1,1),\n  sub_sample=1,\n  verbose = FALSE,\n  check = TRUE)\n\n\n\n\n\n\n\nReflection\n\n\n\nThis code chunk was referenced from Gelb’s implementation of TNKDE (2023) as provided in this vignette. Below is a breakdown of some arguments used in this code chunk.\n\nbw_net_range = c(100,1000), bw_net_step = 100: These parameters define the range and step size for the network bandwidths that will be evaluated. The function will calculate the cross-validation likelihood for network bandwidths from 100 to 1000, in steps of 100.\nbw_time_range = c(3,24), bw_time_step = 3: Similarly, these parameters define the range and step size for the time bandwidths that will be evaluated. The function will calculate the cross-validation likelihood for time bandwidths from 3 to 24, in steps of 3.\nkernel_name = \"quartic\": This parameter specifies the kernel function to use for the calculation. In this case, the quartic kernel is used.\nmethod = \"discontinuous\": This parameter specifies the method to use when calculating the TNKDE. The discontinuous method is used similar to what we did in NKDE.\ndiggle_correction = FALSE: This parameter specifies whether to use the correction factor for edge effect. In this case, since we are only looking at the neighborhood level, edge correction is not necessary and the correction factor is not used.\nagg = 15: This parameter specifies a function to aggregate the events when they are too close.\n\n\n\nWe can see the outputs of bws_tnkde_cv_likelihood_calc() function in a table format using knitr package.\n\nknitr::kable(cv_scores)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n6\n9\n12\n15\n18\n21\n24\n\n\n\n\n100\n-250.91794\n-182.71584\n-155.27632\n-116.78531\n-113.26649\n-102.39516\n-93.35400\n-89.81173\n\n\n200\n-145.76563\n-90.84170\n-69.02605\n-61.88347\n-56.56890\n-49.40677\n-49.57432\n-49.73512\n\n\n300\n-100.00061\n-65.40228\n-47.32325\n-43.88449\n-40.43182\n-36.94576\n-37.12520\n-37.29583\n\n\n400\n-68.95819\n-41.82902\n-32.92989\n-31.35903\n-29.73603\n-29.93138\n-30.11827\n-30.29386\n\n\n500\n-54.37326\n-34.65375\n-29.47804\n-29.75000\n-28.14349\n-28.34934\n-28.54072\n-28.71939\n\n\n600\n-47.21774\n-34.83465\n-29.69234\n-29.97549\n-28.37665\n-28.58649\n-28.77993\n-28.96001\n\n\n700\n-36.42261\n-29.55281\n-28.07569\n-28.36425\n-28.59379\n-28.80594\n-29.00069\n-29.18167\n\n\n800\n-32.88500\n-27.88168\n-28.25843\n-28.55089\n-28.79093\n-29.00503\n-29.20083\n-29.38249\n\n\n900\n-31.20502\n-28.04346\n-28.43185\n-28.72756\n-28.97115\n-29.18680\n-29.38338\n-29.56554\n\n\n1000\n-31.33948\n-28.19916\n-28.59400\n-28.89218\n-29.13776\n-29.35455\n-29.55170\n-29.73421\n\n\n\n\n\nAccording to the “leave one out cross validation” method, the optimal set of bandwidths is 800 metres and 6 hrs (the combination that gives the least cv score). As expected, larger bandwidths are required because the density of the events are spread both in space and time.\n\n\n9.5.3 TNKDE Calculation\nNow that we have selected an appropriate bandwidth for both spatial and temporal dimensions, we will proceed to implement NKDE calculation. Our first step involves selecting a sample time-step for the calculation. Given that our analysis is based on a 24-hour window, we adopted a 3-hour time-step as a simple rule-of-thumb. Then, we proceed to calculate TNKDE densities.\n\npg_sample_time &lt;- seq(0, max(pg_destination$dropoff_hr_num), 3)\n\npg_tnkde_densities &lt;- tnkde(lines = pg_network,\n                   events = pg_destination,\n                   time_field = \"dropoff_hr_num\",\n                   w = rep(1, nrow(pg_destination)), \n                   samples_loc = pg_samples,\n                   samples_time = pg_sample_time, \n                   kernel_name = \"quartic\",\n                   bw_net = 800, bw_time = 6,\n                   adaptive = TRUE,\n                   trim_bw_net = 900,\n                   trim_bw_time = 8,\n                   method = \"discontinuous\",\n                   div = \"bw\", max_depth = 10,\n                   digits = 2, tol = 0.01,\n                   agg = 15, grid_shape = c(1,1), \n                   verbose  = FALSE)\n\n\n\n\n\n\n\nReflection\n\n\n\nThis code chunk was referenced from Gelb’s implementation of TNKDE (2023) as provided in this vignette. Below is a breakdown of some arguments used in this code chunk.\n\nbw_net = 800, bw_time = 6: These parameters specify the bandwidths for the network and time dimensions respectively. We use 800 and 6 respectively based on the results of “leave one out cross validation” we carried out in previous section.\ntrim_bw_net = 900, trim_bw_time = 8: These parameters specify the maximum value for the adaptive bandwidth in the network and time dimensions respectively. Here, we use simple rule-of-thumb values.\n\n\n\n\n\n9.5.4 Creating Animated TNKDE Map\nUpon completion of TNKDE densities calculation, we will visulise the results in form of animated GIF.\nFirstly, we will use classInt package to create color breaks for densities values. To determine the class intervals, k-means algorithm will be used.\n\nall_densities &lt;- c(pg_tnkde_densities$k)\ncolor_breaks &lt;- classIntervals(all_densities, n = 10, style = \"kmeans\")\n\nNext, we will generate respective maps for each sample time and compile them all under pg_tnkde_densities using lapply() function. Each map visualizes TNKDE densities for a specific time point. The densities are represented by dots on the map, with the color of the dots indicating the density value. We will use the color breaks and palette specified earlier.\n\nall_maps &lt;- lapply(1:ncol(pg_tnkde_densities$k), function(i){\n  time &lt;- pg_sample_time[[i]]\n  \n  pg_samples$tnkde_density &lt;- pg_tnkde_densities$k[,i]\n  map1 &lt;- tm_shape(pg_samples) + \n  tm_dots(col = \"tnkde_density\", size = 0.01,\n          breaks = color_breaks$brks, palette = viridis(10)) + \n    tm_layout(legend.show=FALSE, main.title = as.character(time), main.title.size = 0.5)\n  return(map1)\n})\n\nGelb (2023) suggested the use of animated maps (GIF or video) to analyse the results of a TNKDE. hence, we will create an animated GIF from the series of maps stored in all_maps and then display the GIF. To achieve this, we will use tmap_animation() function from the tmap package. Once the GIF image is created, we will use include_graphics() function from the knitr package to display the created GIF.\n\ntmap_animation(all_maps, filename = \"images/animated_pg.gif\", \n               width = 1000, height = 1000, dpi = 300, delay = 50)\n\n\nknitr::include_graphics(\"images/animated_pg.gif\")\n\n\n\n\nThe TNKDE map for Punggol provides much more nuanced insights into the spatial clustering patterns across various time intervals. This enables a better understanding of how events are distributed and concentrated within the network over different periods. This can be useful for identification of hotspots and trends both spatially and temporally. Overall, the TNKDE approach offers a comprehensive analysis of spatial and temporal clustering within a network, providing a deeper understanding of the dynamics at play."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01.html#conclusion",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "10.0 Conclusion",
    "text": "10.0 Conclusion\nThis analysis has explored different technqiues of spatial point pattern analysis using Grab’s trajectory datasets. We have carried out three different approaches - Traditional Kernel Density Estimation (KDE), Network Constrained Kernel Density Estimation (NKDE) and Temporal Network Kernel Density Estimation (TNKDE). The findings from these approaches provide valuable insights into the spatial and temporal distribution of Grab’s trajectory datasets for Singapore (and particularly the distribution of destination points).\nIn this study, we have experimented different bandwidth selection methods and kernel function approaches and compare the results of different combinations. We then developed 3 KDE maps using three distinct approaches: Scott’s rule of thumb for fixed bandwidth, adaptive nearest neighbor, and adaptive kernel density. These KDE maps identified six planning subzones as potential hotspots for spatial clustering of Grab destination points.\nWe have also carried out comparative study between KDE and NKDE, outlining the limitations of pixel-based traditional KDE against network-based NKDE. We then created NKDE maps for selected six planning areas: Woodlands, Jurong East, Jurong West, Punggol, Tampines, and Toa Payoh. By utilizing network-based NKDE, we were able to overcome these limitations and provide more accurate and insights into the spatial distribution of various factors within the selected planning areas. The results from NKDE showed significant improvements in precision and interpretability compared to traditional KDE maps.\nFinally, the implementation of TNKDE with a case study in Punggol further enhanced our understanding by incorporating temporal information, allowing us to identify how spatial clusters change over different time intervals.\nFurther research could focus on expanding the application of TNKDE to other planning areas in Singapore. Additionally, exploring the potential of incorporating other data sources, such as demographic or socioeconomic data, could provide a more comprehensive understanding of the spatial distribution of Grab taxi origin and destination points and their temporal dynamics. This could help in identifying patterns and trends related to specific demographics or socioeconomic factors that may influence human mobility patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01.html#references",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "References",
    "text": "References\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial Point Patterns: Methodology and Applications with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b19708.\nBarr, C., & Schoenberg, F.P. (2010). On the Voronoi estimator for the intensity of an inhomogeneous planar Poisson process. Biometrika, 97(4), 977–984.\nBoots, B.N., & Getis, A. (1988). Point Pattern Analysis. Reprint. Edited by G.I. Thrall. WVU Research Repository.\nCronie, O., & van Lieshout, M.N. (2018). A non-model-based approach to bandwidth selection for kernel estimators of spatial intensity functions. Biometrika, 105, 455–462.\nDavies, T.M., & Baddeley, A. (2018). Fast computation of spatially adaptive kernel estimates. Statistics and Computing, 28(4), 937-956.\nDiggle, P.J. (1985). A kernel method for smoothing point process data. Applied Statistics (Journal of the Royal Statistical Society, Series C), 34, 138–147.\nFloch, J.-M., Marcon, E., & Puech, F. (n.d.). Spatial distribution of points. In M.-P. de Bellefon (Ed.), Handbook of Spatial Analysis : Theory and Application with R (pp. 72–111). Insee-Eurostat.\nGelb, J., Apparicio, P.: Temporal Network kernel density estimation. Geographical Analysis. 56, 62–78 (2023).\nGimond (2023). Chapter 11 Point Pattern Analysis. Retrieved from https://mgimond.github.io/Spatial/index.html.\nGonzález, J. A., & Moraga, P. (2022). An adaptive kernel estimator for the intensity function of spatio-temporal point processes.\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app.\nKrisp, J.M., Peters, S., Murphy, C.E., & Fan, H. (2009). Visual bandwidth selection for kernel density maps. Photogrammetrie - Fernerkundung - Geoinformation, 2009, 445–454.\nLoader, C. (1999). Local Regression and Likelihood. Springer, New York.\nO’Sullivan, D., & Wong, D.W. (2007). A surface‐based approach to measuring spatial segregation. Geographical Analysis, 39, 147–168.\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\nRey, S.J., Arribas-Bel, D., & Wolf, L.J. (2023). Point Pattern Analysis. In: Geographic Data Science with python. CRC Press.\nScott, D.W. (1992). Multivariate Density Estimation. Theory, Practice and Visualization. New York: Wiley.\nShen, B., Xu, X., Li, J., Plaza, A., & Huang, Q. (2020). Unfolding spatial-temporal patterns of taxi trip based on an improved network kernel density estimation. ISPRS International Journal of Geo-Information, 9, 683.\nWilkin, J. (2020). Geocomputation 2020-2021 Work Book. University College London. Retrieved from https://jo-wilkin.github.io/GEOG0030/coursebook/analysing-spatial-patterns-iii-point-pattern-analysis.html.\nWolff, M., & Asche, H. (2009). Towards geovisual analysis of crime scenes – a 3D crime mapping approach. Advances in GIScience, 429–448.\nXie, Z., & Yan, J. (2008). Kernel density estimation of traffic accidents in a network space. Computers, Environment and Urban Systems, 32, 396–406.\nYuan, Y., Qiang, Y., Bin Asad, K., & Chow, T. E. (2020). Point Pattern Analysis. In J.P. Wilson (Ed.), The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition). DOI: 10.22224/gistbok/2020.1.13."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHands-On Exercise 01\n\n\n\n\n\nGeospatial Data Wrangling with R!\n\n\n\n\n\n\nJan 7, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 02\n\n\n\n\n\nThematic Mapping and GeoVisualisation with R\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 03\n\n\n\n\n\nSpatial Point Pattern Analysis\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 04\n\n\n\n\n\nSpatial Weights and Applications\n\n\n\n\n\n\nJan 26, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 05\n\n\n\n\n\nGeographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n\n\n\n\nFeb 1, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 05\n\n\n\n\n\nGlobal and Local Measures of Spatial Autocorrelation\n\n\n\n\n\n\nFeb 1, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 08\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nJan 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 03\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 03 NKDE\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 04\n\n\n\n\n\nLocal Spatial Autocorrelation Statistics (LISA)\n\n\n\n\n\n\nJan 29, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 05\n\n\n\n\n\nLocal Spatial Autocorrelation Statistics using sfdep\n\n\n\n\n\n\nFeb 5, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nR Workshop Series:  RevealJS Presentation\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore\n\n\n\n\n\nApplication of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore, implemented using spatstat package (Baddeley, Turner, and Rubak 2022) and spNetwork package (Gelb & Apparicio, 2023) in R environment\n\n\n\n\n\n\nJan 15, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan\n\n\n\n\n\nThis study aims to use spatio-temporal analysis techniques to analyze if the distri-bution of dengue fever outbreak in Tainan City are independent from space and time. The study also attempts to identify clusters and outliers, as well as emerging hot spots/cold spots within the study area. The initial phase of the study utilizes Global and Local Spatial Autocorrelation modelling to discern the spatio-temporal pattern of the disease outbreak. Subsequently, Emerging Hot Spots Analysis (EHSA) is applied to identify spatio-temporal clusters.\n\n\n\n\n\n\nFeb 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html",
    "href": "Hands-on_Ex/hands_on08.html",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#overview",
    "href": "Hands-on_Ex/hands_on08.html#overview",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "href": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "title": "Hands-On Exercise 08",
    "section": "2.0 Importing Datasets and Packages",
    "text": "2.0 Importing Datasets and Packages\nFirstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "href": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "title": "Hands-On Exercise 08",
    "section": "3.0 Data Wrangling",
    "text": "3.0 Data Wrangling\n\n3.1 Geospatial Data Wrangling\nWe use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n3.1 Aspatial Data Wrangling\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 08",
    "section": "4.0 Exploratory Data Analysis (EDA)",
    "text": "4.0 Exploratory Data Analysis (EDA)\n\n4.1 EDA Using Statistical Graphics\nWe can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n4.2 EDA Using Multiple Histogram Plots Distribution of Variables\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n4.3 Drawing Statistical Point Map\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\n#tmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 08",
    "section": "5.0 Hedonic Pricing Modelling in R",
    "text": "5.0 Hedonic Pricing Modelling in R\nIn this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n5.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n5.2 Multiple Linear Regression Method\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n5.3 Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\n\n5.4 Checking for Multicollinearity\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n5.5 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n5.6 Test for Normality Assumption\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the Kolmogorov-Smirnov test\n\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n5.7 Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "href": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "title": "Hands-On Exercise 08",
    "section": "6.0 Building Hedonic Pricing Model using GWmodel",
    "text": "6.0 Building Hedonic Pricing Model using GWmodel\nAfter exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n6.1 Computing Bandwidth\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n6.2 Building Basic GWModel with Fixed and Adaptive Bandwidth\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-02 16:17:26.679824 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-03-02 16:17:28.159495 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-02 16:17:28.160216 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-03-02 16:17:29.80862 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n6.3 Visualisaing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\n#tmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\n\n#tmap_mode(\"view\")\n\n{= 1) +}   tm_view(set.zoom.limits = c(11,14))\nNext, we will visualise the coefficient estimates\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html",
    "href": "Hands-on_Ex/hands_on1.html",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#overview",
    "href": "Hands-on_Ex/hands_on1.html#overview",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#setup",
    "href": "Hands-on_Ex/hands_on1.html#setup",
    "title": "Hands-On Exercise 01",
    "section": "2.0 Setup",
    "text": "2.0 Setup\n\n2.1 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, I extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n\n2.2 Installing R-Packages\nIn this exercise, I will be using two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nI install the required packages using the code chunk below.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "3.0 Importing Geospatial Data into R",
    "text": "3.0 Importing Geospatial Data into R\nIn this section, I will import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n3.1 Importing polygon feature data in shapefile format\nDataset used: MP14_SUBZONE_WEB_PL File format: shapefile Data frame type: polygon feature\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our mpsz simple feature data frame, there are 323 multipolygon features, 15 fields and is in the svy21 projected coordinates system.\n\n\n3.2 Importing polyline feature data in shapefile form\nDataset used: CyclingPathGazette File format: shapefile Data frame type: line feature\n\ncyclingpath = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                      layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our cyclingpath linestring feature data frame, there are 1625 linestring features, 2 fields and is in the svy21 projected coordinates system.\n\n\n3.3 Importing GIS data in kml format\nDataset used: pre-schools-location-kml File format: kml Data frame type: point feature\n\npreschool = st_read(\"~/IS415-GAA/data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we can see that in our preschool point feature data frame, there are 1359 linestring features, 2 fields and is in the wgs84 projected coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "4.0 Importing + Converting Aspatial Data into R",
    "text": "4.0 Importing + Converting Aspatial Data into R\nFor aspatial data, such as the listings Airbnb datset, there’s an extra step in the importing process. We’ll import it into a tibble data frame, then convert it into a simple feature data frame.\n\n4.1 Importing aspatial data\nSince our listings data set is in a csv file format, we’ll use the read_csv() function from the readr package, like so:\n\nlistings &lt;- read_csv(\"~/IS415-GAA/data/aspatial/listings.csv\")\n\nRows: 3457 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(listings) \n\nRows: 3,457\nColumns: 18\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ latitude                       &lt;dbl&gt; 1.34537, 1.34754, 1.34531, 1.29015, 1.2…\n$ longitude                      &lt;dbl&gt; 103.9589, 103.9596, 103.9610, 103.8081,…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n\n\nFrom the output message, we can see that in our listing tibble data frame, there are 4252 rows and 16 columns (not features and fields like in our simple data feature frame!) Take note of the latitude and longitude fields - we’ll be using them in the next phase.\n\nAssumption: The data is in the wgs84 Geographic Coordinate System on account of its latitude/longtitude fields.\n\n\n\n4.2 Converting aspatial data\nNow, let’s convert our listing tibble data frame into a by using the st_as_sf() function from the sf package.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThis gives us the new simple feature data frame, listings_sf:\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\n\nNote that a new column called geometry has been added! In addition, longtitude and latitude have both been dropped."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-On Exercise 01",
    "section": "5.0 Checking the Content of A Simple Feature Data Frame",
    "text": "5.0 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "title": "Hands-On Exercise 01",
    "section": "6.0 Plotting the Geospatial Data",
    "text": "6.0 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. I use plot() to quickly plot a sf object as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "href": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "title": "Hands-On Exercise 01",
    "section": "7.0 Working with Projection",
    "text": "7.0 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, I project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nTo check the coordinate system of mpsz simple feature data frame, I use st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in SVY21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for SVY21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nI take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (25089.46 31299.16 0)\n\n\nPOINT Z (27189.07 32792.54 0)\n\n\nPOINT Z (28844.56 36773.76 0)\n\n\nPOINT Z (24821.92 46303.16 0)\n\n\nPOINT Z (28637.82 35038.49 0)\n\n\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "title": "Hands-On Exercise 01",
    "section": "8.0 Geoprocessing with sf package",
    "text": "8.0 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n8.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\n8.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNext, I calculate the density of pre-school by planning subzone.\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 01",
    "section": "9.0 Exploratory Data Analysis (EDA)",
    "text": "9.0 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\nUsing ggplot2 method, I plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html",
    "href": "Hands-on_Ex/hands_on04.html",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates.\nComputing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights.\nParticularly, we will explore using spdep, an R package specially designed for spatial weight analysis.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep,\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyverse\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nIn this section, we will carry out exploratory spatial data anlysis.\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nIn this section, we will explore how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area.\nThe poly2nb() function accepts an argument named queen, which can be set to either TRUE or FALSE. This argument plays a pivotal role in determining the criteria used for identifying neighboring regions. If the queen argument is not explicitly specified, the function defaults to TRUE and the function will generate a list of first-order neighbors using the Queen’s contiguity criteria.\n\n\nWe will start out by computing Queen contiguity weight matrix\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. To see the neighbors for the first polygon in the object, we can just use the following code chunk:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class. We can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str() function.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n\nIn previous section, we have created neighbours based on QUEEN contiguity. In this section, we will try ROOK contiguity to create another set of neighbours which we will call wm_r.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.\n\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column. We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]]and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nThen, we will check the first few observations to see if the coordinates are formatted properly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nOnce we have extracted the coordinates of the centroid of each map unit, we will go ahead and plot neighbours maps.\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\n\n\n\n\n\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\npar(mar = c(0,0,1,0),mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\n\nIn this section, we will explore how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band. This can be achieved by following the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nAverage number of links: 3.681818” in the output refers to the average number of neighboring regions each region has within the distance range of 0-62km. In the context of spatial analysis, a “link” is a connection between two regions that are considered neighbors based on the criteria we have set (in this case, distance of up to 62km).\nSo, an average of 3.681818 means that, on average, each region in our study area has about 3 to 4 neighboring regions within a distance of 62 units.\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix. This time, we will combine table() and card() of spdep to display the matrix more neatly than simply using str().\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\n\n\n\n\nThe purple lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotably, it is observed that each county has exactly six neighbours.\n\n\nWe can plot the adoptive weight matrix we obtained in previous section using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"purple\")\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance Weighted (IDW) method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors, we will use the code chunk below.\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n\n\n\n\n\nTip\n\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by by its corresponding weight, (in this case, 0.125) before being tallied.\n\n\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\n\nIn this section, we will explore how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nSpatial lag with row-standardized weights is a measure that represents the average value of a given variable in the neighbouring regions, taking into account the weights assigned to each neighbour. The weights are usually row-standardized, meaning that the weights assigned to the neighbors of each region sum up to 1. This ensures that the spatial lag is not influenced by the number of neighbors a region has.\nIn this section, we will compute the average neighbor GDPPC value for each county.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nOnce the computation is done, we will retrieve the GDPPC values for the five neighboring regions of Anxiang county,as we have explored in section 6.1. To do so, first, we will save these 5 neighbours as nb1 then will extract their respective GDPPC values.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame through following steps.\n\nFirst, we will create a variable called lag.list\nThen, we will calculate the spatial lag value using lag.listw() function that takes in the spatial weights matrix rswm_q, and the GDPPC values for each region in the hunan dataframe, hunan$GDPPC.\nNext, we will convert lag.list into a dataframe named lag.res\nFinally, we will use left_join() function from the dplyr package to merge the hunan dataframe with lag.res dataframe.\n\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nWe will now look at a few values of the newly created column in hunan called lag GDPPC.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nThis spatial lag measure do not consider spatial weights, but simply sums up the values of the variable for all the neighbouring regions.\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist argument in the nb2listw function to explicitly assign these weights.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the binary weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nhead(lag_sum)\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan dataframe by using left_join.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNext, we will plot both the GDPPC and lag_sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSpatial window average is a measure that calculates the average value of a variable within a specified spatial window or area. The spatial window can be defined in various ways, such as a fixed distance from a point or a predefined geographic area.\nIn this section, we will use row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nThis line of code modifies the wm_q neighbor list by including the diagonal element, resulting in the updated neighbor list wm_qs.\n\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of Anxiang county (area [1]) by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nNext, we will plot both the lag_gdppc and w_avg_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSimilar to the spatial window average, this measure sums up the values of a variable within a specified spatial window. When calculating spatial window sum, we do not need to use row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw() function.\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\", fill.palette=\"plasma\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#overview",
    "href": "Hands-on_Ex/hands_on04.html#overview",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates.\nComputing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights.\nParticularly, we will explore using spdep, an R package specially designed for spatial weight analysis."
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#importing-packages",
    "href": "Hands-on_Ex/hands_on04.html#importing-packages",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep,\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyverse\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on04.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on04.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#exploratory-spatial-data-analysis",
    "href": "Hands-on_Ex/hands_on04.html#exploratory-spatial-data-analysis",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will carry out exploratory spatial data anlysis.\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/hands_on04.html#computing-contiguity-spatial-weights",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area.\nThe poly2nb() function accepts an argument named queen, which can be set to either TRUE or FALSE. This argument plays a pivotal role in determining the criteria used for identifying neighboring regions. If the queen argument is not explicitly specified, the function defaults to TRUE and the function will generate a list of first-order neighbors using the Queen’s contiguity criteria.\n\n\nWe will start out by computing Queen contiguity weight matrix\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. To see the neighbors for the first polygon in the object, we can just use the following code chunk:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class. We can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str() function.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n\nIn previous section, we have created neighbours based on QUEEN contiguity. In this section, we will try ROOK contiguity to create another set of neighbours which we will call wm_r.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.\n\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column. We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]]and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nThen, we will check the first few observations to see if the coordinates are formatted properly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nOnce we have extracted the coordinates of the centroid of each map unit, we will go ahead and plot neighbours maps.\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\n\n\n\n\n\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\npar(mar = c(0,0,1,0),mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/hands_on04.html#computing-distance-based-neighbours",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band. This can be achieved by following the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nAverage number of links: 3.681818” in the output refers to the average number of neighboring regions each region has within the distance range of 0-62km. In the context of spatial analysis, a “link” is a connection between two regions that are considered neighbors based on the criteria we have set (in this case, distance of up to 62km).\nSo, an average of 3.681818 means that, on average, each region in our study area has about 3 to 4 neighboring regions within a distance of 62 units.\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix. This time, we will combine table() and card() of spdep to display the matrix more neatly than simply using str().\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\n\n\n\n\nThe purple lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotably, it is observed that each county has exactly six neighbours.\n\n\nWe can plot the adoptive weight matrix we obtained in previous section using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"purple\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#spatial-weights-based-on-inverse-distance-weighted-idw-method",
    "href": "Hands-on_Ex/hands_on04.html#spatial-weights-based-on-inverse-distance-weighted-idw-method",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, you will learn how to derive a spatial weight matrix based on Inversed Distance Weighted (IDW) method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors, we will use the code chunk below.\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n\n\n\n\n\nTip\n\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by by its corresponding weight, (in this case, 0.125) before being tallied.\n\n\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/hands_on04.html#application-of-spatial-weight-matrix",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nSpatial lag with row-standardized weights is a measure that represents the average value of a given variable in the neighbouring regions, taking into account the weights assigned to each neighbour. The weights are usually row-standardized, meaning that the weights assigned to the neighbors of each region sum up to 1. This ensures that the spatial lag is not influenced by the number of neighbors a region has.\nIn this section, we will compute the average neighbor GDPPC value for each county.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nOnce the computation is done, we will retrieve the GDPPC values for the five neighboring regions of Anxiang county,as we have explored in section 6.1. To do so, first, we will save these 5 neighbours as nb1 then will extract their respective GDPPC values.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame through following steps.\n\nFirst, we will create a variable called lag.list\nThen, we will calculate the spatial lag value using lag.listw() function that takes in the spatial weights matrix rswm_q, and the GDPPC values for each region in the hunan dataframe, hunan$GDPPC.\nNext, we will convert lag.list into a dataframe named lag.res\nFinally, we will use left_join() function from the dplyr package to merge the hunan dataframe with lag.res dataframe.\n\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nWe will now look at a few values of the newly created column in hunan called lag GDPPC.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nThis spatial lag measure do not consider spatial weights, but simply sums up the values of the variable for all the neighbouring regions.\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist argument in the nb2listw function to explicitly assign these weights.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the binary weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nhead(lag_sum)\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan dataframe by using left_join.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNext, we will plot both the GDPPC and lag_sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSpatial window average is a measure that calculates the average value of a variable within a specified spatial window or area. The spatial window can be defined in various ways, such as a fixed distance from a point or a predefined geographic area.\nIn this section, we will use row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nThis line of code modifies the wm_q neighbor list by including the diagonal element, resulting in the updated neighbor list wm_qs.\n\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of Anxiang county (area [1]) by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nNext, we will plot both the lag_gdppc and w_avg_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSimilar to the spatial window average, this measure sums up the values of a variable within a specified spatial window. When calculating spatial window sum, we do not need to use row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw() function.\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\", fill.palette=\"plasma\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html",
    "href": "Hands-on_Ex/hands_on07.html",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.\n\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(rgdal, spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#overview",
    "href": "Hands-on_Ex/hands_on07.html#overview",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#analytical-objective",
    "href": "Hands-on_Ex/hands_on07.html#analytical-objective",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#importing-packages",
    "href": "Hands-on_Ex/hands_on07.html#importing-packages",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "The R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(rgdal, spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on07.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html",
    "href": "Hands-on_Ex/hands_on05.html",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "“The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable. The variable can assume values either:\n\nat any point on a continuous surface (such as land use type or annual precipitation levels in a region); \nat a set of fixed sites located within a region (such as prices at a set of retail outlets); or \nacross a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region). \n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001):  \n\nthe difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation;\nmeasurement error;\nspatial diffusion, spillover, interaction, and dispersal processes;\ninheritance by one variable through causal association with another;\nmodel misspecification \n\nIn this hands-on exercise, we will explore how to compute Global and Local Measures of Spatial Autocorrelation by using spdep package.\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep for computing spatial weights, global and local spatial autocorrelation statistics\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyversefor wrangling attribute data in R\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\nMoran’s I and Geary’s c are well known tests for spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function. \nIn this section, we will explore how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation in R environment.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above!\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\n\n\nIn this section, we will explore how to perform Moran’s I statistics testing by using moran.test() of spdep package in R environment.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.300749970, which is significantly different from the expectation under the null hypothesis of -0.011494253. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is 4.7351. This is the value of the Moran’s I statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 1.095e-06, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘greater’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.30075, which is the same as in our previous test (See Section 5.3.1).\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Moran’s I test. There is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using hist() and abline() of R Graphics.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Moran’s I statistics is -0.01504572. This is close to zero, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Moran’s I statistics is 0.004371574. This gives us an idea of the spread of the Moran’s I statistics under the null hypothesis.\nThe minimum and maximum of the simulated Moran’s I statistics are -0.18339 and 0.27593, respectively. This gives us the range of Moran’s I statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are -0.06168, -0.02125, and 0.02611, respectively. These give us more detailed information about the distribution of the Moran’s I statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at 0 to represent the expected value under the null hypothesis.\n\n\nAlternatively, we can also run a similar plot using ggplot2 package.\n\n# Create a data frame with the simulated Moran's I values\ndf &lt;- data.frame(MoranI = bperm$res[1:999])\n\n# Create the histogram\nggplot(df, aes(x = MoranI)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(aes(xintercept = 0.05), color = \"red\", linetype = \"dashed\") +\n  xlab(\"Simulated Moran's I\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Histogram of Simulated Moran's I\")\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nhe Geary’s C statistic is 0.6907223, which is significantly different from the expectation under the null hypothesis of 1.0000000. This suggests that there is a significant local spatial structure in the data.\nThe standard deviate of the Geary’s C statistic is 3.6108. This is the value of the Geary’s C statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 0.0001526, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘Expectation greater than statistic’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Geary’s C statistic is 0.69072, which is the same as in your previous test.\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Geary’s C test. There is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nSimilart to what we did in Moran’s I test, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Geary’s C statistics is 1.004402. This is close to 1, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Geary’s C statistics is 0.007436493. This gives us an idea of the spread of the Geary’s C statistics under the null hypothesis.\nThe minimum and maximum of the simulated Geary’s C statistics are 0.7142 and 1.2722, respectively. This gives us the range of Geary’s C statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are 0.9502, 1.0052, and 1.0595, respectively. These give us more detailed information about the distribution of the Geary’s C statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at Geary’s C = 1 represents the expected value under the null hypothesis of no spatial autocorrelation.\n\n\n\n\n\n\n\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Moran’s I statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, 3, 5, and 6, indicating that the spatial autocorrelation at these distances is statistically significant. For lag 4, however, the p-value is greater than 0.05, indicating that the spatial autocorrelation at this distance is not statistically significant.\nThe Moran’s I statistic is positive for lags 1 to 4, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 5 and 6, the Moran’s I statistic is negative, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Geary’s C statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, and 5, indicating that the spatial autocorrelation at these distances is statistically significant. For lags 3, 4, and 6, however, the p-values are greater than 0.05, indicating that the spatial autocorrelation at these distances is not statistically significant.\nThe Geary’s C statistic is less than 1 for lags 1, 2, and 3, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 4, 5, and 6, the Geary’s C statistic is greater than 1, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\n\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will explore how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(z != E(Ii)): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth shown above is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values. The code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\n\n\nNext, we will plot Moran scatterplot with standardised variables. First we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\n\n\n\n\n\nReflection\n\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\n\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\nNow that we have tried plotting Moran scatterplot, we will proceed to carry out necessary steps for preparing LISA maps. The code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\nNow that we have prepared all necessary steps, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\",\n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\ntmap_arrange(gdppc, LISAmap,localMI.map, pvalue.map, asp=2, ncol=2)\n\n\n\n\n\n\n\n\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas. The term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). Getis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas. It measures the degree of association that results from the concentration of weighted points (or area represented by a weighted point) and all other weighted points included within a radius of distance d from the original weighted point. It identifies areas where high or low cluster in space. \nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\n\n\n\n\n\n\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\n\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n\n\n\n\nReflection\n\n\n\nthe code chunk above performs three tasks. First, it convert the output vector gi.fixed into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (fixed)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap_adaptive &lt;-tm_shape(hunan.gi) +\n  tm_fill(\"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (adaptive)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_adaptive, asp=1, ncol=2)\n\n\n\n\n\n\n\n\ntmap_arrange(Gimap, Gimap_adaptive, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#overview",
    "href": "Hands-on_Ex/hands_on05.html#overview",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "“The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable. The variable can assume values either:\n\nat any point on a continuous surface (such as land use type or annual precipitation levels in a region); \nat a set of fixed sites located within a region (such as prices at a set of retail outlets); or \nacross a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region). \n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001):  \n\nthe difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation;\nmeasurement error;\nspatial diffusion, spillover, interaction, and dispersal processes;\ninheritance by one variable through causal association with another;\nmodel misspecification \n\nIn this hands-on exercise, we will explore how to compute Global and Local Measures of Spatial Autocorrelation by using spdep package.\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#importing-packages",
    "href": "Hands-on_Ex/hands_on05.html#importing-packages",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep for computing spatial weights, global and local spatial autocorrelation statistics\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyversefor wrangling attribute data in R\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on05.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on05.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/hands_on05.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Moran’s I and Geary’s c are well known tests for spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function. \nIn this section, we will explore how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation in R environment.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above!\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\n\n\nIn this section, we will explore how to perform Moran’s I statistics testing by using moran.test() of spdep package in R environment.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.300749970, which is significantly different from the expectation under the null hypothesis of -0.011494253. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is 4.7351. This is the value of the Moran’s I statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 1.095e-06, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘greater’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.30075, which is the same as in our previous test (See Section 5.3.1).\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Moran’s I test. There is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using hist() and abline() of R Graphics.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Moran’s I statistics is -0.01504572. This is close to zero, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Moran’s I statistics is 0.004371574. This gives us an idea of the spread of the Moran’s I statistics under the null hypothesis.\nThe minimum and maximum of the simulated Moran’s I statistics are -0.18339 and 0.27593, respectively. This gives us the range of Moran’s I statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are -0.06168, -0.02125, and 0.02611, respectively. These give us more detailed information about the distribution of the Moran’s I statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at 0 to represent the expected value under the null hypothesis.\n\n\nAlternatively, we can also run a similar plot using ggplot2 package.\n\n# Create a data frame with the simulated Moran's I values\ndf &lt;- data.frame(MoranI = bperm$res[1:999])\n\n# Create the histogram\nggplot(df, aes(x = MoranI)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(aes(xintercept = 0.05), color = \"red\", linetype = \"dashed\") +\n  xlab(\"Simulated Moran's I\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Histogram of Simulated Moran's I\")\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nhe Geary’s C statistic is 0.6907223, which is significantly different from the expectation under the null hypothesis of 1.0000000. This suggests that there is a significant local spatial structure in the data.\nThe standard deviate of the Geary’s C statistic is 3.6108. This is the value of the Geary’s C statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 0.0001526, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘Expectation greater than statistic’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Geary’s C statistic is 0.69072, which is the same as in your previous test.\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Geary’s C test. There is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nSimilart to what we did in Moran’s I test, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Geary’s C statistics is 1.004402. This is close to 1, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Geary’s C statistics is 0.007436493. This gives us an idea of the spread of the Geary’s C statistics under the null hypothesis.\nThe minimum and maximum of the simulated Geary’s C statistics are 0.7142 and 1.2722, respectively. This gives us the range of Geary’s C statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are 0.9502, 1.0052, and 1.0595, respectively. These give us more detailed information about the distribution of the Geary’s C statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at Geary’s C = 1 represents the expected value under the null hypothesis of no spatial autocorrelation."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#spatial-correlogram-for-global-spatial-autocorrelation-statistics",
    "href": "Hands-on_Ex/hands_on05.html#spatial-correlogram-for-global-spatial-autocorrelation-statistics",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Moran’s I statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, 3, 5, and 6, indicating that the spatial autocorrelation at these distances is statistically significant. For lag 4, however, the p-value is greater than 0.05, indicating that the spatial autocorrelation at this distance is not statistically significant.\nThe Moran’s I statistic is positive for lags 1 to 4, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 5 and 6, the Moran’s I statistic is negative, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Geary’s C statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, and 5, indicating that the spatial autocorrelation at these distances is statistically significant. For lags 3, 4, and 6, however, the p-values are greater than 0.05, indicating that the spatial autocorrelation at these distances is not statistically significant.\nThe Geary’s C statistic is less than 1 for lags 1, 2, and 3, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 4, 5, and 6, the Geary’s C statistic is greater than 1, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#local-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/hands_on05.html#local-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will explore how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(z != E(Ii)): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth shown above is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values. The code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/hands_on05.html#creating-a-lisa-cluster-map",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\n\n\nNext, we will plot Moran scatterplot with standardised variables. First we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\n\n\n\n\n\nReflection\n\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\n\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\nNow that we have tried plotting Moran scatterplot, we will proceed to carry out necessary steps for preparing LISA maps. The code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\nNow that we have prepared all necessary steps, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\",\n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\ntmap_arrange(gdppc, LISAmap,localMI.map, pvalue.map, asp=2, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/hands_on05.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas. The term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). Getis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas. It measures the degree of association that results from the concentration of weighted points (or area represented by a weighted point) and all other weighted points included within a radius of distance d from the original weighted point. It identifies areas where high or low cluster in space. \nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\n\n\n\n\n\n\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\n\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n\n\n\n\nReflection\n\n\n\nthe code chunk above performs three tasks. First, it convert the output vector gi.fixed into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (fixed)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap_adaptive &lt;-tm_shape(hunan.gi) +\n  tm_fill(\"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (adaptive)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_adaptive, asp=1, ncol=2)\n\n\n\n\n\n\n\n\ntmap_arrange(Gimap, Gimap_adaptive, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html",
    "href": "Hands-on_Ex/hands_on03.html",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This hands-on exercise will explore the basic concepts and methods of Spatial Point Pattern Analysis.\nParticularly, we will explore using spatstat, an R package specially designed for Spatial Point Pattern Analysis.\n\n\nToday, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?\n\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, devtools,sp)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")%&gt;% st_transform(crs =3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"../data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 338\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.653796e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)\n\n\n\n\n\n\n\n\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n306.6986 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\ntm = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\nbp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"BUKIT PANJANG\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.51429, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.7747, p-value = 0.002306\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#overview",
    "href": "Hands-on_Ex/hands_on03.html#overview",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Today, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-packages",
    "href": "Hands-on_Ex/hands_on03.html#importing-packages",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, devtools,sp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")%&gt;% st_transform(crs =3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"../data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "childcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 338\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.653796e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "href": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "After data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n306.6986 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\ntm = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\nbp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"BUKIT PANJANG\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.51429, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.7747, p-value = 0.002306\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html",
    "href": "Hands-on_Ex/hands_on02.html",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#overview",
    "href": "Hands-on_Ex/hands_on02.html#overview",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-packages",
    "href": "Hands-on_Ex/hands_on02.html#importing-packages",
    "title": "Hands-On Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nSince readr, tidyr and dplyr are part of tidyverse package, we will only need to install and import tidyverse.\n\npacman::p_load(sf, tmap, tidyverse)\ndevtools::install_github(\"thomasp85/patchwork\")\n\nSkipping install of 'patchwork' from a github remote, the SHA1 (d9437579) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(patchwork)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "href": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "title": "Hands-On Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use two datasets as follows:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\nFor geospatial data, we will use st_read() function of sf package to import shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"~/IS415-GAA/data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "title": "Hands-On Exercise 02",
    "section": "4.0 Data Preparation and Wrangling",
    "text": "4.0 Data Preparation and Wrangling\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n4.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n4.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nHence, we will standard the data values in these two fields.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 02",
    "section": "5.0 Choropleth Mapping Geospatial Data Using tmap",
    "text": "5.0 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n5.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\",\n    fill.palette =\"plasma\")\n\n\n\n\n\n\n5.2 Plotting a choropleth map quickly by using qtm()\nHowever, in real-life application, the quick choropleth map produced in the previous session may not be sufficient enough to properly visualize geospatial data. However, tmap packages allow us to customise and control how we design our choropleth maps. We will exploit tmap’s drawing elements to create a high quality cartographic choropleth map that includes more accurate and informative information.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nNext, we will breakdown the different tmpa functions used to plot the additional elements in the map above.\n\n\n5.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e.DEPENDENCY)\n\n\n\n\n\n5.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n5.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "title": "Hands-On Exercise 02",
    "section": "6.0 Data Classification Methods of tmap",
    "text": "6.0 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher, and\njenks.\n\n\n6.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNext, we will try equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nNext, we will try other data classification methods.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\nAlso, we can try exploring using the same classification methods, but with different numbers of classes. As an example, we will use kmeans clustering method with different class sizes (2,6,10,20)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\noriginal &lt;- ggplot(data=mpsz_pop2020, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\n#Try to remove outliers\nmpsz_pop2020_no_outlier &lt;- subset(mpsz_pop2020, mpsz_pop2020$DEPENDENCY &lt;3)\n\nfiltered &lt;- ggplot(data=mpsz_pop2020_no_outlier, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\noriginal + filtered\n\n\n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 1.00. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we use the above-mentioned breaks?\nThe reason behind choosing those break points is mainly stemmed from the 1st quantile and 3rd quantile of the datasets. While the minimum value is 0.10 and maximum value is 19.0, the 1st quantile (the value under which 25% of data points are found) is 0.7147 and the 3rd quantile (the value under which 75% of data points are found) is 0.8763. Using these two values, we may assume that the dataset might have outliers on the right end, and the majority of the dataset might be scattered in the range of 0.7147 and 0.8763. Hence, we use the mentioned break points.\nOtherwise, we can use non-heuristic approach in this case as well. We can easily plot the data to see the distribution first (see above). As we assumed earlier, you can clearly see the outliers on the right-side of the histogram. After removing the outliers (temporarily), we can see the new plot (see above). Majority of the datasets are scattered within the range of 0.6 - 1.0. This is why we break the datasets into 0.6, 0.7, 0.8 and 0.9 respectively so that there is balanced quantity of data points in each break.\n\n\nUsing this information, we will now proceed to plot the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "href": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "title": "Hands-On Exercise 02",
    "section": "7.0 Controlling and Customizing Map Layots",
    "text": "7.0 Controlling and Customizing Map Layots\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n7.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-On Exercise 02",
    "section": "8.0 Drawing Small Multiple Choropleth Maps",
    "text": "8.0 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-On Exercise 02",
    "section": "9.0 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "9.0 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\nFor example, we have select the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#references",
    "href": "Hands-on_Ex/hands_on02.html#references",
    "title": "Hands-On Exercise 02",
    "section": "10. References",
    "text": "10. References\nTutorial provided by Professor Kam Tin Seong©, Singapore Management University\nReference: https://r4gdsa.netlify.app/chap02.html\n\n10.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n10.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n10.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "library_workship.html#why-are-you-here",
    "href": "library_workship.html#why-are-you-here",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Why are you here?",
    "text": "Why are you here?\n\nBecause you are :\n\ntired of creating PowerPoint slides\ninterested to create awesome web slides using R"
  },
  {
    "objectID": "library_workship.html#hello-plot",
    "href": "library_workship.html#hello-plot",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Hello Plot",
    "text": "Hello Plot\n\n\n\n\npacman::p_load(tidyverse)\nexam_data &lt;- read_csv(\"data/library/Exam_data.csv\")\n\nggplot(data=exam_data, aes(x=MATHS))+\n  geom_histogram(bins=10,\n                 boundary =100,\n                 color=\"white\",\n                 fill =\"grey\")+\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "library_workship.html#working-with-tabsets",
    "href": "library_workship.html#working-with-tabsets",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Working with Tabsets",
    "text": "Working with Tabsets\n\nCode Chunk (Highlighted Lines)Plot\n\n\n\n\npacman::p_load(tidyverse)\nexam_data &lt;- read_csv(\"data/library/Exam_data.csv\")\n\nggplot(data=exam_data, aes(x=MATHS))+\n  geom_histogram(bins=10,\n                 boundary =100,\n                 color=\"white\",\n                 fill =\"grey\")+\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "library_workship.html#section",
    "href": "library_workship.html#section",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "",
    "text": "Adding Table Static html table\n\nhead(exam_data) %&gt;%\n  knitr::kable(format=\"html\")\n\n\n\n\nID\nCLASS\nGENDER\nRACE\nENGLISH\nMATHS\nSCIENCE\n\n\n\n\nStudent321\n3I\nMale\nMalay\n21\n9\n15\n\n\nStudent305\n3I\nFemale\nMalay\n24\n22\n16\n\n\nStudent289\n3H\nMale\nChinese\n26\n16\n16\n\n\nStudent227\n3F\nMale\nChinese\n27\n77\n31\n\n\nStudent318\n3I\nMale\nMalay\n27\n11\n25\n\n\nStudent306\n3I\nFemale\nMalay\n31\n16\n16"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I am Khant Min Naing, an undergraduate student at Singapore Management University. This course website is developed to document my learning journey at IS415: Geospatial Analytics and Applications under Professor Kam Tin Seong. Follow my journey as I venture into the world of big data, geospatial analysis and urban planning.\nConnect with me!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "",
    "text": "Dengue Hemorrhagic Fever, also referred to as dengue fever in short, is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. Historically, the majority of outbreaks during the first half of the 20th century were reported in East Asia and Western Pacific countries (Chen, 2018). In 2015, Taiwan experienced its most severe outbreak of dengue fever, recording over 43,000 cases and 228 fatalities (Yi-ning & Kao, 2023). Subsequently, the annual reported cases of dengue fever remained below 200. However, in 2023, there was a significant increase with 26,703 recorded cases. Notably, the majority of these outbreaks were reported in Tainan City, situated in the southern part of the island.\nThis study aims to use spatio-temporal analysis techniques to analyze if the distribution of dengue fever outbreak in Tainan City are independent from space and time. The study also attempts to identify clusters and outliers, as well as emerging hot spots/cold spots within the study area. The initial phase of the study utilizes Global and Local Spatial Autocorrelation modelling to discern the spatio-temporal pattern of the disease outbreak. Subsequently, Emerging Hot Spots Analysis (EHSA) is applied to identify spatio-temporal clusters. The results of this study will provide valuable insights into the spatial and temporal dynamics of dengue fever outbreak in Tainan City. By identifying clusters and outliers, as well as emerging hot spots and cold spots, it will help inform targeted interventions and prevention strategies to effectively control the spread of the disease."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatial-autocorrelation",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.1 Spatial Autocorrelation",
    "text": "2.1 Spatial Autocorrelation\nThe first law of geography states that everything is related to everything else but near things are more related than distant things (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable, as explained by Tobler’s first law. The concept of spatial autocorrelation, although it may be viewed as a special case of correlation, has a meaning all its own. Whereas correlation statistics were designed to show relationships between or among variables, spatial autocorrelation shows the correlation within variables across georeferenced space (Getis, 2009).\nThe variable can assume values either at any point on a continuous surface (such as land use type or annual precipitation levels in a region); at a set of fixed sites located within a region (such as prices at a set of retail outlets); or across a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region).\n\n\n\nExamples of configurations of areas showing different types of spatial autocorrelation (Ref: Moraga, 2023)\n\n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001): a) the difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation; b) measurement error; c) spatial diffusion, spillover, interaction, and dispersal processes; d) inheritance by one variable through causal association with another; e) model misspecification.\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Cliff and Ord (1973) demonstrated statistically how one can test residuals of regression analysis for spatial randomness by using spatial autocorrelation statistics. These spatial statistics are well suited for a number of geospatial analysis such as:\n\ndetecting clusters or outliers: spatial clustering algorithms are dependent on the conjecture that there is spatial autocorrelation among some nearby values of one or more variables of interest.\nmeasuring the strength of spatial effect on variable: spatial autocorrelation cofficients in regression models help us to understand the strength of spatial effects (Haining, 1990; Anselin and Rey, 1991). \nassessing the assumptions of stationarity: spatial autocorrelation measures allow for tests on hypotheses of no spatial differences in distribution parameters such as the mean and variance. (Haining, 1977; Leung et al., 2000).\ndesigning an appropriate spatial sample: if the goal is to avoid, as much as possible, spatial autocorrelation in the sample, then a reasonable sample design would benefit from a study of spatial autocorrelation in the region where the sample is selected (:egendre & Fortin, 1989; Legendre et al., 2002; Griffith, 2005).\n\nMeasures of spatial autocorrelation can be categorized as global or local indicators of spatial association (LISA). Moran’s I (Moran, 1950) and Geary’s C (Geary, 1954) are well known tests for global spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function.\nLISA statistics quantify the degree to which points in proximity to a given point exhibit similar values, based on a measure of contiguity within a defined radius. This makes them instrumental in identifying local spatial autocorrelation (Anselin, 1995). Global spatial autocorrelation statistics such as Moran’s I and Geary’s C can be further decomposed and used as LISA statistics to analyse local spatial autocorrelation as well. Another widely employed LISA statistic in spatial research is the Getis-Ord Gi* statistic (Getis & Ord, 1992). The Gi* statistic is essentially a ratio of the sum of values within a specified area to the global total, providing a measure of local concentration relative to the overall distribution."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.2 Emerging Hotspot Analysis",
    "text": "2.2 Emerging Hotspot Analysis\nA hotspot is defined as an area exhibiting a higher concentration of events than would be expected under a random distribution. This concept, initially rooted in the study of point distributions and spatial arrangements, has significantly evolved (Chakravorty, 1995). There are different methods for analyzing spatial patterns and detecting hotspots including spatial autocorrelation and cluster analysis(). Emerging Hot Spot Analysis (EHSA) is a specialised spatio-temporal technique for analysing hotspots over observation time period. It combines two established methods: the traditional Getis-Ord Gi* statistic for hotspot analysis and the time-series Mann-Kendall test for monotonic trends. EHSA’s primary objective is to evaluate the temporal changes in hot and cold spots, specifically addressing whether these spots are intensifying, diminishing, or remaining stable (Parry & Locke, 2022).\nEHSA works by calculating the Gi* for each time period. The series of Gi* at each location is treated as a time-series and evaluated for a trend using the Mann-Kendall statistic. The Gi* and the Mann-Kendall are compared together to create 17 unique classifications base on ESRI’s emerging hot spot classification criteria to help better understand how the locations have changed over time.\n\n\n\n\n\n\n\nPattern Name\nDefinition\n\n\n\n\nNo Pattern Detected\nDoes not fall into any of the hot or cold spot patterns defined below.\n\n\nNew Hot Spot\nA location that is statistically significant hot spot for the final time step and has never been a statistically significant hot spot before.\n\n\nConsecutive Hot Spot\nA location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.\n\n\nIntensifying Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of high counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Hot Spot\nA location that has been statistically hot spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering over time.\n\n\nDiminishing Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Hot Spot\nA statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically cold spots.\n\n\nOscillating Hot Spot\nA statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nHistorical Hot Spot\nThe most recent time period is not hot, but at least 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nNew Cold Spot\nA location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.\n\n\nConsecutive Cold Spot\nA location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.\n\n\nIntensifying Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.\n\n\nDiminishing Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Cold Spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\nOscillating Cold Spot\nA statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.\n\n\nHistorical Cold Spot\nThe most recent time period is not cold, but at least 90 percent of the time-step intervals have been statistically significant cold spots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatio-temporal-analytics-in-epidemiological-studies",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatio-temporal-analytics-in-epidemiological-studies",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.3 Spatio-Temporal Analytics in Epidemiological Studies",
    "text": "2.3 Spatio-Temporal Analytics in Epidemiological Studies\nSpatial autocorrelation and hotspot analysis play a crucial role in epidemiological studies by enabling to discover and analyse the distribution and clustering of transmissible diseases.These diseases, due to their transmission dynamics, often exhibit distinct spatial patterns and commonly occur in spatial clusters (Mergenthaler et al., 2022). Understanding and quantifying spatial autocorrelation is a pivotal aspect of epidemiological research. It provides insights into the dynamics and spread of diseases. Moreover, it is essential for the statistical testing of epidemiological risk factors (Mergenthaler et al., 2022).\nSpatial autocorrelation can also be leveraged to detect spatiotemporal clustering and outliers, thereby aiding in the identification of disease hotspots and the targeting of control measures. Numerous works in this area integrate the temporal component of emerging hotspot analysis with the spatial component of autocorrelation analysis. Emerging hotspot analysis helps in identifying areas that are experiencing a significant increase in disease incidence over time. Singh et al. (2023) highlighted in particular how understanding the spatiotemporal patterns of infectious illnesses fever provides important insights on the spread of the disease and how it relates to local risk factors.\nOverall, spatial autocorrelation and emerging hotspot analysis provide valuable insights into the spatial patterns and dynamics of epidemiological studies of communicable diseases. Consequently, this understanding holds promise for developing forecasting models to optimise resource allocation and plan effective vector control measures (Singh et al., 2023) in low- and middle-income countries."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#datasets",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.1 Datasets",
    "text": "4.1 Datasets\nIn this exercise, we will use two datasets. The first dataset, TAIWAN_VILLAGE_2020, is a geospatial dataset that delineates the village boundaries of Taiwan. This data is presented in the ESRI shapefile format and is based on the Taiwan Geographic Coordinate System. This data is extracted from Historical map data of the village boundary: TWD97 longitude and latitude.\nThe second dataset, Dengue_Daily.csv, is an aspatial dataset that contains records of reported dengue cases in Taiwan since 1998. This data is extracted from Dengue Daily Confirmed Cases Since 1998."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-geospatial-data",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.2 Importing Geospatial Data",
    "text": "4.2 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import TAIWAN_VILLAGE_2020 dataset into R environment.\n\ntainan &lt;- st_read(\"~/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.shp\") \n\nReading layer `TAINAN_VILLAGE' from data source \n  `/Users/khantminnaing/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\n\n\nWe will verify the coordinate reference systems of the tainan object to ensure the assignment of the correct CRS value.\n\nst_crs(tainan)\n\nCoordinate Reference System:\n  User input: TWD97 \n  wkt:\nGEOGCRS[\"TWD97\",\n    DATUM[\"Taiwan Datum 1997\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.\"],\n        BBOX[17.36,114.32,26.96,123.61]],\n    ID[\"EPSG\",3824]]\n\n\nNext, we will generate a plot of the tainan object to visualise its structure."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-aspatial-data",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.3 Importing Aspatial Data",
    "text": "4.3 Importing Aspatial Data\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\ndengue_daily &lt;- read_csv(\"~/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/aspatial/Dengue_Daily.csv\")\ndengue_daily\n\n# A tibble: 106,861 × 26\n   發病日     個案研判日 通報日     性別  年齡層 居住縣市 居住鄉鎮 居住村里\n   &lt;date&gt;     &lt;chr&gt;      &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1 1998-01-02 None       1998-01-07 男    40-44  屏東縣   屏東市   None    \n 2 1998-01-03 None       1998-01-14 男    30-34  屏東縣   東港鎮   None    \n 3 1998-01-13 None       1998-02-18 男    55-59  宜蘭縣   宜蘭市   None    \n 4 1998-01-15 None       1998-01-23 男    35-39  高雄市   苓雅區   None    \n 5 1998-01-20 None       1998-02-04 男    55-59  宜蘭縣   五結鄉   None    \n 6 1998-01-22 None       1998-02-19 男    20-24  桃園市   蘆竹區   None    \n 7 1998-01-23 None       1998-02-02 男    40-44  新北市   新店區   None    \n 8 1998-01-26 None       1998-02-19 女    65-69  台北市   北投區   None    \n 9 1998-02-11 None       1998-02-13 女    25-29  台南市   南區     None    \n10 1998-02-16 None       1998-02-24 男    20-24  高雄市   楠梓區   None    \n# ℹ 106,851 more rows\n# ℹ 18 more variables: 最小統計區 &lt;chr&gt;, 最小統計區中心點X &lt;chr&gt;,\n#   最小統計區中心點Y &lt;chr&gt;, 一級統計區 &lt;chr&gt;, 二級統計區 &lt;chr&gt;,\n#   感染縣市 &lt;chr&gt;, 感染鄉鎮 &lt;chr&gt;, 感染村里 &lt;chr&gt;, 是否境外移入 &lt;chr&gt;,\n#   感染國家 &lt;chr&gt;, 確定病例數 &lt;dbl&gt;, 居住村里代碼 &lt;chr&gt;, 感染村里代碼 &lt;chr&gt;,\n#   血清型 &lt;chr&gt;, 內政部居住縣市代碼 &lt;chr&gt;, 內政部居住鄉鎮代碼 &lt;chr&gt;,\n#   內政部感染縣市代碼 &lt;chr&gt;, 內政部感染鄉鎮代碼 &lt;chr&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#understanding-administrative-division-in-taiwan",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#understanding-administrative-division-in-taiwan",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.1 Understanding Administrative Division in Taiwan",
    "text": "5.1 Understanding Administrative Division in Taiwan\nBefore we carry out data wrangling, it is of utmost importance to contexualise ourselves with the nature of datasets. This is particularly important when conducting spatial analysis, as understanding the nature of local spatial information, such as the administrative division, is key to accurate and meaningful results.\nIn this section, we will explore administrative division of Taiwan.\n\n\n\n\n\nAccording to the Local Government Act, the local government in the ROC (Taiwan) is subdivided into provinces and special municipalities.\n\n\n\n\n\n\n5.1.1 First Level: Special municipalities, counties, and cities\nCurrently there are three types and in total 22 administrative divisions are directly governed by the central government.\n\n\n\n\n\n\n\n\n\nSpecial Municipality (直轄市) : Currently, Taiwan comprises 6 special municipalities - Taipei, New Taipei, Taoyuan, Taichung, Tainan, and Kaohsiung. [Colored in Red]\n\nProvinces are sub-divided into County (縣) & City (市) |\n\nCounty (縣): Currently, Taiwan comprises 13 counties - Hsinchu, Miaoli, Changhua, Nantou, Yunlin, Chiayi, Pingtung, Yilan, Hualien, Taitung, Penghu, Kinmen, and Lienchiang. [Colored in Green]\nCity (市) : Currently, Taiwan comprises 3 cities - Keelung, Hsinchu, and Chiayi. [Colored in Purple]\n\n\n\n\nThe 22 main divisions in the country are further divided into 368 subdivisions. These 368 divisions can be categorized as the following.\n\n\n5.1.2 Second Level: Townships, county-administered cities and districts\nThe 22 main divisions in previous level are further divided into 368 subdivisions. These 368 divisions can be categorized as the following.\n\n\n\n\n\n\n\n\n\nRural Township (鄉) : Currently, Taiwan comprises 122 rural townships.\nMountain Indigenous Township (山地鄉) : Currently, Taiwan comprises 24 mountain indigenous townships.\nUrban Township (鎮) : Currently, Taiwan comprises 38 urban townships.\nCounty-administered City (縣轄市) : Currently, Taiwan comprises 14 county-administered cities.\nMountain Indigenous District (原住民區) : Currently, Taiwan comprises 6 mountain indigenous district.\nDistrict (區): Currently, Taiwan comprises 164 districts.\n\n\n\n\n\n\n5.1.3 Lower-Level Administrative Divisions\nThe 368 divisions in previous level are further divided into villages and neighborhoods.\n\n\n\n\n\n\n\n\n\nRural/Urban Village (村里) : Currently, Taiwan comprises 7,835 rural and urban villages.\nNeighbourhood (鄰) : Currently, Taiwan comprises 147,877 neighbourhoods."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#translating-dataset-columns-of-dengue_daily-object",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#translating-dataset-columns-of-dengue_daily-object",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.2 Translating Dataset Columns of dengue_daily object",
    "text": "5.2 Translating Dataset Columns of dengue_daily object\nNotice that the data columns are currently represented in traditional Chinese. To enhance clarity in subsequent analysis, we will update the column names to English. It is an essential step in making data analysis more reproducible. In R, the colnames() function can be utilized to rename the columns. Before that, we will explicitly print out the column names.\n\ncolnames(dengue_daily)\n\n [1] \"發病日\"             \"個案研判日\"         \"通報日\"            \n [4] \"性別\"               \"年齡層\"             \"居住縣市\"          \n [7] \"居住鄉鎮\"           \"居住村里\"           \"最小統計區\"        \n[10] \"最小統計區中心點X\"  \"最小統計區中心點Y\"  \"一級統計區\"        \n[13] \"二級統計區\"         \"感染縣市\"           \"感染鄉鎮\"          \n[16] \"感染村里\"           \"是否境外移入\"       \"感染國家\"          \n[19] \"確定病例數\"         \"居住村里代碼\"       \"感染村里代碼\"      \n[22] \"血清型\"             \"內政部居住縣市代碼\" \"內政部居住鄉鎮代碼\"\n[25] \"內政部感染縣市代碼\" \"內政部感染鄉鎮代碼\"\n\n\nNext we will translate these column names into English using references from administrative divisions of Taiwan as well as the language translation of Google Translate (Chinese Traditional to English). Each literal translation is cross-referenced and cross-checked to be accurate and intuitive.\n\n\n\n\n\n\n\n\nColumn Name (Chinese)\nData Type\nColumn Name To-Be-Converted (English)\n\n\n\n\n發病日\n&lt;date&gt;\nOnset_Date\n\n\n個案研判日\n&lt;chr&gt;\nTesting_Date\n\n\n通報日\n&lt;date&gt;\nNotification_Date\n\n\n性別\n&lt;chr&gt;\nGender\n\n\n年齡層\n&lt;chr&gt;\nAge_Group\n\n\n居住縣市\n&lt;chr&gt;\nCountyCity_Residence\n\n\n居住鄉鎮\n&lt;chr&gt;\nRuralUrbanTownship_Residence\n\n\n居住村里\n&lt;chr&gt;\nRuralUrbanVillage_Residence\n\n\n最小統計區\n&lt;chr&gt;\nGrid_Area\n\n\n最小統計區中心點X\n&lt;chr&gt;\nX_Coordinate\n\n\n最小統計區中心點Y\n&lt;chr&gt;\nY_Coordinate\n\n\n一級統計區\n&lt;chr&gt;\nPrimary_Statistical_Area\n\n\n二級統計區\n&lt;chr&gt;\nSecondary_Statistical_Area\n\n\n感染縣市\n&lt;chr&gt;\nCountyCity_Exposure\n\n\n感染鄉鎮\n&lt;chr&gt;\nRuralUrbanTownship_Exposure\n\n\n感染村里\n&lt;chr&gt;\nRuralUrbanVillage_Exposure\n\n\n是否境外移入\n&lt;chr&gt;\nImported_Case\n\n\n感染國家\n&lt;chr&gt;\nCountry_Infection_Origin\n\n\n確定病例數\n&lt;num&gt;\nNo_Infected_Cases\n\n\n居住村里代碼\n&lt;chr&gt;\nCode_RuralUrbanVillage_Residence\n\n\n感染村里代碼\n&lt;chr&gt;\nCode_RuralUrbanTownship_Residence\n\n\n血清型\n&lt;chr&gt;\nSerotype\n\n\n內政部居住縣市代碼\n&lt;chr&gt;\nMOI_Code_CountyCity_Residence\n\n\n內政部居住鄉鎮代碼\n&lt;chr&gt;\nMOI_Code_RuralUrbanTownship_Residence\n\n\n內政部感染縣市代碼\n&lt;chr&gt;\nMOI_Code_CountyCity_Exposure\n\n\n內政部感染鄉鎮代碼\n&lt;chr&gt;\nMOI_Code_RuralUrbanTownship_Exposure\n\n\n\n\ncolnames(dengue_daily) &lt;- c(\"Onset_Date\",\"Testing_Date\",\"Notification_Date\",\"Gender\",\"Age_Group\",\"CountyCity_Residence\",\"RuralUrbanTownship_Residence\",\"RuralUrbanVillage_Residence\",\"Grid_Area\",\"X_Coordinate\",\"Y_Coordinate\",\"Primary_Statistical_Area\",\"Secondary_Statistical_Area\",\"CountyCity_Exposure\",\"RuralUrbanTownship_Exposure\",\"RuralUrbanVillage_Exposure\",\"Imported_Case\",\"Country_Infection_Origin\",\"No_Infected_Cases\",\"Code_RuralUrbanVillage_Residence\",\"Code_RuralUrbanTownship_Residence\",\"Serotype\",\"MOI_Code_CountyCity_Residence\",\"MOI_Code_RuralUrbanTownship_Residence\",\"MOI_Code_CountyCity_Exposure\n\",\"MOI_Code_RuralUrbanTownship_Exposure\")\n\n\nhead(dengue_daily)\n\n# A tibble: 6 × 26\n  Onset_Date Testing_Date Notification_Date Gender Age_Group\n  &lt;date&gt;     &lt;chr&gt;        &lt;date&gt;            &lt;chr&gt;  &lt;chr&gt;    \n1 1998-01-02 None         1998-01-07        男     40-44    \n2 1998-01-03 None         1998-01-14        男     30-34    \n3 1998-01-13 None         1998-02-18        男     55-59    \n4 1998-01-15 None         1998-01-23        男     35-39    \n5 1998-01-20 None         1998-02-04        男     55-59    \n6 1998-01-22 None         1998-02-19        男     20-24    \n# ℹ 21 more variables: CountyCity_Residence &lt;chr&gt;,\n#   RuralUrbanTownship_Residence &lt;chr&gt;, RuralUrbanVillage_Residence &lt;chr&gt;,\n#   Grid_Area &lt;chr&gt;, X_Coordinate &lt;chr&gt;, Y_Coordinate &lt;chr&gt;,\n#   Primary_Statistical_Area &lt;chr&gt;, Secondary_Statistical_Area &lt;chr&gt;,\n#   CountyCity_Exposure &lt;chr&gt;, RuralUrbanTownship_Exposure &lt;chr&gt;,\n#   RuralUrbanVillage_Exposure &lt;chr&gt;, Imported_Case &lt;chr&gt;,\n#   Country_Infection_Origin &lt;chr&gt;, No_Infected_Cases &lt;dbl&gt;, …\n\n\nAfter translating the column names, we can better understand what each data column is. To reduce the memory load, we can drop columns which are not relevant for this study and store only relevant columns.\n\ndengue_daily &lt;- subset(dengue_daily, select = c(Onset_Date, CountyCity_Residence, RuralUrbanTownship_Residence, RuralUrbanVillage_Residence, X_Coordinate, Y_Coordinate))\n\ndengue_daily\n\n# A tibble: 106,861 × 6\n   Onset_Date CountyCity_Residence RuralUrbanTownship_R…¹ RuralUrbanVillage_Re…²\n   &lt;date&gt;     &lt;chr&gt;                &lt;chr&gt;                  &lt;chr&gt;                 \n 1 1998-01-02 屏東縣               屏東市                 None                  \n 2 1998-01-03 屏東縣               東港鎮                 None                  \n 3 1998-01-13 宜蘭縣               宜蘭市                 None                  \n 4 1998-01-15 高雄市               苓雅區                 None                  \n 5 1998-01-20 宜蘭縣               五結鄉                 None                  \n 6 1998-01-22 桃園市               蘆竹區                 None                  \n 7 1998-01-23 新北市               新店區                 None                  \n 8 1998-01-26 台北市               北投區                 None                  \n 9 1998-02-11 台南市               南區                   None                  \n10 1998-02-16 高雄市               楠梓區                 None                  \n# ℹ 106,851 more rows\n# ℹ abbreviated names: ¹​RuralUrbanTownship_Residence,\n#   ²​RuralUrbanVillage_Residence\n# ℹ 2 more variables: X_Coordinate &lt;chr&gt;, Y_Coordinate &lt;chr&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-subsetting-to-study-area",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-subsetting-to-study-area",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.3 Data Subsetting to Study Area",
    "text": "5.3 Data Subsetting to Study Area\nFor this study, we have identified a total of 8 townships/district that we want to investigate as below.\n\n\n\nTOWNID\nTOWNCODE\nChinese\n\n\n\n\nD01\n67000320\n東區\n\n\nD02\n67000330\n南區\n\n\nD04\n67000340\n北區\n\n\nD06\n67000350\n安南區\n\n\nD07\n67000360\n安平區\n\n\nD08\n67000370\n中西區\n\n\nD32\n67000270\n仁德區\n\n\nD39\n67000310\n永康區\n\n\n\nNow, we will filter the tainan data frame to only include selected townships we have identified above. Firstly, we will create a character vector named township_list that contains the codes of selected counties. Then, we will use filter function to filter the rows of the tainan data frame where the TOWNID is in the township_list and create a new sf object called study_area_sf.\n\ntownship_list &lt;- c(\"D01\", \"D02\", \"D04\", \"D06\", \"D07\", \"D08\", \"D32\", \"D39\") \n\nstudy_area_sf &lt;- tainan %&gt;%\n  filter(TOWNID %in% township_list)\n\nstudy_area_sf\n\nSimple feature collection with 258 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\nFirst 10 features:\n      VILLCODE COUNTYNAME TOWNNAME VILLNAME       VILLENG COUNTYID COUNTYCODE\n1  67000350032     臺南市   安南區   青草里  Qingcao Vil.        D      67000\n2  67000270011     臺南市   仁德區   保安里   Bao'an Vil.        D      67000\n3  67000370005     臺南市   中西區   赤嵌里  Chihkan Vil.        D      67000\n4  67000330004     臺南市     南區   大成里  Dacheng Vil.        D      67000\n5  67000350028     臺南市   安南區   城北里 Chengbei Vil.        D      67000\n6  67000350030     臺南市   安南區   城南里 Chengnan Vil.        D      67000\n7  67000370009     臺南市   中西區   法華里    Fahua Vil.        D      67000\n8  67000350017     臺南市   安南區   海南里   Hainan Vil.        D      67000\n9  67000350049     臺南市   安南區   國安里   Guo'an Vil.        D      67000\n10 67000350018     臺南市   安南區   溪心里    Xixin Vil.        D      67000\n   TOWNID TOWNCODE NOTE                       geometry\n1     D06 67000350 &lt;NA&gt; POLYGON ((120.1176 23.08387...\n2     D32 67000270 &lt;NA&gt; POLYGON ((120.2304 22.93544...\n3     D08 67000370 &lt;NA&gt; POLYGON ((120.2012 22.99966...\n4     D02 67000330 &lt;NA&gt; POLYGON ((120.1985 22.98147...\n5     D06 67000350 &lt;NA&gt; POLYGON ((120.1292 23.06512...\n6     D06 67000350 &lt;NA&gt; POLYGON ((120.1246 23.06904...\n7     D08 67000370 &lt;NA&gt; POLYGON ((120.2094 22.98452...\n8     D06 67000350 &lt;NA&gt; POLYGON ((120.175 23.02218,...\n9     D06 67000350 &lt;NA&gt; POLYGON ((120.1866 23.02766...\n10    D06 67000350 &lt;NA&gt; POLYGON ((120.1834 23.06086...\n\n\n\nclass(study_area_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nLikewise, we will also need to subset the dengue_daily layer to only include dengue cases that are within the selected townships we have identified above. In dengue_daily layer, TOWNID information is not included. Hence we will use RuralUrbanTownship_Residence column, which represents the township name (in Chinese) to filter the data.\n\ntownship_name &lt;- c(\"東區\",\"南區\",\"北區\",\"安南區\",\"安平區\",\"中西區\",\"仁德區\",\"永康區\")\n\ndengue_tainan &lt;- dengue_daily %&gt;%\n  filter(CountyCity_Residence==\"台南市\", RuralUrbanTownship_Residence %in% township_name)\n\n\n\n\n\n\n\nTip\n\n\n\nIn the code chunk above, I have to include CountyCity_Residence==\"台南市\" (台南市 = Tainan) because there are many data entry errors observed in the dengue_daily dataset.\n\n\n\n\n\nFor example, East District (東區) is located in Tainan City and is one of the selected townships for our study. In dengue_daily dataset, there are data instances like in the snapshot above, where data ponts in other counties and cities have been mis-classified as East District (東區).\nHence, we need to make sure that the data instances we will filter our for the analysis has correct information for both CountyCity_Residence and RuralUrbanTownship_Residence columns .\n\n\nNow we will use check the number of rows (data instances) in newly created dengue_tainan object.\n\nnrow(dengue_tainan)\n\n[1] 44786"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#updating-township-names-in-datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#updating-township-names-in-datasets",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.4 Updating Township Names in Datasets",
    "text": "5.4 Updating Township Names in Datasets\nstudy_area_sf object that we created in previous section only has VILLENG which represents the name of village in English. We did not have corresponding English translations for township names, which are crucial for a comprehensive understanding of our analysis results. To address this, we have prepared a list of English translations for each Chinese township name as follows.\n\n\n\nChinese (Origin)\nEnglish (Translation)\n\n\n\n\n東區\nEast District\n\n\n南區\nSouth District\n\n\n北區\nNorth District\n\n\n安南區\nAnnan District\n\n\n安平區\nAnping District\n\n\n中西區\nWest Central District\n\n\n仁德區\nRende District\n\n\n永康區\nYongkang District\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe have obtained the English translations for each township and district name directly from Google Maps. Initially, we input the Chinese names into Google Maps. This leads us to the respective township information panel, as shown below. From this point, we directly adopt the English name for each township.\n\n\n\nBased on the English translation we have tabulated above, we will proceed to create a new data column called TOWNENG which represents the name of township in English.\n\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"東區\"] &lt;- \"East District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"南區\"] &lt;- \"South District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"北區\"] &lt;- \"North District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"安南區\"] &lt;- \"Annan District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"安平區\"] &lt;- \"Anping District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"中西區\"] &lt;- \"West Central District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"仁德區\"] &lt;- \"Rende District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"永康區\"] &lt;- \"Yongkang District\"\n\nSimilarly, in dengue_tainan object that we created in previous section, the values for RuralUrbanTownship_Residence column are in Chinese. To enhance interpretability, we will replace these values with their corresponding English translations.\n\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"東區\"] &lt;- \"East District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"南區\"] &lt;- \"South District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"北區\"] &lt;- \"North District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"安南區\"] &lt;- \"Annan District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"安平區\"] &lt;- \"Anping District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"中西區\"] &lt;- \"West Central District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"仁德區\"] &lt;- \"Rende District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"永康區\"] &lt;- \"Yongkang District\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-study-area-of-tainan-city",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-study-area-of-tainan-city",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.5 Plotting Study Area of Tainan City",
    "text": "5.5 Plotting Study Area of Tainan City\nOnce subsetting of the study area is completed, we will now plot a choropleth map of the newly created study_area_sf object.\n\ntm_shape(study_area_sf)+\n  tm_fill(\"TOWNENG\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          title = \"Township Name\") +\n  tm_borders(col = \"white\")+\n  tm_layout(main.title = \"Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-epidemiology-week-for-dengue-cases",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-epidemiology-week-for-dengue-cases",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.6 Deriving Epidemiology Week for Dengue Cases",
    "text": "5.6 Deriving Epidemiology Week for Dengue Cases\nEpidemiology week is a standardised method of counting weeks to allow for the comparison of data year over year. Each epidemiological week begins on a Sunday and ends on a Saturday (i.e. The first epidemiological week of the year ends on the first Saturday of January).\nHowever, the current dengue_tainan object do not has epidemiology week information yet. It only has &lt;date&gt; type data under Onset_Date column. Hence, we will need to calculate and derive epidemiology week information from Onset_Date. To achieve this, we will take the following data wrangling steps:\n\nWe will add a new column year to the dengue_tainan data frame. The year() function will be used to extract the year (numerical) from the Onset_Date value.\nNext, we will adds one more new column Epidemiol_Week to the dengue_tainan data frame. We will use strftime() function to format the time of the Onset_Date column. The \"%V\" format is used to extract the week of the year as decimal number (01–53) as defined in ISO 8601. This newly created Epidemiol_Week column contains the epidemiology week required for our analysis.\n\n\ndengue_tainan &lt;- dengue_tainan %&gt;% mutate(year = year(dengue_tainan$Onset_Date))\ndengue_tainan &lt;- dengue_tainan %&gt;% mutate(Epidemiol_Week = strftime(dengue_tainan$Onset_Date, format = \"%V\"))\n\n\nclass(dengue_tainan$Epidemiol_Week)\n\n[1] \"character\"\n\n\nThe data values in Epidemiol_Week column are observed to be &lt;character&gt;. Hence, we will change these values into &lt;numeric&gt; data type using as.numeric() function.\n\ndengue_tainan$Epidemiol_Week &lt;- as.numeric(dengue_tainan$Epidemiol_Week)\n\nFor this study, we are interested to investigate the dengue fever that occurred during epidemiological weeks 31 through 50 of 2023. Next, we will proceed with filtering the dataset, Hence, we will create a new dengue fever layer called dengue_tainan_fever. This layer will include dengue fevers cases within Tainan City which occurred between epidemiological weeks 31 and 50 of 2023.\n\ndengue_tainan_fever &lt;- dengue_tainan %&gt;%\n  filter(year == \"2023\", Epidemiol_Week&gt;30, Epidemiol_Week&lt;51)\n\nPlotting histograms is a good practice in data wrangling to visualise and understand the underlying distribution as well as patterns, trends and outliers. Hence, we will plot a distribution histogram for dengue_tainan_fever using mapping techniques from ggplot package.\n\nggplot(dengue_tainan_fever, aes(x=Epidemiol_Week)) +\n  geom_histogram(bins = 20,color=\"white\") +\n  labs(x=\"Epidemiology Week\", y=\"Count\", title=\"Histogram of Epidemiology Weeks 31-50 in 2023\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#converting-dengue_tainan_fever-object-to-sf-object",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#converting-dengue_tainan_fever-object-to-sf-object",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.7 Converting dengue_tainan_fever object to sf Object",
    "text": "5.7 Converting dengue_tainan_fever object to sf Object\ndengue_tainan_fever object we created in previous section is of &lt;tibble dataframe&gt; type. Hence, we will need to convert it into a sf object to perform spatial analysis.\n\nclass(dengue_tainan_fever)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nBefore we convert to sf object, it is required to check if any NULL values exist in X_Coordinate and Y_Coordinate column values. A NULL value could mean that the location data for a particular record was not collected or is not available\n\nany(is.na(dengue_tainan_fever[,\"X_Coordinate\"]))\n\n[1] FALSE\n\nany(is.na(dengue_tainan_fever[,\"Y_Coordinate\"]))\n\n[1] FALSE\n\n\nHowever, sometimes Null values may be represented in other forms, such as “None”, “NA”, “NaN”, or even a blank space. Hence, we did a visual inspection of the dataset and found that NULL values are denoted as None as shown below.\n\n\n\n\n\nHence, we need to do manual filtering of all the data instances which has None values for X_Coordinate and Y_Coordinate. To achieve this, we will use the subset function to filter the dengue_tainan_fever data frame. We will use X_Coordinate!=\"None\" & Y_Coordinate!=\"None\" to specify that our data frame only includes the rows where both X_Coordinate and Y_Coordinate are not “None”.\n\ndengue_tainan_fever&lt;-subset(dengue_tainan_fever, X_Coordinate!=\"None\" & Y_Coordinate!=\"None\")\n\nNow that we have cleaned the dataset and removed NULL values, we will proceed to convert this to a sf object called dengue_tainan_fever.sf using column values from X_Coordinate and Y_Coordinate .\n\ndengue_tainan_fever.sf &lt;- st_as_sf(dengue_tainan_fever,\n                            coords = c(\"X_Coordinate\", \"Y_Coordinate\"))\ndengue_tainan_fever.sf\n\nSimple feature collection with 18800 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 120.091 ymin: 22.9 xmax: 120.2803 ymax: 23.08108\nCRS:           NA\n# A tibble: 18,800 × 7\n   Onset_Date CountyCity_Residence RuralUrbanTownship_R…¹ RuralUrbanVillage_Re…²\n   &lt;date&gt;     &lt;chr&gt;                &lt;chr&gt;                  &lt;chr&gt;                 \n 1 2023-07-31 台南市               Yongkang District      埔園里                \n 2 2023-07-31 台南市               East District          大智里                \n 3 2023-07-31 台南市               Yongkang District      五王里                \n 4 2023-07-31 台南市               Rende District         成功里                \n 5 2023-07-31 台南市               Yongkang District      中興里                \n 6 2023-07-31 台南市               Yongkang District      復華里                \n 7 2023-07-31 台南市               Rende District         仁德里                \n 8 2023-07-31 台南市               East District          崇善里                \n 9 2023-07-31 台南市               East District          崇學里                \n10 2023-07-31 台南市               Annan District         鳳凰里                \n# ℹ 18,790 more rows\n# ℹ abbreviated names: ¹​RuralUrbanTownship_Residence,\n#   ²​RuralUrbanVillage_Residence\n# ℹ 3 more variables: year &lt;dbl&gt;, Epidemiol_Week &lt;dbl&gt;, geometry &lt;POINT&gt;\n\n\nThe resulting dengue_tainan_fever.sf object is a simple feature with POINT geometry. It is also noted that the CRS information is absent in this sf object. Hence, we will assign the relevant CRS value for Taiwan, which is ESPG:3824.\n\ndengue_tainan_fever.sf &lt;- st_set_crs(dengue_tainan_fever.sf,3824)\n\nNow that we have assigned the correct CRS value for dengue_tainan_fever.sf, we will try to plot the points from dengue_tainan_fever.sf to ensure all the points are contained within the boundaries of study_area_sf object.\n\ntmap_mode('plot')\ntm_shape(study_area_sf)+\n  tm_fill(\"TOWNENG\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          title = \"Township Name\") +\n  tm_borders(col = \"white\")+\ntm_shape(dengue_tainan_fever.sf)+\n  tm_dots()+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculating-overall-number-of-cases-for-each-village",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculating-overall-number-of-cases-for-each-village",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.8 Calculating Overall Number of Cases for Each Village",
    "text": "5.8 Calculating Overall Number of Cases for Each Village\nIn previous sections, we have created and processed two data layers study_area_sf (which contains polygons representing each village in our study area) and dengue_tainan_fever.sf (which contains points representing recorded cases of dengue fever within our study area). Now, we will calculate the number for cases for each village. To achieve this, we will use st_intersects() and lengths() functions from the sf package.\n\nIdentify Intersecting Points: We will use the st_intersects() function to identify which points from dengue_tainan_fever.sf intersect with each village polygon in study_area_sf.\nCalculate Case Counts: Next, we will use the lengths() function to count the number of intersecting points (i.e., dengue fever cases) within each village polygon.\nRecord Results: Finally, we will record these counts in a new column, CASE_COUNT, in the study_area_sf data frame.\n\n\n(study_area_sf$CASE_COUNT &lt;- lengths(st_intersects(study_area_sf, dengue_tainan_fever.sf)))\n\n  [1]   2  19 111  29   1  10  38  44 112  65  28   2   3  11  24  20  84  24\n [19] 198  96  74  59 166 112  12   6  83  89  52  51  34 194  38 124 106 173\n [37]  27 166  97  32 140 117 139 205  66  71  78  86 113 196 123 106  27  82\n [55]  41 149  53  89  21  42  88  22  79 136 114 104  43  56  72  96  63 100\n [73] 100 163  83  84  47  13  15   7   5  21  17  78 126 157 184 137 112 191\n [91]  15  74 118 204  31 130  78  37  15  16 100  24 170  81  81  75  71  72\n[109] 132 211 167  91 250 311  11 169   3   2  74  61  79  63  55  59 242  30\n[127]  41  62   9  17 105  24  19  15  39 109 102  13  51  96  44 110   3  27\n[145]  57  61  15 150  72 351 189  59  80  78  55  57   4  67  25   5  21  73\n[163]   6  75   0  36  15   8  23  25 178  82  50  62  49  52  84  68 154  48\n[181]  89 101 118  67  86  36 125  83 102 249  54  79  26  83  74  58  63  49\n[199]  91  76  32  31  52  56  83  44  59  60  16  24  39  65   4  78  66 121\n[217]  87  70  33 130  82  14   9 163  15  37 110 145  73 120 111  97 118  34\n[235]  17  18  18   8 243  27 118  27  23 113  32  50  23  11   4  19  12  19\n[253]  18   8  12  28  75  95"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-spacetime-cubes",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-spacetime-cubes",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.9 Creating Spacetime Cubes",
    "text": "5.9 Creating Spacetime Cubes\nsfdep introduces a new s3 class called spacetime by Edzer Pebesma (2012) to represent spatio-temporal data. The spacetime class links a flat data set containing spatio-temporal information with the related geometry. As spacetime class can encapsulate both the spatial and temporal aspects of data, it is suitable for spatio-temporal analysis.\nThere are four important data required to create the spacetime object:\n\ndata: a tibble data.frame object\ngeometry: an sf object\nlocation identifier: a common column between data and geometry\ntime: a column in data that includes temporal information.\n\nBefore creating a spacetime object, we need to carry out data wrangling to prepare the requred data. The data object will ideally include the number of dengue fever cases by village and by epidemiology week. To prepare the data, we will carry out the following steps:\n\nInitialize vectors : We will initializes six empty vectors to store the epidemiology week epi_week_vector, village code village_code_vector, village name village_name_vector, township/district name town_name_vector, village geomtery village_vector, and number of cases num_cases.\nLoop over epidemiology weeks: We will create a loop ver each epidemiology week from 31 to 50. We will create a new variable epi_week to represent the epidemiology week in each iteration.\nFilter points for the current week: For each epidemiology week epi_week, we will filter the dengue_tainan_fever.sf data frame to include only the points (i.e., cases) that occurred during the current week. The filtered points will be temporarily stored in a temporary data frame called dengue_tainan_fever_epiwk.\nLoop over polygons (villages): Under each iteration of epideiology week, we will loop over each polygon (i.e., village) in the study_area_sf spatial data frame. For each polygon, we will extract the polygon itself, VILLCODE value, VILLENG values, TOWNENG values, and geometry values, and store them in temporary variables - village, village_code, village_name, town_name, village_geometry.\nFind intersecting points: We will find the points from dengue_tainan_fever_epiwk that intersect with the current polygon using st_intersects() and lengths() functions. The number of intersecting points is the number of dengue fever cases in the current village for the current week. We will store the result in a temporary variable called fever_count.\nStore the results: We will then append the values of six temporary variables each representing epidemiology week epi_week, village code village_code, village name village_name, township/district name town_name, village geometry village_geometry, and number of cases to their respective vectors fever_count into empty vectors that we initialised earlier.\nCreate a data frame: After looping over all epidemiology weeks and polygons, the script creates a data frame st_data from the six vectors.\nSet column names: Finally, we will update the column names of the st_data data frame to match with the nomenclature of study_area_sf object.\n\n\n# Initialize an empty list to store the results\n# Initialize an empty list to store the results\nepi_week_vector &lt;- c()\nvillage_code_vector &lt;- c()\nvillage_name_vector &lt;- c()\ntown_name_vector &lt;- c()\nvillage_vector &lt;- c()\nnum_cases &lt;- c()\n\n# Loop over each epidemiology week\nfor(epi_week in 31:50) {\n  # Filter the points for the current week\n  dengue_tainan_fever_epiwk &lt;- dengue_tainan_fever.sf %&gt;% \n    filter(Epidemiol_Week == epi_week)\n  \n  # Calculate the number of points in each polygon\n  for(i in 1:nrow(study_area_sf)) {\n    # Get the current polygon\n    village &lt;- study_area_sf[i, ]\n    village_code &lt;- village$VILLCODE\n    village_name &lt;- village$VILLENG\n    town_name &lt;- village$TOWNENG\n    village_geometry &lt;- village$geometry\n    # Find the points that intersect with the current polygon\n    fever_count &lt;- lengths(st_intersects(village, dengue_tainan_fever_epiwk))\n    \n    # Store the results\n    epi_week_vector &lt;- append(epi_week_vector, as.double(epi_week))\n    village_code_vector &lt;- append(village_code_vector, village_code)\n    village_name_vector &lt;- append(village_name_vector,village_name)\n    town_name_vector &lt;- append(town_name_vector,town_name)\n    village_vector &lt;- append(village_vector,village_geometry)\n    num_cases &lt;- append(num_cases, as.double(fever_count))\n  }\n}\n\n#Create a data frame\nst_data &lt;- data.frame(epi_week_vector,village_code_vector,village_name_vector,town_name_vector,num_cases, village_vector)\n\ncolnames(st_data) &lt;- c(\"Epidemiol_Week\", \"VILLCODE\",\"VILLENG\",\"TOWNENG\",\"CASE_COUNT\", \"geometry\")\n\nBefore we create spacetime cube, we will first save the dataframe into an sf object called st_data.sf using st_as_sf(). This data will capture the both geometry and case count per epidemiology week for all villages in our study and will be used later for visualisation purpose.\n\nst_data.sf &lt;- st_as_sf(st_data)\n\nAfter creating st_data.sf, we will drop the geometry of st_data object as we do not need for creating spacetime cube. To do so, we will use select() function to select only first 4 columns - Epidemiol_Week, VILLCODE, VILLENG, TOWNENG and CASE_COUNT. Then, we will sort the st_data by VILLCODE.\n\nst_data &lt;- st_data %&gt;% select(c(1:5))\nst_data &lt;- st_data[order(st_data$VILLCODE),]\n\nLastly, we will convert st_data into a tibble data.frame object for creating spacetime cube. To do so, we will use as_tibble() function.\n\nst_data &lt;- as_tibble(st_data)\n\n\n\n\n\n\n\nReflection\n\n\n\nAlthough the sfdep document mention that the data context for spacetime cube has to be a data.frame​ object, it did not work when I use a data.frame​ object to create spacetime cube and used it to create neighbour list. I ran into the error such this.\n\n\n\n\n\nI figured out that I needed to use a tibble​ object and create a new spacetime cube after many trials and errors.\n\n\nWith all the necessary data prepared, we can now create a spacetime object called dengue_tanan_spt using the spacetime function, using the following data:\n\ndata: st_data\ngeometry: study_area_sf\nlocation identifier: VILLCODE (from st_data and study_area_sf)\ntime column: Epidemiol_Week (from st_data)\n\n\ndengue_tainan_spt &lt;- spacetime(st_data, study_area_sf,\n                                     .loc_col = \"VILLCODE\",\n                                     .time_col = \"Epidemiol_Week\",\n                               active = \"data\")\n\nA spacetime object is known as a spacetime cube if every location has a value for every time index. In other words, each location is associated with a regular time-series. Spacetime cubes are particularly useful in the analysis of emerging hot spots because they allow for the comprehensive tracking of changes over both space and time.\n\n\n\n\n\nWe will use is_spacetime_cube() function to check if our newly created dengue_tainan_spt object is a spacetime cube or not.\n\nis_spacetime_cube(dengue_tainan_spt)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-village-level-dengue-distribution-maps",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-village-level-dengue-distribution-maps",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.10 Creating Village-Level Dengue Distribution Maps",
    "text": "5.10 Creating Village-Level Dengue Distribution Maps\nIn this section, we will create a village-level dengue distribution maps utilising the data we have processed and prepared for epidemiology weeks 31-50. To do so, we will employ relevant tmap functions.\n\nstudy_area_count &lt;- tm_shape(study_area_sf)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Dengue Cases\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Villege-Level Distribution of Dengue Cases \\n from Epidemiology Week 31-50 in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.8,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nstudy_area_count"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-time-series-animated-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-time-series-animated-map",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.11 Creating Time-Series Animated Map",
    "text": "5.11 Creating Time-Series Animated Map\nIn addition to static map we created for total cumulative case counts, we can also create a time-series animated map to visualise the temporal distribution of dengue cases, which will effectively illustrate the changes over each epidemiology week.\nFor visualization purpose, we will mutate an additional column called Epidemiol_Week_Label as follows.\n\nst_data.sf &lt;- st_data.sf %&gt;% mutate(\n  Epidemiol_Week_Label = paste(\"Epidemiology Week\", as.character(Epidemiol_Week)))\n\nNow that we have prepared the data, we will use the along = \"Epidemiol_Week_Label\" argument within the tm_facets function to generate facets for each epidemiological week. Subsequently, we will employ the tmap_animation function to create an animation from the temporal map. This animation will be saved as a GIF file, named dengue_tainan_temporal.gif.”\n\ntemporal_maps &lt;- tm_shape(st_data.sf)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\",\"#e1ecbb\", \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          style = \"pretty\", \n          title = \"Dengue Cases\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(legend.title.size = 1.8,\n            legend.text.size = 1.5)+\n  tm_facets(along = \"Epidemiol_Week_Label\", free.coords = FALSE)\n\ntmap_animation(temporal_maps, filename = \"dengue_tainan_temporal.gif\", delay = 150, width = 1000, height = 1200)\n\n\nknitr::include_graphics(\"dengue_tainan_temporal.gif\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#defining-contiguity-neighbours",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#defining-contiguity-neighbours",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.1 Defining Contiguity Neighbours",
    "text": "6.1 Defining Contiguity Neighbours\nSpatial relationships are multidirectional and multilateral. To systematically transcribe the complexity of a geographic space into a final set of data analysable by a computer, first the study zone is divided into mutually exclusive areas. Each area contains a reference point (often its centroid). Then, the spatial relationships can be specified by a neighbourhood graph connecting the areas considered to be neighbouring, or by a matrix containing the geographical coordinates of the reference points. The third step consists in coding the graph in a neighbourhood matrix or transforming the geographic coordinates into a distance matrix.\n\n\n\n\n\nThere are multiple approaches in defining spatial neighbors. Some of the most commonly used approaches include a) adjacency measure where neighbourhood graphs are constructed by materialising the links between different points which are immediately adjacent to each other; and b) distance measure where neighbours are selected based on \\(k\\)-closet points adjacent to a spatial unit.\nAnselin discussed a new approach in identifying spatial neighbours called contiguity Measure (Anselin, 2020). Neighbors based on contiguity are constructed by assuming that neighbours of a given area are other areas that share a common boundary. Operationally, we can further distinguish between a rook, a queen and a bishop criterion of contiguity as described below.\n\n\n\n\n\n\nRook criterion (Common Edge) defines neighbours by the existence of a common edge between two spatial units.\nBishop criterion (Common Vertex) defines neighbours by the existence of a common vertex between two spatial units.\nQueen criterion (Either Common Edge or Vertext) defines neighbours as spatial units sharing a common edge or a common vertex. Therefore, the number of neighbours according to the queen criterion will always be at least as large as for the rook or bishop criterion."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-neighbours",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-neighbours",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.2 Computing Contiguity Neighbours",
    "text": "6.2 Computing Contiguity Neighbours\nWe will use st_contiguity() function from sfdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. However, this function only support rook and queen’s criterion. For this study, we will use queen criteria to calculate our neighbour list.\n\ntainan_nb_q &lt;- st_contiguity(study_area_sf, queen=TRUE)\nsummary(tainan_nb_q)\n\nNeighbour list object:\nNumber of regions: 258 \nNumber of nonzero links: 1526 \nPercentage nonzero weights: 2.29253 \nAverage number of links: 5.914729 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 10 11 12 14 \n 4 17 47 49 49 41 26 14  6  3  1  1 \n4 least connected regions:\n77 117 138 238 with 2 links\n1 most connected region:\n128 with 14 links"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-row-standardised-weight-matrix",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-row-standardised-weight-matrix",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.3 Computing Row-Standardised Weight Matrix",
    "text": "6.3 Computing Row-Standardised Weight Matrix\nNext, we need to assign spatial weights to each neighboring polygon.\nst_weights() function from sfdep pacakge can be used to supplement a neighbour list with spatial weights based on the chosen coding scheme. There are as least 5 different coding scheme styles supported by this function:\n\nB is the basic binary coding\nW is row standardised (sums over all links to n)\nC is globally standardised (sums over all links to n)\nU is equal to C divided by the number of neighbours (sums over all links to unity)\nS is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. (1999) (sums over all links to n).\n\nIn this study, we will use row-standardised weight matrix (style=\"W\"). Row standardisation of a matrix ensure that the sum of the values across each row add up to 1. This is accomplished by assigning the fraction 1/(# of neighbors) to each neighboring county then summing the weighted income values. Row standardisation ensures that all weights are between 0 and 1. This facilities the interpretation of operation with the weights matrix as an averaging of neighboring values, and allows for the spatial parameter used in our analyses to be comparable between models.\n\ntainan_wm_rs &lt;- st_weights(tainan_nb_q, style=\"W\")\n\nWe will mutate the newly created neighbour list object tainan_nb_q and weight matrix tainan_wm_rs into our existing study_area_sf. The result will be a new object, which we will call wm_q.\n\nwm_q &lt;- study_area_sf %&gt;%\n  mutate(nb = tainan_nb_q,\n         wt = tainan_wm_rs,\n         .before = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.4 Global Moran’s \\(I\\)",
    "text": "6.4 Global Moran’s \\(I\\)\n\n6.4.1 Computing Global Moran’s \\(I\\)\nMoran’s \\(I\\) is the correlation coefficient for the relationship between a variable and its neighbouring values. Moran’s \\(I\\) describes how features differ from the values in the study area as a whole and quantifies how similar each region is with its neighbors and averages all these assessments. Moran’s \\(I\\) values usually range from –1 to 1. We can test spatial autocorrelation by following these hypotheses:\n\nNull Hypothesis \\(H_0:I=E[I]\\). This suggests there is no spatial autocorrelation.\nAlternative Hypothesis \\(H_1:I≠E[I]\\). This indicates the presence of spatial autocorrelation.\n\n\nmoranI &lt;- global_moran(wm_q$CASE_COUNT,\n                        wm_q$nb,\n                        wm_q$wt)\n\n\n\n6.4.2 Global Moran’s \\(I\\) test\nThe Global Moran’s I test, which can be implemented using the global_moran_test() function from the sfdep package, is a method for testing spatial autocorrelation. The primary goal of this test is to determine whether the spatial autocorrelation is positive, negative, or non-existent. The hypotheses for this test are as follows:\n\nNull Hypothesis \\(H_0:I≤E[I]\\). This suggests that there is either no spatial autocorrelation ( \\(I=E[I]\\)). or negative spatial autocorrelation ( \\(I&lt;E[I]\\)).\nAlternative Hypothesis \\(H_1:I&gt;E[I]\\). This indicates the presence of positive spatial autocorrelation.\n\nWe will set alternative = \"greater\" based on our alternative hypothesis.\n\nglobal_moran_test(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  alternative = \"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 12.865, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.468214327      -0.003891051       0.001346663 \n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can draw the following statistical inferences:\n\nThe Moran’s I statistic is 0.468214327, which is significantly different from the expectation under the null hypothesis of -0.003891051. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is -0.003891051. This is the variance of the Moran’s I statistic.\nThe p-value is &lt;2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in our study area. This means that areas with similar values of CASE_COUNT are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n6.4.3 Performing Global Moran’s \\(I\\) permutation test\nThe assumptions underlying the test are sensitive to the form of the graph of neighbour relationships and other factors, and results may be checked against those of Monte Carlo permutations. Monte Carlo randomization creates random patterns by reassigning the observed values among the areas and calculates the Moran’s \\(I\\) for each of the patterns (Moraga, 2023).\nWe will use global_moran_perm() function from sfdep package with nsim = 999 which represent 1000 Monte Carlo simulations to be carried out.\n\nset.seed(1234)\ngmoranMC &lt;- global_moran_perm(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 999)\ngmoranMC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.46821, observed rank = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can observe that the Moran’s I statistic (after 1000 permutations) is 0.46821 with a p-value of &lt; 2.2e-16, which is almost identical to our previous result using global_moran_test(). It confirms that our result is stable and statistically significant.\n\n\nWe can also create a histogram to visualise the permutation results and compare them to the expectation value under null hypotheses.\n\nhist(gmoranMC$res, main=\"Histogram of Global Moran's I Monte-Carlo Simulation Results\", xlab=\"Monte-Carlo Results\", ylab=\"Frequency\")\n\nabline(v = gmoranMC$statistic, col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.5 Global Geary’s \\(C\\)",
    "text": "6.5 Global Geary’s \\(C\\)\nIntroduced by Geary, Geary’s \\(c\\) statistic studies the degree of intensity of a given feature in spatial objects described with the use of a weight matrix. Similarly to Moran’s analysis, Geary’s \\(c\\) can be used to quantify the extent of spatial autocorrelation in the data.\n\n6.4.2 Global Geary’s \\(C\\) test\nThe Global Geary’s \\(C\\) test, which can be implemented using the global_c_test() function from the sfdep package.\n\nglobal_c_test(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  alternative = \"greater\")\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw \n\nGeary C statistic standard deviate = 11.317, p-value &lt; 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.495364563       1.000000000       0.001988186 \n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can draw the following statistical inferences:\n\nThe Geary’s C statistic is 0.495364563, which is significantly different from the expectation under the null hypothesis of 1. This suggests the presence of spatial autocorrelation in the data.\nThe p-value is &lt;2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the test suggests that there is significant spatial autocorrelation in our study area. This means that areas with similar values of CASE_COUNT are more likely to be spatially clustered near each other than would be expected if the data were randomly distributed.\n\n\n\n\n6.4.3 Performing Global Geary’s \\(C\\) Permutation Test\nSimilar to what we did in Moran’s \\(I\\) test, we will use global_c_perm() function from sfdep package with nsim = 999 which represent 1000 Monte Carlo simulations to be carried out.\n\nset.seed(1234)\nbperm &lt;- global_c_perm(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw \nnumber of simulations + 1: 1000 \n\nstatistic = 0.49536, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can observe that the Geary’s C statistic (after 1000 permutations) is 0.4953 with a p-value of 0.01, which is comparable to our previous result using global_c_test(). It confirms that our result is stable and statistically significant.\n\n\nWe can also create a histogram to visualise the permutation results and compare them to the expectation value under null hypotheses.\n\nhist(bperm$res, main=\"Histogram of Global Geary's C Monte-Carlo Simulation Results\", xlab=\"Monte-Carlo Results\", ylab=\"Frequency\")\n\nabline(v = bperm$statistic, col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i_i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i_i",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "7.1 Local Moran’s \\(I_i\\)",
    "text": "7.1 Local Moran’s \\(I_i\\)\nWe can decompose Global Moran’s \\(I_i\\) that we calculate in previous section into a localized measure of autocorrelation, called Local Moran’s \\(I_i\\). To compute Local Moran’s \\(I_i\\), the local_moran() function of sfdep will be used.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    CASE_COUNT, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\nlisa\n\nSimple feature collection with 258 features and 26 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\n# A tibble: 258 × 27\n        ii      eii  var_ii    z_ii    p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  1.39   -0.0482  0.563    1.92   0.0552      0.02         0.01   -0.684\n 2  0.747   0.00231 0.192    1.70   0.0895      0.04         0.02   -0.547\n 3  0.0391  0.0305  0.0553   0.0363 0.971       0.92         0.46    0.492\n 4 -0.217   0.0220  0.0821  -0.836  0.403       0.34         0.17   -0.357\n 5  1.42    0.0466  0.292    2.54   0.0111      0.02         0.01   -0.478\n 6  1.29   -0.00600 0.157    3.27   0.00108     0.02         0.01   -0.217\n 7  0.319  -0.0191  0.0956   1.09   0.274       0.24         0.12   -0.431\n 8  0.0174  0.00346 0.0277   0.0840 0.933       0.96         0.48   -0.264\n 9  0.411   0.0418  0.101    1.16   0.247       0.22         0.11    0.747\n10  0.0820 -0.00329 0.00368  1.41   0.160       0.18         0.09   -0.438\n# ℹ 248 more rows\n# ℹ 19 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, VILLCODE &lt;chr&gt;, COUNTYNAME &lt;chr&gt;, TOWNNAME &lt;chr&gt;,\n#   VILLNAME &lt;chr&gt;, VILLENG &lt;chr&gt;, COUNTYID &lt;chr&gt;, COUNTYCODE &lt;chr&gt;,\n#   TOWNID &lt;chr&gt;, TOWNCODE &lt;chr&gt;, NOTE &lt;chr&gt;, geometry &lt;POLYGON [°]&gt;,\n#   TOWNENG &lt;chr&gt;, CASE_COUNT &lt;int&gt;\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations\np_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative=\np_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\n\n\n7.1.1 Visualising Local Moran’s \\(I_i\\)\nThe best approach to describe and explain Local Moran’s \\(I_i\\) values is tthrough visualization on a map. In this section, we will employ relevant tmap functions to visualize the Local Moran’s \\(I_i\\) values across the study area.\n\ntm_shape(lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe local spatial autocorrelation (Moran’s I test) results in a total of 6 different zones with varying level of Moran’s I values.\n\nBlue Zone : Villages in blue zone of the map has local Moran’s I values ranging from -1 to 0. This may suggests that these villages tend to be outliers which has dissimilar values compared to neighbouring villages.\nGreen & Yellow, Light Orange, Dark Orange and Red Zones: Villages in these zones of the map has local Moran’s I values ranging from 1 to 5. This may suggest that these villages tend to be spatial clusters which has similar values compared to neighbouring villages.\n\nLooking at the distribution histogram, the majority of the villages fall within Green Zone, followed by Blue Zone and Yellow Zone. The number of villages within Light Orange, Dark Orange and Red Zones is significantly smaller.\nOverall, the spatial autocorrelation map reveals a concentration of dengue cases in the central region of the study area, as indicated by the relatively high Local Moran’s I values. This pattern suggests that the central region of our study area is a high-risk zone for dengue. However, a comprehensive understanding of this pattern requires an analysis of the statistical significance associated with each Local Moran’s I value. Without this, our interpretation of the spatial distribution and clustering of dengue cases remains incomplete.\n\n\n\n\n7.1.2 Visualising Local Moran’s \\(I_i\\) p-value\nAs mentioned in the analysis section above, we will need to examine the Local Moran’s \\(I_i\\) value alongside their corresponding p-values. Following a similar approach, we will use relevant tmap functions to visualise p-values associated wtih Local Moran’s \\(I_i\\) values across the study area.\n\ntm_shape(lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#b7dce9\",\"#c9e3d2\",\"#f5f3a6\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n7.1.3 Visualising Statistically Significant Local Spatial Autocorrelation Map\nFrom the p-value map above, it appears that not every village exhibits a statistically significant Local Moran’s \\(I_i\\) value (i.e., p-value &lt; 0.05). Consequently, our analysis will focus solely on villages with statistically significant local Moran’s \\(I_i\\) values. To achieve this, we will filter out all local Moran’s \\(I_i\\) values with a p-value greater than 0.05. Subsequently, we will use relevant tmap functions to create a statistically significant local spatial autocorrelation map for our study area.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;% mutate(label = paste(VILLENG,TOWNENG))\n\ntm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level Spatial Autocorrelation Map \\n of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe local spatial autocorrelation (Moran’s I test) results after the exclusion of statistically significant Moran’s I values in a total of 6 different zones with varying levels of Moran’s I values.\n\nBlue Zone : Villages in blue zone of the map has local Moran’s I values ranging from -1 to 0. This may suggests that these villages tend to be outliers which has dissimilar values compared to neighbouring villages.\nGreen & Yellow, Light Orange, Dark Orange and Red Zones: Villages in these zones of the map has local Moran’s I values ranging from 1 to 5. This may suggest that these villages tend to be spatial clusters which has similar values compared to neighbouring villages.\n\nLooking at the distribution histogram, the majority of the villages fall within Green Zone, followed by Yellow Zone. The number of villages within Blue, Light Orange, Dark Orange and Red Zones is comparatively smaller.\nIn our previous analysis, we hypothesized that the central part of our study area is a high-risk zone for dengue, given the concentration of elevated Moran’s I values. However, after the exclusion of statistically significant Moran’s I values, it turns out that majority of Moran’s I values in this central region lack statistical significance. Hence, we cannot conclude that the central part of our study area is a high-risk zone, based on statistical evidence.\nContrasting Moran’s I Values Within Close Proximity?\nInterestingly, it is observed that one village falls into Red Zone (4~5) and shows highest Moran’s I values (4.9070), signifying a single, significant spatial cluster. Adjacent to this, on the right, a total of four villages seems to fall under Blue Zone (-1~0) and have negative Moran’s I values, indicating spatial outliers. Such a contrast in spatial autocorrelation within close proximity suggests the presence of localized factors influencing the transmission of dengue case. Further investigation into these specific villages may provide insights into the underlying reasons for such a contrasting result.\n\n\n\n\n7.1.4 Visualising Statistically Significant Local Spatial Autocorrelation Map for Each Township\nIn our previous section, we have prepared and conducted analysis of local spatial autocorrelation across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the spatial patterns and correlations within each individual district.\nFirstly, we will filter study_area_sf, lisa and lisa_sig objects to each districts within our study area. Following this, we will generate Local Moran’s I maps for each district. These maps will serve as the basis for our subsequent discussions and analyses of the findings.\n\n\nShow the code\nannan &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Annan District\")\nrende &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping&lt;- study_area_sf %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral &lt;- study_area_sf %&gt;% filter(TOWNENG == \"West Central District\")\nsouth &lt;- study_area_sf %&gt;% filter(TOWNENG == \"South District\")\neast &lt;- study_area_sf %&gt;% filter(TOWNENG == \"East District\")\nnorth &lt;- study_area_sf %&gt;% filter(TOWNENG == \"North District\")\n\n\nannan_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Annan District\")\nrende_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"South District\")\neast_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"East District\")\nnorth_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"North District\")\n\nannan_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Annan District\")\nrende_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"South District\")\neast_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"East District\")\nnorth_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"North District\")\n\n\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nannan_local &lt;- tm_shape(annan_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nannan_sig &lt;- tm_shape(annan_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(annan_local,annan_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nThe single, significant spatial cluster that we identified in previous analysis is revealed to be Xiqi Village of Annan District. Apart from this, the Annan District does not exhibit any spatial outliers, as all villages with statistically significant Local Moran’s I values display positive values. This suggests a consistent pattern of dengue cases within these villages. There is a comparable number of villages that fall within the Green (0~1), Yellow (1~2), and Light Orange (2~3) Zones, each representing varying degrees of positive spatial autocorrelation. Notably, no villages were observed in the Dark Orange Zone (3~4).\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nrende_local &lt;- tm_shape(rende_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\", \"#c9e3d2\",\"#f5f3a6\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nrende_sig &lt;- tm_shape(rende_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(rende_local,rende_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nThe Rende District displays a total of three spatial clusters - Chenggong Village, Bao’an Village, and Shanglun Village - all exhibiting Local Moran’s I values that range from 0 to 1. This range suggests a moderate level of spatial autocorrelation, indicating that the distribution of dengue cases in this district is less clustered and not as strong as in other districts.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nyongkang_local &lt;- tm_shape(yongkang_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nyongkang_sig &lt;- tm_shape(yongkang_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(yongkang_local,yongkang_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nYongkang District observes a total of 7 spatial clusters and 3 spatial outliers with varying levels of local Moran’s I values, suggesting heterogeneous distribution of dengue cases across the district. This suggests a heterogeneous distribution of dengue cases across the district. Erwang Village appears to exhibit the highest spatial autocorrelation with local Moran’s I value of 2.254. The rest of the spatial clusters have local Morna’s I values ranging between 0.5 and 2, indicating moderate to strong positive spatial autocorrelation. Three spatial ouliers observed in this district are Wangliao Village, Sanhe Village and Zhongxing Village, all exhibiting a negative local Moran’s I value between -0.5 to 1. This implies these villages have dissimilar dengue case counts compared to their neighboring villages.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nanping_local &lt;- tm_shape(anping_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nanping_sig &lt;- tm_shape(anping_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          n =5,\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(anping_local,anping_sig, asp=1, ncol=2)\n\n\n\n\n\n\nAnalysis & Discussion\nIn Anping District, no village exhibits a statistically significant Local Moran’s I value. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\nwestcentral_local &lt;- tm_shape(westcentral_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#c9e3d2\",\"#f5f3a6\",\"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nwestcentral_sig &lt;- tm_shape(westcentral_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            legend.position = c(\"LEFT\",\"TOP\"),\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(westcentral_local,westcentral_sig, asp=1, ncol=2)\n\n\n\n\n\n\nAnalysis & Discussion\nIn the West Central District, no village exhibits a statistically significant Local Moran’s I value. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nsouth_local &lt;- tm_shape(south_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#c9e3d2\",\"#f5f3a6\",\"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nsouth_sig &lt;- tm_shape(south_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(south_local,south_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSouth District did not observe any spatial outliers, but a total of 7 spatial clusters, all exhibiting Local Moran’s I values that range from 0 to 4. This range suggests varying degrees of spatial autocorrelation, indicating that the distribution of dengue cases in this district is not uniformly clustered. Mingliang Village appears to exhibit the highest spatial autocorrelation with local Moran’s I value of 3.550. The rest of the spatial clusters have local Moran’s I values ranging between 0 and 3, indicating small to moderate positive spatial autocorrelation.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\neast_local &lt;- tm_shape(east_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          bin = 4) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\neast_sig &lt;- tm_shape(east_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(east_local,east_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSimilar to South District, East District did not observe any spatial outliers, but a total of 4 spatial clusters, exhibiting Local Moran’s I values that range from 0 to 2.5. Ziqiang Village exhibit the highest spatial autocorrelation with local Moran’s I value of 2.456. Still, this results is comparatively smaller than the other districts, suggesting a moderate spatial autocorrelation pattern across the district.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nnorth_local &lt;- tm_shape(north_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\", \"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nnorth_sig &lt;- tm_shape(north_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          n =5,\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(north_local,north_sig, asp=1, ncol=2)\n\n\n\n\n\n\nInteractive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nNorth District observed one spatial outlier - Huade Village with local Moran’s I value of -0.397 and one spatial cluster - Wenyuan Village with local Moran’s I value of 0.318. While both villages shows statistically significant autocorrelation effect, the degree of autocorrelation is significantly small compared to patterns observed in other districts."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-classification",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-classification",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "7.2 LISA Classification",
    "text": "7.2 LISA Classification\nThe local indicator of spatial association (LISA) for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation. In general, the analysis will calculate a local statistic value, a z-score, a pseudo p-value, and a code representing the cluster type for each statistically significant feature. LISA map is a categorical map showing type of outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters.\nSpecific to our study, we may infer LISA classifications as below.\n\nHigh-Low Outliers: Villages with a high value of dengue cases, surrounded by neighbouring villages with low values of dengue cases.\nLow-High Outliers: Villages with a low value of dengue cases, surrounded by neighbouring villages with high values of dengue cases.\nHigh-High Clusters: Villages with a high value of dengue cases, surrounded by neighbouring villages with high values of dengue cases.\nLow-Low Clusters: Villages with a low value of dengue cases, surrounded by neighbouring villages with low values of dengue cases.\n\n\n7.2.1 Visualising Statistically Significant LISA Map for Study Area\nIn lisa sf data.frame we created when calculating local Moran’s \\(I_i\\) , we can find three fields contain the LISA categories. They are mean, median and pysal. We will use mean column to visualise LISA classification maps with relevant tmap functions.\n\ntmap_mode(\"plot\")\nstudy_area_lisa &lt;- tm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#e1ecbb\", \"#d21b1c\"),\n          title = \"LISA class\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level LISA Map of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nstudy_area_lisa\n\n\n\n\n\ntmap_mode(\"view\")\ntm_shape(lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#e1ecbb\", \"#d21b1c\"),\n          title = \"LISA class\",\n          midpoint = NA,\n          id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nLISA Classification map for the study area has revealed a total of three LISA classes, each representing different spatial autocorrelation patterns. Low-Low Class has the highest number of villages, followed by High-High Class. Low-High Class has relatively smaller number of villages.\n\nLow-Low Class: Villages classified into the Low-Low class are primarily situated in the upper and lower periphery of the study area, suggesting a clustering of low dengue cases. Interestingly, there is a single Low-Low class village observed in the central part of the study area, which could be an exception or an indication of a potential transition zone.\nHigh-Low Class: The absence of any High-Low classification suggests that there are no high-value villages surrounded by low-value villages.\nHigh-High Class: Villages classified into the High-High class are scattered across the central part of the study area. These villages exhibit similar high values of dengue cases, and are hence “dengue hotspots”.\nLow-High Class: All villages classified into the Low-High class are located in the central part of the study area. Interestingly, these villages tend to be in close proximity to villages in the High-High class. These are the villages which has reportedly lower values depsite their neighbouring villages being dengue hotspots. Investigation into these villages may allow for more concrete insights on the underlying causes leading to such spatial outlier - either due to effective vector control measures, or increased immunity of the population living in these villages.\n\n\n\n\n\n7.2.2 Visualising Statistically Significant LISA Map for Each Township\nIn our previous section, we have prepared and conducted analysis of LISA classfication across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the spatial patterns and correlations within each individual district.\nSince we already have filtered study_area_sf, lisa and lisa_sig objects to each districts in previous section, we will proceed with visualisations and analysis dicsussion.\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\nannan_dengue_map &lt;- tm_shape(annan)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nannan_sig_map &lt;- \ntm_shape(annan_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(annan_dengue_map,annan_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nThe Annan District presents an intriguing spatial autocorrelation pattern characterized by the co-existence of Low-Low and High-High clusters. This suggests a diverse distribution of dengue cases across the district.\nThe Northern part of the Annan District predominantly exhibits Low-Low clusters. On the other hand, the Southern part of the Annan District, bordering the North District, displays High-High clusters.\nThis may suggest that villages in northern part of Annan District might serve as dengue hotspots and potentially transmit the disease to the villages in Northern part of Annan District which currently have low dengue cases. The movement of people between these villages could potentially facilitate the spread of the disease.\n\n\n\n\n\nShow the code\nrende_dengue_map &lt;- tm_shape(rende)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nrende_sig_map &lt;- \ntm_shape(rende_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(rende_dengue_map,rende_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nRende District only observe Low-Low class villages. This result may suggest that while dengue cases are detected in the Rende District, the outbreak is well under control, limiting the rise of any potential High-Low or High-High class villages. This could be due to effective public health interventions, or other underlying factors that limit the breeding grounds for mosquitoes or transmission of the disease.\n\n\n\n\n\nShow the code\nyongkang_dengue_map &lt;- tm_shape(yongkang)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nyongkang_sig_map &lt;- \ntm_shape(yongkang_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(yongkang_dengue_map,yongkang_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nYongkang District observe a mixture of Low-Low clusters, Low-High outliers and High-High clusters. Interestingly, all Low-Low class villages are located in the northeast part of Yongkang District. This suggests that these areas have been successful in controlling the spread of dengue.\nLow-High and High-High class are located in close proximity in the western part of Yongkang District. Based on these findings, three Low-High class villages observed has high risk of transforming into High-High class. This is because they are currently low-value areas surrounded by high-value areas, and the continued spread of dengue could potentially increase their case counts. Given the risk of transformation, these Low-High class villages require proper vector control measures to prevent an increase in dengue cases.\n\n\n\n\nAnalysis & Discussion\nNo statistically significant LISA classes have been detected in the Anping District. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\nAnalysis & Discussion\nNo statistically significant LISA classes have been detected in the West Central District. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\nsouth_dengue_map &lt;- tm_shape(south)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nsouth_sig_map &lt;- \ntm_shape(south_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(south_dengue_map, south_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nSouth District observes the co-existence of both High-High class villages and Low-Low class villages. Majority of the Low-Low class villages tend to located in the southern part of South District and all High-High class villages are in the northen part. This indicates a significant hotspot of dengue cases in the northern part of the district, while the southern part have been successful in controlling the spread of dengue.\nInterestingly, Xinsheng Village, which is currently a Low-Low class village, lies in close proximity to the High-High class villages. This suggests that while Xinsheng Village currently has a low number of dengue cases, it is at risk due to its location near high-risk areas. As a result, intervention efforts should indeed be maintained and strengthened to control the spread of dengue cases to this village.\n\n\n\n\n\nShow the code\neast_dengue_map &lt;- tm_shape(east)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\neast_sig_map &lt;- \ntm_shape(east_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(east_dengue_map,east_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nEast District observed a total of 4 High-High cluster villages in the southeastern part of the district, bordering Rende District. This suggests a potential risk of dengue spread to neighboring districts like Rende through movement of people or mosquitoes. Given the risk, intervention efforts should indeed be maintained and strengthened in East District.\n\n\n\n\n\nShow the code\nnorth_dengue_map &lt;- tm_shape(north)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nnorth_sig_map &lt;- \ntm_shape(north_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(north_dengue_map,north_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nNorth District observed one High-High cluster and one Low-High outlier right next to each other. The observed High-High cluster is Wenyuan Village, and it is neighbouring to other High-High clusters in Annan District. This spatial pattern suggests a potential spread of dengue cases from Annan to North District.\nThe observed Low-High class outlier, Huade Village is also at risk of becoming a dengue hotspot due to its close proximity to numerous High-High clusters. Intervention efforts may be strengthened or inter-village movements may be temporarily limited to prevent the village from becoming a hotspot."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-hot-spot-and-cold-spot-area-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-hot-spot-and-cold-spot-area-analysis",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.1 Local Getis-Ord \\(G_i^*\\) for Hot Spot and Cold Spot Area Analysis",
    "text": "8.1 Local Getis-Ord \\(G_i^*\\) for Hot Spot and Cold Spot Area Analysis\nLocal Getis-Ord \\(G_i\\) and \\(G_i^∗\\) are one of the earliest LISAs. The Gi and Gi* measures are typically reported as a z-score where high values indicate a high-high cluster, and negative z-scores indicate a low-low cluster. There are no high-low and low-high classifications like the local Moran.\n\\(G_i\\) statistic consist of a ratio of the weighted average of the values in the neighbouring locations, to the sum of all values, not including the value at location (\\(x_i\\)). The \\(G_i^∗\\) statistic includes the focal (or self, or \\(i^{th}\\)) observation in the neighbourhood. Spatial weights used in calculating \\(G_i\\) and \\(G_i^*\\) statistics used a distance-based approach - i.e., spatial weights identify locations of statistically significant hot spots and cold spots that are in proximity to one another based on a calculated distance.\nIn this study, we will To compute local \\(G_i\\) statistic in R, we will use st_contiguity() to create a neighbour list, then include_self() to include the focal observation in the neighbour list. Then, we use the neighbour list to create a weight list using st_inverse_distance() function.\n\nwm_idw &lt;- study_area_sf %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nNext, we will calculate local \\(G_i^∗\\) using local_gstart_perm() function from sfdep package. This function uses a neighbour list nb and a weight list wt as an input and generate \\(G_i^∗\\) statistics through a Monte Carlo permutation with specified nsim. The results will then be stored into a new object called HCSA.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    CASE_COUNT, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\nNext, we will use relevant tmap functions to visualise the result of local \\(G_i^*\\) values for our study area. For visualisation purpose, we will create a new column label similar to what we did in Local Moran’s I.\n\nHCSA &lt;- HCSA%&gt;%\n  mutate(label = paste(VILLENG,TOWNENG))\n\n\ntmap_mode(\"plot\")  \ntm_shape(HCSA)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \" Hotspots & Coldspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\nSimilar to what we have done for LISA, we will only focus on villages with statistically significant Local Getis-Ord \\(G_i^∗\\) values. To achieve this, we will filter out all Local Getis-Ord \\(G_i^∗\\) values with a p-value &gt; 0.05. Subsequently, we will use relevant tmap functions to create a statistically significant local spatial autocorrelation map for our study area.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(HCSA) +\n  tm_polygons(id=\"label\") +\ntm_shape(HCSA_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe hotspot coldspot map after the exclusion of statistically significant Local Gi* values result in different coldspot and hotpots with varying levels of Local Gi* values. The interpretation of the Local Gi* statistics is very straightforward - a positive value suggests a High-High cluster or hot spot and a negative value indicates a Low-Low cluster or cold spot.\nA visual inspection suggests that the majority of the coldspots are situated on the periphery of the study areas, while the hotspots tend to be concentrated in the centre. This observation aligns with the findings from the LISA maps.\n\n\n\n\n\n\n\nNext, we will retrieve three villages with highest local \\(G_i^∗\\) values and lowest local \\(G_i^∗\\) values respectively. These villages will serve as focus areas for Mann-Kendall Trend Test later.\n\nset.seed(123)\n\nthree_hotspots &lt;- (head((HCSA_sig[HCSA_sig$gi_star &gt; 4,]), 3)$label)\nthree_coldspots &lt;-  (head((HCSA_sig[HCSA_sig$gi_star &gt; -2,]), 3)$label)\n\nthree_hotspots\n\n[1] \"Haidian Vil. Annan District\" \"Xiqi Vil. Annan District\"   \n[3] \"Da'an Vil. Annan District\"  \n\nthree_coldspots\n\n[1] \"Qingcao Vil. Annan District\" \"Guo'an Vil. Annan District\" \n[3] \"Xuedong Vil. Annan District\"\n\n\nLet’s proceed to visualise the three most significant hotspots and the three most significant coldspots that we have identified, by plotting them on the map using the appropriate tmap functions.\n\n\nShow the code\nHCSA_three_hotspots &lt;- HCSA_sig %&gt;% filter(label %in% three_hotspots)\nHCSA_three_coldspots &lt;- HCSA_sig %&gt;% filter(label %in% three_coldspots)\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_three_hotspots)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#f67774\")) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_text(\"label\",auto.placement = T)+\n  tm_layout(main.title = \"Three Most Significant Hotspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\nShow the code\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_three_coldspots)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\")) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_text(\"label\", auto.placement = T)+\n  tm_layout(main.title = \"Three Most Significant Coldspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nBased on the output above, we can draw the following insights\nHighest Local Gi* Hotspots: Haidian Village (Annan District), Xiqi Village (Annan District) and Da’an Village (Annan District) are three villages that exhibit highest local Gi* values and are hence most significant hotspots. Interestingly, all villages are from Annan District. This concentration of hotspots in Annan District suggests a localized outbreak of dengue cases, which warrants immediate attention and intervention.\nLowest Local Gi* Coldspots: Qingcao Village (Annan District), Guo’an Village (Annan District) and Xuedong Village (Anan District) are three villages that exhibit lowest local Gi* values and hence are most significant coldspots. Surprisingly, all villages are from Annan District, similar to hosspots. This concentration of coldspots in Annan District present a stark contrast from what we observed in Hotspots. In subsequent analysis, we will try to investigate in this matter."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-each-epidemiology-week",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-each-epidemiology-week",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.2 Local Getis-Ord \\(G_i^*\\) for Each Epidemiology Week",
    "text": "8.2 Local Getis-Ord \\(G_i^*\\) for Each Epidemiology Week\nIn previous section, we calculate local \\(G_i^*\\) values using the overall count of cases for each village. In this section, we are interested to analyse the dynamics of dengue case count over each epidemiology week from 31 to 50. To do so, we will have to calculate the local \\(G_i^*\\) that accounts for both spatial and temporal aspects. To do so, we will make use of the spacetime cube dengue_tainan_spt that we created.\n\ndengue_nb &lt;- dengue_tainan_spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n    set_nbs(\"nb\") %&gt;%\n    set_wts(\"wt\")\n\n\nhead(dengue_nb)\n\n# A tibble: 6 × 7\n  Epidemiol_Week VILLCODE    VILLENG       TOWNENG        CASE_COUNT nb    wt   \n           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;\n1             31 67000350032 Qingcao Vil.  Annan District          0 &lt;int&gt; &lt;dbl&gt;\n2             31 67000270011 Bao'an Vil.   Rende District          1 &lt;int&gt; &lt;dbl&gt;\n3             31 67000370005 Chihkan Vil.  West Central …          0 &lt;int&gt; &lt;dbl&gt;\n4             31 67000330004 Dacheng Vil.  South District          0 &lt;int&gt; &lt;dbl&gt;\n5             31 67000350028 Chengbei Vil. Annan District          0 &lt;int&gt; &lt;dbl&gt;\n6             31 67000350030 Chengnan Vil. Annan District          0 &lt;int&gt; &lt;dbl&gt;\n\n\nSince we are interested in understanding the spatio-temporal dynamics of dengue cases over each epidemiology week, we will groups the data by the Epidemiol_Week variable using group_by() function. By doing so, subsequent calculation of gi_star will be performed separately for each epidemiological week. As a result, local_gstart_perm() function will return a list-column fo each epidemiology week, where each element is gi_star value for each village in the particular epidemiology week. Hence, we will use unnest() function to convert list-column to a regular column.\n\ngi_stars_epiweek &lt;- dengue_nb %&gt;%\n  group_by(Epidemiol_Week) %&gt;%\n  mutate(gi_star = local_gstar_perm(CASE_COUNT, nb, wt)) %&gt;%\n  unnest(gi_star)\n\nhead(gi_stars_epiweek)\n\n# A tibble: 6 × 15\n# Groups:   Epidemiol_Week [1]\n  Epidemiol_Week VILLCODE VILLENG TOWNENG CASE_COUNT nb    wt    gi_star    e_gi\n           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1             31 6700035… Qingca… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -0.790 0.00243\n2             31 6700027… Bao'an… Rende …          1 &lt;int&gt; &lt;dbl&gt;   0.128 0.00395\n3             31 6700037… Chihka… West C…          0 &lt;int&gt; &lt;dbl&gt;  -0.530 0.00333\n4             31 6700033… Dachen… South …          0 &lt;int&gt; &lt;dbl&gt;  -0.339 0.00304\n5             31 6700035… Chengb… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -0.867 0.00277\n6             31 6700035… Chengn… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -1.07  0.00313\n# ℹ 6 more variables: var_gi &lt;dbl&gt;, p_value &lt;dbl&gt;, p_sim &lt;dbl&gt;,\n#   p_folded_sim &lt;dbl&gt;, skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test-for-trends",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test-for-trends",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.3 Mann-Kendall Test for Trends",
    "text": "8.3 Mann-Kendall Test for Trends\nNext step of our analysis will be Mann-Kendall Trend Test. The Mann-Kendall statistical test for trend is used to assess whether a set of data values is increasing or decreasing over time, and whether the trend in either direction is statistically significant. It is a non-parametric test and compares the relative magnitudes of sample data rather than the data values themselves (Gilbert, 1987).\nMann-Kendall trend test genereate two values - Kendall’s Tau tau and Kendall Score S.\nKendall’s Tau (τ) is a correlation coefficient and is a measure of the relationship between two variables. The Tau correlation coefficient returns a value of 0 to 1. Kendall’s Tau can be calculate by the formula \\((C-D/C+D)\\) where C is the number of concordant pairs and D is the number of discordant pairs. Kendall’s Tau is used to test the following hypotheses.\n\nNull Hypothesis: the correlation coefficient τ = 0 (There is no correlation.)\nAlternative Hypothesis: the correlation coefficient τ ≠ 0 (There is a correlation.)\n\nKhambhammettu (2005) explained the methodology of Kendall Score S as follows. The initial value of the Mann-Kendall statistic, S, is assumed to be 0 (e.g., no trend). If a later data point is higher than an earlier one, S is incremented. Conversely, if a later data point is lower, S is decremented. The final value of S indicates the overall trend. A strongly positive value of S suggests an upward trend, while a deeply negative value indicates a downward trend. To statistically assess the significance of the trend, the associated p-value is calculated.\nTo implement Mann-Kendall trend testing in R, MannKendall() function from Kendall package can be used. In this section, we will run Mann-Kendall Test for three most significant hotspots and three most significant coldspots that we identified in our HCSA analysis.\n\n8.3.1 Trend Test of Three Most Significant Hotspots\nWe will first filter out all \\(G_i^*\\) values for each village.\n\nHaidian Village (Annan District)Xiqi Village (Annan District)Da’an Village (Annan District)\n\n\n\ncbg_hd &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Haidian Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_xq &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Xiqi Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_da &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Da'an Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\nWe are now set to visualize the trend of the \\(G_i^*\\) values of three most significant hotspots across epidemiological weeks 31 to 50. To achieve this, we will use relevant ggplot2 functions to create an interactive plot.\n\np_hotspots &lt;- ggplot() +\n  geom_line(data = cbg_hd, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Haidian Village\")) +\n  geom_line(data = cbg_xq, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Xiqi Village\")) + \n  geom_line(data = cbg_da, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Da'an Village\")) +\n  labs(x = \"Epidemiology Week\", y = \"Gi* Value\", \n       title = \"Gi* of Three Most Significant Hotspots Over Epidemiology Week 31-50\",\n       color = \"Village\")\n\nplotly::ggplotly(p_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nIt is interesting to see that all three villages have very similar trend pattern. In fact, these three villages are neighbouring villages and it appears that they all experience almost same outbreak pattern. The Gi* values of all three villages peaked at Week 34 before declining dramatically by Week 39. Afterwards, they all experience wild fluctuations in Gi* values and in fact end up as coldspots by the end of Week 50!! These villages were identified as significant hotspots due to the cumulative count of cases. However, this approach overlooks the temporal nuances and changes in disease clustering over time. Unless we conduct Mann-Kendall trend test, we would not realise that they end up as coldspots by the end.\n\n\nWe will now calculate the Kendall’s tau and Kendall score S for each village using MannKendall() function from Kendall package.\n\nHaidian Village (Annan District)Xiqi Village (Annan District)Da’an Village (Annan District)\n\n\n\ncbg_hd %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.579 0.000406  -110  190.   950\n\n\n\n\n\ncbg_xq %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau        sl     S     D  varS\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.642 0.0000865  -122  190.   950\n\n\n\n\n\ncbg_da %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.568 0.000517  -108  190.   950\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFor all three villages, the Kendall’s tau values are negative, indicating a negative association over time. This suggests that as time progresses, the spatial clustering of disease cases tends to decrease in these villages. The negative S scores for all villages further support this decreasing trend. The p-values sl for all three villages are less than 0.05, indicating that these results are statistically significant.\n\n\n\n\n8.3.2 Trend Test of Three Most Significant Coldspots\nWe will first filter out all \\(G_i^*\\) values for each village.\n\nQingcao Village (Annan District)Guo’an Village (Annan District)Xuedong Village (Annan District)\n\n\n\ncbg_qc &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Qingcao Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_ga &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Guo'an Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_xd &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Xuedong Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\nWe are now set to visualize the trend of the \\(G_i^*\\) values of three most significant coldspots across epidemiological weeks 31 to 50. To achieve this, we will use relevant ggplot2 functions to create an interactive plot.\n\np_coldspots &lt;- ggplot() +\n  geom_line(data = cbg_qc, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Qingcao Village\")) +\n  geom_line(data = cbg_ga, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Guo'an Village\")) + \n  geom_line(data = cbg_xd, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Xuedong Village\")) +\n  labs(x = \"Epidemiology Week\", y = \"Gi* Value\", \n       title = \"Gi* of Three Most Significant Coldspots Over Epidemiology Week 31-50\",\n       color = \"Village\")\n\nplotly::ggplotly(p_coldspots)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the plot, it appears that Xuedong Village and Qingcao Village shares very similar trend, likely due to their spatial proximity. Both remained as coldspots throughout the entirety of epidemiology weeks 31-50. What is interesting was Guo’an Village, which initially started at Gi* value of as low as 0.6 at Week 31 and suddenly reached a peak of around 6.5 at week 34, indicating a sudden outbreak. However, this was followed by a rapid decline, falling below 0 by Week 40 and remaining as a coldspot until the end of the period.\n\n\nWe will now calculate the Kendall’s tau and Kendall score S for each village using MannKendall() function from Kendall package.\n\nQingcao Village (Annan District)Guo’an Village (Annan District)Xuedong Village (Annan District)\n\n\n\ncbg_qc %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau    sl     S     D  varS\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.105 0.538   -20  190.   950\n\n\n\n\n\ncbg_ga %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau      sl     S     D  varS\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.526 0.00132  -100  190.   950\n\n\n\n\n\ncbg_xd %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n      tau    sl     S     D  varS\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0105 0.974    -2  190.   950\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFor all three villages, the Kendall’s tau values are negative, indicating a negative association over time. This suggests that as time progresses, the spatial clustering of disease cases tends to decrease in these villages.\nHowever, for Qingcao Village, the p-value sl is 0.538, which is greater than 0.05, indicating that this result is not statistically significant. Similary, Xuedong Village has the p-value is 0.974, which is much greater than 0.05, indicating that this result is not statistically significant. This means that the observed trend in these two villages could be due to random chance.\nGuo’an Village shows a strong negative association over time, with a tau value of -0.526 and an S score of -100. The p-value is 0.00132, which is less than 0.05, indicating that this result is statistically significant. This suggests a significant decrease in disease spread over time in this village.\n\n\nWe can replicate the operation of MannKendall test for each location by using group_by() function of dplyr package. The results of the MannKendall test will then be stored in a new object called trend_combined.\n\ntrend_combined &lt;- gi_stars_epiweek %&gt;%\n  group_by(VILLENG) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\ntrend_combined\n\n# A tibble: 249 × 6\n   VILLENG         tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Andong Vil.  0.432  0.00859      82  190.   950\n 2 Anfu Vil.   -0.611  0.000191   -116  190.   950\n 3 Anhe Vil.    0.579  0.000406    110  190.   950\n 4 Ankang Vil. -0.0316 0.871        -6  190.   950\n 5 Anqing Vil.  0.337  0.0410       64  190.   950\n 6 Anshun Vil.  0.0526 0.770        10  190.   950\n 7 Anxi Vil.    0.137  0.417        26  190.   950\n 8 Bao'an Vil.  0.0316 0.871         6  190.   950\n 9 Beihua Vil.  0.674  0.0000378   128  190.   950\n10 Beimen Vil.  0.0316 0.871         6  190.   950\n# ℹ 239 more rows\n\n\nAs the next step, we will remove the results that are not statistically significant. To do so, we will filter the data frame to include only those rows where the sl value is less than 0.05.\n\ntrend_combined_sig &lt;- trend_combined %&gt;% filter(sl &lt; 0.05)\ntrend_combined_sig\n\n# A tibble: 90 × 6\n   VILLENG            tau        sl     S     D  varS\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Andong Vil.      0.432 0.00859      82  190.   950\n 2 Anfu Vil.       -0.611 0.000191   -116  190.   950\n 3 Anhe Vil.        0.579 0.000406    110  190.   950\n 4 Anqing Vil.      0.337 0.0410       64  190.   950\n 5 Beihua Vil.      0.674 0.0000378   128  190.   950\n 6 Chengda Vil.     0.474 0.00388      90  190.   950\n 7 Chengde Vil.     0.505 0.00205      96  190.   950\n 8 Chenghuang Vil.  0.516 0.00165      98  190.   950\n 9 Chihkan Vil.     0.653 0.0000659   124  190.   950\n10 Chongcheng Vil. -0.526 0.00132    -100  190.   950\n# ℹ 80 more rows\n\n\nNext, we will identify the top 5 villages with the most significant emerging trends in \\(G_i^*\\) values, either upward or downward.\n\nemerging_vill &lt;- trend_combined_sig %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)\nemerging_vill\n\n# A tibble: 5 × 6\n  VILLENG          tau         sl     S     D  varS\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Fuqian Vil.    0.768 0.00000250   146  190.   950\n2 Chongxin Vil. -0.737 0.00000649  -140  190.   950\n3 Wuwang Vil.   -0.716 0.0000119   -136  190.   950\n4 Chongde Vil.  -0.695 0.0000214   -132  190.   950\n5 Chongxue Vil. -0.695 0.0000214   -132  190.   950\n\n\n\nFuqian Village (West Central District)Chongxin Village (East District)Wuwang Village (Yongkang District)Chongde Village (East District)Chongxue Village (East District)\n\n\n\ncbg_fuqian &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Fuqian Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_fuqian &lt;- ggplot() +\n  geom_line(data = cbg_fuqian, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_fuqian)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Fuqian Village exhibit noticeable fluctuations across the epidemiological weeks The Gi* values tends to fall below zero, indicating a coldspot during the early weeks until Epidemiology Week 41 when it first reaches a positive value. A sharp peak is observed at Week 49 where Gi* value soared beyond 3. This suggests a strong clustering of high values (hotspot) during this week. It could indicate a sudden outbreak or spread of dengue cases in certain areas. However, by the conclusion of Week 50, the Gi∗ value recedes to approximately 1.4.\n\n\n\n\ncbg_chongxin &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongxin Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongxin &lt;- ggplot() +\n  geom_line(data = cbg_chongxin, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongxin)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongxin Village exhibit fluctuations across the epidemiological weeks. There’s an initial round of increase and decrease in local Gi* values between Epidemiology Week 30 to 37, but the values relatively stable between 3 to 5. Starting from Week 37, the trend heads downward gradually, reaching the dip at Week 48 with local Gi* value of -0.68. This could suggest that the village has effectively managed to control the outbreak, leading to a gradual decline in case count and ultimately transforming into a cold spot.\n\n\n\n\ncbg_wuwang &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Wuwang Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_wuwang &lt;- ggplot() +\n  geom_line(data = cbg_wuwang, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_wuwang)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Wuwang Village starts at a whopping 10.32 at Epidemiology Week 31, signifying an extremely significant hotspot. However, the local Gi* values drastically drop over the subsequent weeks and reach at 1.64 by Week 35. From this point forward, the values oscillate between 0 and 2 without any further increase. This could suggest that the village has effectively managed to control the outbreak and prevent any potential second outbreak.\n\n\n\n\ncbg_chongde &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongde Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongde &lt;- ggplot() +\n  geom_line(data = cbg_chongde, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongde)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongde Village exhibit the peak at 6.55 at Epidemiology Week 32, signifying an significant hotspot. After week 32, there is a decline in the local Gi* values, followed by subsequent smaller peaks and troughs, indicating fluctuations in the values over the weeks. It eventually goes below 0 at Week 46. The village experience a sudden surge at Week 49 to 1.5, potentially signaling the onset of a second outbreak. Despite this, the value ultimately falls back to -1 by Week 50. This could suggest that the village was successful in averting the second outbreak.\n\n\n\n\ncbg_chongxue &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongxue Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongxue &lt;- ggplot() +\n  geom_line(data = cbg_chongxue, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongxue)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongxue Village exhibit very similar pattern as Chongde Village. It peaks at 6.37 at Epidemiology Week 32, signifying an significant hotspot. After week 32, there is a decline in the local Gi* values, followed by subsequent smaller peaks and troughs, indicating fluctuations in the values over the weeks. The value eventually goes below 0 at Week 46. The village experience a sudden surge at Week 49 to 1.8, potentially signaling the onset of a second outbreak. Despite this, the value ultimately falls back to -1.1 by Week 50. This could suggest that the village was successful in averting the second outbreak."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hot-spot-analysis-ehsa",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.4 Performing Emerging Hot Spot Analysis (EHSA)",
    "text": "8.4 Performing Emerging Hot Spot Analysis (EHSA)\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object dengue_tainan_spt, and the name of the variable of interest CASE_COUNT for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = dengue_tainan_spt, \n  .var = \"CASE_COUNT\", \n  k = 1, \n  nsim = 99\n)\n\ntainan_ehsa &lt;- study_area_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(VILLCODE == location)) %&gt;%\n  mutate(label = paste(VILLENG,TOWNENG))\n\n\n8.4.1 Visualizing Distribution of EHSA Classes\nBefore creating EHSA maps, we can start by plotting a bar chart to reveal different EHSA classes that have been identified and number of villages in each classification. To do so, we will plot a histogram using ggplot2 functions.\n\nggplot(data = ehsa,\n       aes(y = classification,fill = classification)) +\n  geom_bar(show.legend = FALSE)\n\n\n\n\nBased on the figure above, the EHSA analysis has identified a total of 8 distinct hotspot and coldspot classes. The identified hotspot classes include consecutive hotspots, new hotspots, oscillating hotspots, and sporadic hotspots. The identified coldspot classes include consecutive coldspots, oscillating coldspots, and sporadic coldspots. It is interesting to see that oscillating hotspots and oscillating coldspots constitute the highest and second highest count of villages respectively, which seems to suggest that a significant number of villages undergo periodic fluctuations in disease incidence.\nHowever, it is imperative to evaluate the statistical significance of these findings. To accomplish this, we will isolate only those EHSA classes that possess a p-value less than 0.05.\n\ntainan_ehsa_sig &lt;- tainan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\nNext, we will try to plot a new bar chart with tainan_ehsa_sig to observe any changes in distribution after the exclusion of statistically insignificant classes.\n\nggplot(data = tainan_ehsa_sig,\n       aes(y = classification, fill = classification))+\n  geom_bar(show.legend = FALSE)\n\n\n\n\nIt seems that the count of oscillating coldspots has dropped significantly while oscillating hotspots remain consistent.\n\n\n8.4.2 Visualizing EHSA Maps\nIn this section, we will create EHSA maps to identify the geographic distribution of different hotspots and coldspots in our study area. Prior to this, we will formulate two new sf objects, tainan_ehsa_sig_cold and tainan_ehsa_sig_hot which will respectively represent the statistically significant coldspots and hotspots within our study area.\n\ntainan_ehsa_sig_cold &lt;- tainan_ehsa_sig %&gt;% filter(classification %in% c(\"consecutive coldspot\",\"oscilating coldspot\",\"sporadic coldspot\"))\n  \ntainan_ehsa_sig_hot &lt;- tainan_ehsa_sig %&gt;% filter(classification %in% c(\"consecutive hotspot\",\"new hotspot\",\"oscilating hotspot\",\"sporadic hotspot\"))\n\n\n\n8.4.3 Emerging Hotspots of Dengue Cases in Study Area\nIn this section, we will employ relevant tmap functions to visualize the identified emerging hotspots of dengue cases across the study area.\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_hot)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f67774\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_hot)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f67774\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n8.4.4 Emerging Coldspots of Dengue Cases in Study Area\nIn this section, we will employ relevant tmap functions to visualize the identified emerging coldspots of dengue cases across the study area.\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_cold)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Coldspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_cold)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n8.4.5 Emerging Hospots and Coldspots of Dengue Cases in Tainan City\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#de573e\",\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Tainan City\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            legend.hist.width = 0.5,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#de573e\",\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\n8.5.4 Visualising EHSA Map for Each Township\nIn our previous section, we have prepared and conducted analysis of emerging hotspots and coldspots across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the hotspots and coldspots within each individual district.\nFirstly, we will filter tainan_ehsa and tainan_ehsa_sig objects to each districts within our study area. Following this, we will generate EHSA for each district. These maps will serve as the basis for our subsequent discussions and analyses of the findings.\n\n\nShow the code\nannan_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Annan District\")\nrende_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"South District\")\neast_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"East District\")\nnorth_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"North District\")\n\n\nannan_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Annan District\")\nrende_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"South District\")\neast_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"East District\")\nnorth_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"North District\")\n\n\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\ntm_shape(annan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"RIGHT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nAnnan District is predominantly characterized by hotspots rather than coldspots. A significant number of oscillating hotspots are observed in this district, suggesting that these villages, despite being coldspots at one point, have experienced a rise in case counts. Two new hotspots have been identified - Chengnan Village and Chengbei Village. Additionally, Chengxi Village has been identified as a sporadic hotspot, which may indicate intermittent outbreaks of dengue cases in the village. In close proximity to these hotspots are oscillating coldspots, suggesting that these villages have effectively controlled the spread of cases, reduced the case counts, and eventually transitioned into coldspots.\n\n\n\n\n\nShow the code\ntm_shape(rende_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Rende District\",\n           main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"LEFT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nRende district observes a mixture of coldspots, hotspots and villages with no pattern detected. Interestingly, two villages - Chenggong Village and Wenxian Village - are identified as oscillating hotspots, while two others - Dajia Village and Renyi Village - are identified as oscillating coldspots. This contrast may suggest a non-uniform spatial distribution of dengue cases, influenced by localized factors that either increase or decrease case counts. Additionally, one village, Bao’an Village, is identified as a sporadic coldspot. This could indicate intermittent periods of lower case counts in this area.\n\n\n\n\n\nShow the code\ntm_shape(yongkang_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nYongkand district is predominantly characterised by hotspots. Three villages - Ankang Village, Xishi Village, and Wangxing Village - stand out as consecutive hotspots. This could suggest that these villages have maintained statistically significant dengue clusters over the observed period. Upon closer inspection, it appears that many villages neighboring these consecutive hotspots are identified as oscillating hotspots. This indicates that dengue cases have been spreading from the consecutive hotspots to their neighbors, which were previously coldspots. It is critically important for the local governments to implement more proactive and effective vector control and intervention measures to prevent the outbreak from spreading further.\nIt is also interesting to see three oscilating coldspots - Erwang Village, Zhongxing Village, Zhengqiang Village - and one sporadic coldspot - Guangfu Village, despite being surrounded by hotspots. These villages may have implemented localized measures that have effectively prevented the spread of cases from their neighboring hotspots.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(anping_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(anping_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Anping District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(anping_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(anping_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nAnping District observes a mixture of coldspots and hotspots. Three villages - Yizai Village, Pingtong Village, and Ping’an Village - are identified as oscillating hotspots. In contrast, two villages - Huaping Village and Jianping Village - are identified as oscillating coldspots. Yiping Village is singled out as a sporadic coldspot. Overall, it appears that Anping District has generally low case counts, resulting in a predominance of coldspots over the observed period. However, the presence of three oscillating hotspots signals potential for the spread of outbreaks, necessitating effective containment measures.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(westcentral_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(westcentral_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for West Central District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(westcentral_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(westcentral_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nWest Central District is predominantly chracterised by hotspots, evident by one sporatic hotspot - Xihu Village, three oscilating hotspot - Xihe Village, Duiyue Village, Wutiaogang Village and one new hotspot - Chuhkkan Village. It appears that the cases have been spreading eastward from Xihu Village to its neighboring villages. Interestingly, Xixan Village, which is located adjacent to Xihu Village, is classified as an oscillating coldspot. This suggests that Xixan Village has managed to contain the spread from Xihu Village and gradually reduce the case count.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(south_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            legend.position = c(\"LEFT\", \"TOP\"),\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"RIGHT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSouth District is predominantly characterized by hotspots, either oscilating hotspots or sporadic hotspots. Interestingly, two villages - Zhangnan Village and Wennan Village - located in proximity to these hotspots, are identified as oscillating coldspots. This suggests that these two villages have implemented effective localized measures that have successfully curtailed the spread of cases from their neighboring hotspots, thereby reducing the case count over the observed period.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(east_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nEast District is predominantly characterised by hotspots, majority of which are either sporadic or oscilating hotspots. Datong Village is classified as consecutive hotspots. There are two oscilating coldspots observed - Quannan Village and Dongzhi Village. Particularly intriguing is the fact that Quannan Village, despite being in immediate proximity to Datong Village, has managed to contain the spread of cases from Datong Village and transition into a coldspot.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(north_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nNorth District exhibits a blend of oscillating hotspots, sporadic hotspots, and oscillating coldspots. Intriguingly, North District is the only district in our study area that records a consecutive coldspot - Zhonglou Village. Despite its two neighboring villages, Zhangsheng Village and Ren’ai Village, being classified as an oscillating hotspot and a sporadic hotspot respectively, it is remarkable to observe that Zhonglou Village has successfully prevented the spread of cases from these hotspots throughout the entire observed period, maintaining its status as a stable coldspot. This village will be of interest for further investigation to understand the localized factors and measures that might have contributed to this successful containment."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "References",
    "text": "References\nGriffith, D. (2017). Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed). DOI: 10.22224/gistbok/2017.4.13\nKhambhammettu, P. (2005). Mann-Kendall Analysis for the Fort Ord Site (Report No. OU-1 2004 Annual Groundwater Monitoring Report-Former Fort Ord, California). HydroGeoLogic, Inc.\nGilbert, R.O. (1987). Statistical methods for environmental pollution monitoring. Van Nostrand Reinhold.\nMergenthaler, C., Gurp, M., Rood, E., & Bakker, M. (2022). The study of spatial autocorrelation for infectious disease epidemiology decision-making: A systematized literature review. CABI Reviews. https://doi.org/10.1079/cabireviews202217018\nMoraga, P. (2024). Spatial neighbourhood matrices. In Spatial statistics for Data Science: Theory and practice with R (pp. 83–94). CRC Press.\nHaining, R. P. (2001). Spatial autocorrelation. International Encyclopaedia of the Social & Behavioural Sciences, 14763–14768. https://doi.org/10.1016/b0-08-043076-7/02511-0\nSalima, B. A., & Bellefon, M.-P., (2018). Spatial autocorrelation indices. In Handbook of Spatial Analysis: Theory and Application with R (pp. . INSEE.\nAnselin, L. (2020). Contiguity-Based Spatial Weights. GeoDa: An Introduction to Spatial Data Science. https://geodacenter.github.io/workbook/4a_contig_weights/lab4a.html\nAnselin, L. (2018). Applications of Spatial Weights. GeoDa: An Introduction to Spatial Data Science. https://geodacenter.github.io/workbook/4d_weights_applications/lab4d.html\nGetis, A., & Ord, J. K. (1992). The analysis of Spatial Association by use of Distance Statistics. Geographical Analysis, 24(3), 189–206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#overview",
    "href": "In-class_Ex/In-class_Ex02.html#overview",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "title": "In-Class Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow for reading and writing Apache Parquet files\nlubridate for tackling with temporal data (dates and times)\ntidyverse for manipulating and wrangling data, as well as, implementing data science functions\ntmap for creating and visualizing thematic maps\nsf for handling geospatial data.\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "href": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "title": "In-Class Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia.\n\nApart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\n\n\n3.1 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format.\n\nFirstly, we will use read_parquet function from arrow package\n\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet')\ndf_1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet')\n\n\nNext, we will use head() function to quickly scan through the data columns and values.\n\n\nhead(df)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)\n\n\n\n3.2 Extracting Trip Starting Locations and Temporal Data Values\nAfter loading the Grab-Posisi dataset, we will extract features that we want to use for analysis. Firstly, we will extract trip starting locations for all trajectories in the dataset and save it into a new df called origin_df.\nAlso, we are interested to derive useful temporal data such as day of the week, hour, and yy-mm-dd. To do so, we will use the following functions from lubridate package, and add the newly derived values as new columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n3.3 Extracting Trip Ending Locations and Temporal Data Values\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, for destination_df, we use arrange(desc()) argument to sort the timestamps in descending order\n\n\n\n\n3.4 Saving R Objects in RDS Format\nRDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow us to preserve the state of an object between R sessions. Saving R object as an RDS file in R can be useful for sharing our work with others, replicating our analysis, or simply storing our work for later use.\n\nwrite_rds(origin_df, \"../data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"../data/rds/destination_df.rds\")\n\n\n\n3.4 Importing RDS Objects\n\norigin_df &lt;- read_rds(\"../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../data/rds/destination_df.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04.html",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates. Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights. Particularly, we will explore using spdep and GWmodel.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf\nsp\nspdep,\ntmap\ntidyverse\nknitr\nGWmodel\n\n\npacman::p_load(sf, sp, spdep, tmap, tidyverse, knitr, GWmodel)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nhunan_sp &lt;- hunan %&gt;% as_Spatial()\n\n\n\n\n\nThis function calculates basic and robust GWSS, including geographically weighted means, standard deviations and skew. Robust alternatives include geographically weighted medians, inter-quartile ranges and quantile imbalances. This function also calculates basic geographically weighted covariances together with basic and robust geographically weighted correlations.\n\ngwstat &lt;- gwss(data= hunan_sp,\n               vars = \"GDPPC\",\n               bw = 6,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\nWhat can we learn from this code chunk?\n\nThe data argument is set to hunan_sp, which means the data being used is stored in the hunan_sp variable.\nThe vars argument is set to \"GDPPC\". This indicates that the variable of interest in the hunan_sp data is \"GDPPC\".\nThe bw argument is set to 6. This is the bandwidth parameter for the geographical weighting, which controls the degree of smoothing.\nThe kernel argument is set to \"bisquare\". This means the bisquare kernel function is used for weighting.\nThe adaptive argument is set to TRUE, which means the bandwidth is adaptive. In other words, the bandwidth adjusts depending on the density of the data points.\nThe longlat argument is set to T (short for TRUE). This indicates that the data’s coordinates are in longitude and latitude.\n\n\ngwstat\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n\n   ***********************Calibration information*************************\n\n   Local summary statistics calculated for variables:\n    GDPPC\n   Number of summary points: 88\n   Kernel function: bisquare \n   Summary points: the same locations as observations are used.\n   Adaptive bandwidth: 6 (number of nearest neighbours)\n   Distance metric: Great Circle distance metric is used.\n\n   ************************Local Summary Statistics:**********************\n   Summary information for Local means:\nGDPPC_LM \n    Min.  1st Qu.   Median  3rd Qu.     Max. \n10313.11 17616.21 21981.39 28547.05 73545.30 \n   Summary information for local standard deviation :\nGDPPC_LSD \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n  926.3095  3319.0782  5041.7997  7602.3637 24377.4421 \n   Summary information for local variance :\nGDPPC_LVar \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n   858049.3  11030994.6  25419746.0  57798649.1 594259684.0 \n   Summary information for Local skewness:\nGDPPC_LSKe \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-3.74328455 -0.06903772  0.74202796  1.20539540  5.98374890 \n   Summary information for localized coefficient of variation:\nGDPPC_LCV \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n0.04955751 0.15100369 0.22079470 0.33316203 0.95567593 \n\n   ************************************************************************\n\n\nThe output of the gwss() function is a SpatialPointsDataFrame (SDF). We can view the values inside this dataframe.\n\ngwstat_df &lt;- gwstat[[\"SDF\"]]@data\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127\n\n\nGDPPC_LM refers to local means\nGDPPC_LSD refers to local standard deviation\nGDPPC_LVar refers to local variance\nGDPPC_LSKe refers to local skew\nGDPPC_LCV refers to local coefficients of variation\n\n\n\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#overview",
    "href": "In-class_Ex/In-class_Ex04.html#overview",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates. Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights. Particularly, we will explore using spdep and GWmodel."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex04.html#importing-packages",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf\nsp\nspdep,\ntmap\ntidyverse\nknitr\nGWmodel\n\n\npacman::p_load(sf, sp, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex04.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nhunan_sp &lt;- hunan %&gt;% as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#geographically-weighted-summary-statistics-gwss",
    "href": "In-class_Ex/In-class_Ex04.html#geographically-weighted-summary-statistics-gwss",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "This function calculates basic and robust GWSS, including geographically weighted means, standard deviations and skew. Robust alternatives include geographically weighted medians, inter-quartile ranges and quantile imbalances. This function also calculates basic geographically weighted covariances together with basic and robust geographically weighted correlations.\n\ngwstat &lt;- gwss(data= hunan_sp,\n               vars = \"GDPPC\",\n               bw = 6,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\nWhat can we learn from this code chunk?\n\nThe data argument is set to hunan_sp, which means the data being used is stored in the hunan_sp variable.\nThe vars argument is set to \"GDPPC\". This indicates that the variable of interest in the hunan_sp data is \"GDPPC\".\nThe bw argument is set to 6. This is the bandwidth parameter for the geographical weighting, which controls the degree of smoothing.\nThe kernel argument is set to \"bisquare\". This means the bisquare kernel function is used for weighting.\nThe adaptive argument is set to TRUE, which means the bandwidth is adaptive. In other words, the bandwidth adjusts depending on the density of the data points.\nThe longlat argument is set to T (short for TRUE). This indicates that the data’s coordinates are in longitude and latitude.\n\n\ngwstat\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n\n   ***********************Calibration information*************************\n\n   Local summary statistics calculated for variables:\n    GDPPC\n   Number of summary points: 88\n   Kernel function: bisquare \n   Summary points: the same locations as observations are used.\n   Adaptive bandwidth: 6 (number of nearest neighbours)\n   Distance metric: Great Circle distance metric is used.\n\n   ************************Local Summary Statistics:**********************\n   Summary information for Local means:\nGDPPC_LM \n    Min.  1st Qu.   Median  3rd Qu.     Max. \n10313.11 17616.21 21981.39 28547.05 73545.30 \n   Summary information for local standard deviation :\nGDPPC_LSD \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n  926.3095  3319.0782  5041.7997  7602.3637 24377.4421 \n   Summary information for local variance :\nGDPPC_LVar \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n   858049.3  11030994.6  25419746.0  57798649.1 594259684.0 \n   Summary information for Local skewness:\nGDPPC_LSKe \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-3.74328455 -0.06903772  0.74202796  1.20539540  5.98374890 \n   Summary information for localized coefficient of variation:\nGDPPC_LCV \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n0.04955751 0.15100369 0.22079470 0.33316203 0.95567593 \n\n   ************************************************************************\n\n\nThe output of the gwss() function is a SpatialPointsDataFrame (SDF). We can view the values inside this dataframe.\n\ngwstat_df &lt;- gwstat[[\"SDF\"]]@data\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127\n\n\nGDPPC_LM refers to local means\nGDPPC_LSD refers to local standard deviation\nGDPPC_LVar refers to local variance\nGDPPC_LSKe refers to local skew\nGDPPC_LCV refers to local coefficients of variation\n\n\n\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127"
  }
]