[
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "title": "In-Class Exercise 03 NKDE",
    "section": "2.0 Installing and launching the R package",
    "text": "2.0 Installing and launching the R package\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nrgdal, which provides bindings to the ‘Geospatial’ Data Abstraction Library (GDAL) (&gt;= 1.11.4) and access to projection/transformation operations from the PROJ library. In this exercise, rgdal will be used to import geospatial data in R and store as sp objects.\nsp, which provides classes and methods for dealing with spatial data in R. In this exercise, it will be used to manage SpatialPointsDataFrame and SpatiaLinesDataFrame, and for performing projection transformation.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, spNetwork, tmap, classInt, virdis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "title": "In-Class Exercise 03 NKDE",
    "section": "3.0 Importing Data to R Environment",
    "text": "3.0 Importing Data to R Environment\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format.\n\nnetwork &lt;- st_read(dsn=\"../data/geospatial\", \n                   layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\nchildcare &lt;- st_read(dsn=\"../data/geospatial\",\n                     layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\nNext, we will plot the childcare locations (as points) and road network (as lines) as follows.\n\ntmap_mode('view')\ntm_shape(childcare)+\n  tm_dots(col='orange')+\n  tm_shape(network)+\n  tm_lines()\n\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "title": "In-Class Exercise 03 NKDE",
    "section": "4.0 Network Constrained KDE (NetKDE) Analysis",
    "text": "4.0 Network Constrained KDE (NetKDE) Analysis\n\n4.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nlixels &lt;- lixelize_lines(network, 750, mindist = 375)\n\n\n\n4.2 Generating Line Centers\nNext, we will used lines_center() of spNetwork to generate a SpatialPointsDataFrame (i.e. samples) with line centre points.\n\nsamples &lt;- lines_center(lixels)\n\n\n\n4.3 Performing NKDE\nOnce we have obtained all the datasets required, we can perform NKDE by using nkde() function of spNetwork.\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1,nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n4.4 Visualisation\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nNext, we will rescale the density values to help with better mapping results\n\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\", palette=\"plasma\")+\ntm_shape(childcare)+\n  tm_dots()\n\n\n\n\n\ntmap_mode('plot')\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)\nIn practical use, we can use these results to effectively identify areas where new pedestrian roads can be built."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05.html",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "A collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Local spatial autocorrelation statics can be decomposed from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*. In this exercise, we will explore sfdep package in R-environment, with a case study on gross domestic product per captial (GDPPC) of Hunan.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\npacman::p_load(sf, tmap, sfdep, tmap, tidyverse)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan_GDPPC &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\") \n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan_GDPPC &lt;- left_join(hunan_GDPPC,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\ntmap_mode('plot')\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\",\n          style =\"quantile\",\n          palette = \"plasma\",\n          title= \"GDPPC\") + \n  tm_layout(main.title = \"Distribution of GDP per capita by County, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size=2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nNow, we will derive contiguity weight matrix using Queen’s method. To achieve this, we will use st_contiguity and st_weights functions, respectively\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nThe neighbor list is created by st_contiguity()\nThe weight list is created by st_weights()\nThe style = \"W\" argument indicates that the weights should be row-standardized, which is a common choice in spatial analysis\nThe argument .before = 1 indicates that the new variables should be added as the first columns of the data frame.\n\n\n\n\n\n\nWe will now compute global moran’s I values using global_moran_test() function.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nWe will test the Moran’s I values using Monte Carlo simulations with nsim=99. In sfdep, we can do it by using global_moran_perm() function.\n\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe p-value is &lt; 2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#overview",
    "href": "In-class_Ex/In-class_Ex05.html#overview",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "A collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Local spatial autocorrelation statics can be decomposed from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*. In this exercise, we will explore sfdep package in R-environment, with a case study on gross domestic product per captial (GDPPC) of Hunan."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#overview-1",
    "href": "In-class_Ex/In-class_Ex05.html#overview-1",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\npacman::p_load(sf, tmap, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex05.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan_GDPPC &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\") \n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex05.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan_GDPPC &lt;- left_join(hunan_GDPPC,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\ntmap_mode('plot')\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\",\n          style =\"quantile\",\n          palette = \"plasma\",\n          title= \"GDPPC\") + \n  tm_layout(main.title = \"Distribution of GDP per capita by County, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size=2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nNow, we will derive contiguity weight matrix using Queen’s method. To achieve this, we will use st_contiguity and st_weights functions, respectively\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nThe neighbor list is created by st_contiguity()\nThe weight list is created by st_weights()\nThe style = \"W\" argument indicates that the weights should be row-standardized, which is a common choice in spatial analysis\nThe argument .before = 1 indicates that the new variables should be added as the first columns of the data frame.\n\n\n\n\n\n\nWe will now compute global moran’s I values using global_moran_test() function.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\nWe will test the Moran’s I values using Monte Carlo simulations with nsim=99. In sfdep, we can do it by using global_moran_perm() function.\n\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe p-value is &lt; 2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03.html",
    "title": "In-Class Exercise 03",
    "section": "",
    "text": "pacman::p_load(arrow,lubridate,tidyverse,tmap,sf)\n\n\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 03",
    "section": "3.0 Importing Datasets to R Environment",
    "text": "3.0 Importing Datasets to R Environment\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n3.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 03",
    "section": "4.0 Geospatial Data Wrangling",
    "text": "4.0 Geospatial Data Wrangling\n\n4.1. Assigning Stanadrd Coordinate Systems\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n4.2 Creating Coastal Outline\n\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\nplot(sg_sf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "2.0 Literature Review of Spatial Point Pattern Analysis",
    "text": "2.0 Literature Review of Spatial Point Pattern Analysis\nSpatial point pattern analysis is concerned with description, statistical characterization, modeling and visulisation of point patterns over space and making inference about the process that could have generated an observed pattern (Boots & Getis, 1988 ,Rey et al., 2023; Pebesma & Bivand, 2023). According to this theory, empirical spatial distribution of points in our daily life are not controlled by sampling, but a result of an underlying geographically-continuous process (Rey et al., 2023). For example, an COVID-19 cluster did not happen by chance, but due to a spatial process of close-contact infection.\nWhen analysing real-world spatial points, it is important to analyse whether the observed spatial points are randomly distributed or patterned due to a process or interaction (Floch et al., 2018). In “complete random” distribution, points are located everywhere with the same probability and independently of each other. On the other hand, the spatial points can be clustered or dispersed due to an underlying point process. However, it is challenging to use heuristic observation and intuitive interpretation to detect whether a spatial point pattern exists (Baddeley et al., 2015; Floch et al., 2018). Hence, spatial point pattern analysis can be used to detect the spatial concentration or dispersion phenomena.\n\nWhen analysing and interpreting the properties of a point pattern, these properties can be categorized into two: (a) first-order properties and (b) second-order properties (Yuan et al., 2020; Gimond, 2023). First-order properties concern with the characteristics of individual point locations and their variations of their density across space (Gimond, 2023). Under this conception, observations vary from point to point due to changes in the underlying property. Second-order properties focus on not only individual points, but also the interactions between points and their influences on one another (Gimond, 2023). Under this conception, observations vary from place to place due to interaction effects between observations. First-order properties of point patterns are mostly addressed by density-based techniques, such as quadrat analysis and kernel density estimation, whereas, distance-based techniques, such nearest neighbour index and K-functions, are often used to analyse second-order properties since they take into account the distance between point pairs (Yuan et al., 2020; Gimond, 2023)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "3.0 Importing Packages",
    "text": "3.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nsf : provides a standardised way to encode spatial vector data in R environment, facilitating spatial data operations and analysis.\nst : creats simple features from numeric vectors, matrices, or lists, enabling the representation and manipulation of spatial structures in R.\narrow : for reading and writing Apache Parquet files, a columnar storage file format optimized for use with big data processing frameworks.\nlubridate : for tackling with temporal data (dates and times), providing tools to parse, manipulate, and do arithmetic with date-time objects.\nspatstat: A package for statistical analysis of spatial data, specifically Spatial Point Pattern Analysis. This package was provided by Baddeley, Turner and Ruback (2015) and gives a comprehensive list of functions to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\ntidyverse : a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structure.\nraster : reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\ntmap: Packages used for creating static and interactive visualisations summary statistics and KDE layers.\nspatstat : for spatial statistics with a strong focus on analysing spatial point patterns.\nspNetwork : provides several functions to perform spatial analysis on network, including network kernel density estimation and point pattern analysis.\nclassInt : for choosing univariate class intervals for mapping or other graphics purposes.\nviridis : for providing viridis color palette designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency.\ngifski : for converting images to GIF animations, useful for creating dynamic visualizations and presentations.\n\n\npacman::p_load(sf,st,arrow,lubridate,tidyverse,raster,tmap,ggplot2, patchwork,spatstat,spNetwork,classInt,viridis,gifski)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "4.0 Importing Datasets into R Environment",
    "text": "4.0 Importing Datasets into R Environment\n\n4.1 Datasets\nIn this study, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia. Apart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\nMoreover, we will also use OpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nTo extract the Singapore boundary, we will use Master Plan 2019 Subzone Boundary (No Sea), provided by data.gov.sg.\n\n\n4.2 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format. The Singapore portion of the dataset is packaged into a total of 10 Parquet files.\nFirstly, we will use read_parquet function from arrow package, which allows us to read Parquet files into R environment as a data frame (more specifically, a tibble).\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet',as_data_frame = TRUE)\ndf1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet',as_data_frame = TRUE)\ndf2 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00002.snappy.parquet',as_data_frame = TRUE)\ndf3 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00003.snappy.parquet',as_data_frame = TRUE)\ndf4 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00004.snappy.parquet',as_data_frame = TRUE)\ndf5 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00005.snappy.parquet',as_data_frame = TRUE)\ndf6 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00006.snappy.parquet',as_data_frame = TRUE)\ndf7 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00007.snappy.parquet',as_data_frame = TRUE)\ndf8 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00008.snappy.parquet',as_data_frame = TRUE)\ndf9 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00009.snappy.parquet',as_data_frame = TRUE)\n\nTo consolidate all trajectory instances into a single dataframe, we’ll vertically bind all 10 imported dataframes using bind_rows() function from tidyverse package.\n\ndf_trajectories &lt;- bind_rows(df,df1,df2,df3,df4,df5,df6,df7,df8,df9)\n\nTo get a quick overview of the dataset, we’ll first check the number of trajectory instances using dim() function. Then, we’ll use head() function to quickly scan through the data columns and values\n\ndim(df_trajectories)\n\n[1] 30329685        9\n\nhead(df_trajectories)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 30329685 trajectory instances, each with a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\nOperating System used for Data Recording\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf_trajectories$pingtimestamp &lt;- as_datetime(df_trajectories$pingtimestamp)\n\n\n\n4.3 Importing OpenStreetMap road data for Malaysia, Singapore and Brunei\nThe gis_osm_roads_free_1 dataset, which we downloaded from OpenStreetMap, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nosm_road_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"gis_osm_roads_free_1\") %&gt;% st_transform(crs = 3414)\n\n\n\n4.4 Importing Singapore Master Plan Planning Subzone boundary data\nThe MP14_SUBZONE_WEB_PL dataset, which we downloaded from data.gov.sg, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;% st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we use %&gt;% operator is used to pipe the output of st_read() to the st_transform() function. Since the dataset we are using is the Singapore boundary, we need to assign the standard coordinate reference system for Singapore, which is SVY21 (EPSG:3414). st_transform() function transforms the coordinate reference system of the sf object to 3414.\n\n\nAfter importing the dataset, we will plot it to see how it looks. The plot() function is used to plot the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(mpsz_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "5.0 Data Wrangling",
    "text": "5.0 Data Wrangling\nData wrangling is the process of converting and transforming raw data into a usable form and is carried out prior to conducting any data analysis. In this section, we will carry out different data wrangling steps to prepare our datasets ready for analysis.\n\n5.1 Extracting Trip Starting Locations and Temporal Data Values from Grab-Posisi dataset\nFirstly, we will extract trip starting locations for all unqiue trajectories in the dataset and store them to a new df named origin_df. We are also interested in obtaining valuable temporal data such as the day of the week, the hour, and the date (yy-mm-dd). To do so, we will use the following functions from lubridate package, and add the newly derived values as columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         pickup_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n5.2 Extracting Trip Ending Locations and Temporal Data Values from Grab-Posisi dataset\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         dropoff_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, this default order is applied to origin_df. However, for destination_df, the arrange(desc()) argument is used to modify the default order to descending.\n\n\n\n\n5.3 Converting to sf tibble data.frame\norigin_df & destination_df that we created earlier are of class “DataFrame”. However, when carrying out point pattern analysis, we need to convert them to sf object to allow for spatial operations. The sf package introduces the sf data frame, which is a tibble (a modern reimagining of the data frame) that has some special characteristics for handling spatial data. Hence, we will convert origin_df & destination_df into sf tibble data.frame using rowlng and rowlat columns as inputs for coordinates. Subsequently, we will assign appropriate coordinate reference system for Singapore, SVY21 (EPSG:3414).\n\norigin_sf &lt;- st_as_sf(origin_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\ndestination_sf &lt;- st_as_sf(destination_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n5.4 Creating a CoastalLine of Singapore\nThe original mpsz_sf dataset we imported include information of all URA Master Plan planning area boundaries. However, for this analysis, we only need the national-level boundary of Singapore. Hence, we will need to union all the subzone boundaries to one single polygon boundary. Also, Grab ride-hailing service is only available on the main Singapore islands. Hence, we will need to remove outer islands which Grab service is not available. In particular, we will remove the following planning subzones: NORTH-EASTERN ISLANDS, SOUTHERN GROUP, SUDONG & SEMAKAU.\nWe can remove these subzones using the subset() function. The subset() function is used to extract rows from a data frame that meet certain conditions.\n\nnortheasten.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouthern.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\")\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\")\n\nouterislands &lt;- dplyr::bind_rows(list(northeasten.islands,southern.islands,sudong,semakau))\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we first created four new data frames called northeasten.islands, southern.islands, sudong, and semakau by selecting rows from mpsz_sf where the value in the SUBZONE_N column matches the corresponding value.\nAfter that, we used bind_rows() function from the dplyr package to combine these four data frames into a single data frame called outerislands.\n\n\nAfter importing the dataset, we will plot it to see how it looks.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(outerislands))\n\n\n\n\nAs mentioned earlier, we only need to get national-level boundary of Singapore, without outer islands. To do so, we will need to process the mpsz_sf layer to achieve the outcome. - We will first use st_union() function from the sf package to combine the geometries of mpsz_sf and outerislands sf objects into a single geometry each. - Next, we will use st_difference() function then removes the overlapping areas between the two geometries. - Finally, we will store the non-overlapping areas into a new sf objected called sg_sf.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outerislands))\n\nTo assess whether the geometry of the newly created sg_sf matches our intended outcome, we will plot it out.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(sg_sf))\n\n\n\n\n\n\n5.5 Extracting Road Layers within Singapore\nAs we have seen in Section 4.3., osm_road_sf dataset includes road networks from not only Singapore, but also Malaysia and Brunei. However, our analysis is focused on Singapore. Hence, we will need to remove unecessary data rows. To do so, we will\n\nsg_road_sf &lt;- st_intersection(osm_road_sf,lsg_sf)\n\nNext, we will look at the classification of road networks as provided by OpenStreetMap.\n\nunique(sg_road_sf$fclass)\n\n [1] \"primary\"        \"residential\"    \"tertiary\"       \"footway\"       \n [5] \"service\"        \"secondary\"      \"motorway\"       \"motorway_link\" \n [9] \"trunk\"          \"trunk_link\"     \"primary_link\"   \"pedestrian\"    \n[13] \"living_street\"  \"unclassified\"   \"steps\"          \"track_grade2\"  \n[17] \"track\"          \"secondary_link\" \"cycleway\"       \"path\"          \n[21] \"tertiary_link\"  \"track_grade1\"   \"track_grade3\"   \"unknown\"       \n[25] \"track_grade5\"   \"bridleway\"      \"track_grade4\"  \n\n\nLooking at the road classification, it is observed that not all categories are relevant to our analysis, which is primarily concerned with driving networks where taxis can facilitate pick-ups or drop-offs. Hence, we will implement a filtering process on the dataset to exclude road segments that fall outside the scope of our analysis.\nFirstly we will specify the road classes that we want to retain.\n\ndriving_classes &lt;- c(\"primary\", \"primary_link\", \"residential\", \"secondary\", \"secondary_link\", \"service\", \"tertiary\", \"tertiary_link\") \n\nNext, we will filter sg_road_sf object to remove all the rows that does not have our desired f_class attribute value.\n\nsg_driving_sf &lt;- sg_road_sf %&gt;%\n  filter(fclass %in% driving_classes)\nunique(sg_driving_sf$fclass)\n\n[1] \"primary\"        \"residential\"    \"tertiary\"       \"service\"       \n[5] \"secondary\"      \"primary_link\"   \"secondary_link\" \"tertiary_link\" \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhy motorway and motorway_link classes are not included?\nAccording to OpenStreetMap, fclass motorway refers to expressway. In Singapore, stopping or parking a vehicle on an expressway is illegal under the Road Traffic Act. Hence, motorway (and motorway_link) are not relevant for network constraint kernel density estimation (NKDE) analysis that we will carry out later.\n\n\nNow that we have filterd out the dataset, we will now plot to see the driving road network of Singapore using tmap.\n\ntmap_mode(\"plot\")\n\ntm_shape(sg_sf) +\n  tm_polygons() +\ntm_shape(sg_driving_sf) +\n  tm_lines(col=\"fclass\", palette =\"viridis\") +\n  tm_layout(main.title = \"Road Network in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.outside = TRUE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.5 Converting the Simple Features to Planar Point Pattern Object\nIn order to use the capabilities of spatstat packahe, a spatial dataset should be converted into an object of class planar point pattern ppp (Baddeley et al., 2015). A point pattern object contains the spatial coordinates of the points, the marks attached to the points (if any), the window in which the points were observed, and the name of the unit of length for the spatial coordinates. s. Thus, a single object of class ppp contains all the information required to perform spatial point pattern analysis.\nIn previous section, we have created sf objects of Grab trajectory origin and destination points. Now, we will convert them into ppp objects using as.ppp() function from spatstat package.\n\norigin_ppp &lt;- as.ppp(st_coordinates(origin_sf), st_bbox(origin_sf))\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe code chunk above converts the origin_sf object to a point pattern object of class ppp. st_coordinates() function is used to extract the coordinates of the origin_sf object and st_bbox() function is used to extract the bounding box of the origin_sf object. The resulting object origin_ppp is a point pattern object of class ppp.\n\n\n\ndestination_ppp &lt;- as.ppp(st_coordinates(destination_sf), st_bbox(destination_sf))\n\npar(mar = c(0,0,1,0))\nplot(destination_ppp)\n\n\n\n\n\n\n5.6 Handling Data Errors\nBefore going striaght into analysis, we will need to a quick look at the summary statistics of the newly created ppp objects. This is an important step to ensure that the data is free of errors and that a reliable analysis can be performed.\n\n5.6.1 Data Error Handling for origin_ppp\nWe will use summary() function to get summary information of origin_ppp object.\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\nWe can also check if there is any duplicated points in origin_ppp object using any(duplicated() function.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordaintes in the origin_ppp object.\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we need to check duplication?\nWhen analyzing spatial point processes, it is important to avoid duplication of points. This is because statistical methodology for spatial point processes is based largely on the assumption that processes are simple, i.e., that points of the process can never be coincident. When the data have coincident points, some statistical procedures designed for simple point processes will be severely affected (Baddeley et al., 2015).\n\n\n\n\n5.6.2 Data Error Handling for destination_ppp\nWe will use summary() function to get summary information of destination_ppp object.\n\nsummary(destination_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\nWe can also check if there is any duplicated points in destination_ppp object using any(duplicated() function.\n\nany(duplicated(destination_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordinates in the destination_ppp object.\n\n\n\n5.7 Creating Observation Windows\nMany data types in spatstat require us to specify the region of space inside which the data were observed. This is the observation window and it is represented by an object of class owin. In this analysis, our study area is Singapore, hence we will use Singapore boundary as the observation window for spatial point pattern analysis.\nIn Section 5.4, we have already created the sg_sf object, which represents the Singapore boundary (without outer islands). To convert this sf object to owin object, we will use as.owin() function from spatstat package.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot.owin(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of sg_owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46\n\n\n\n\n5.8 Combining ppp objects and owin object\nIn section 5.5, we have created two ppp objects - origin_ppp and destination_ppp, each representing the spatial points of Grab trajectory origin and destination. In section 5.7, we have created a owin object called sg_owin, which represent the observation window of our analysis.\nThe observation window sg_owin and the point pattern origin_ppp or destination_ppp can be combined, so that the custom window replaces the default ractangular extent (as seen in section 5.5).\n\norigin_ppp_sg = origin_ppp[sg_owin]\ndestination_ppp_sg = destination_ppp[sg_owin]\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp_sg)\n\n\n\nplot(destination_ppp_sg)\n\n\n\n\nWe will use summary() function to get summary information of the newly created origin_ppp_sg object and destination_ppp_sg object.\n\nsummary(origin_ppp_sg)\n\nPlanar point pattern:  28000 points\nAverage intensity 3.965129e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46\n\nsummary(destination_ppp_sg)\n\nPlanar point pattern:  27997 points\nAverage intensity 3.964704e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n56 separate polygons (36 holes)\n                  vertices         area relative.area\npolygon 1            15307  7.00834e+08      9.92e-01\npolygon 2              285  1.61128e+06      2.28e-03\npolygon 3               27  1.50315e+04      2.13e-05\npolygon 4 (hole)        41 -4.01660e+04     -5.69e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.24e-05\npolygon 6 (hole)         3 -4.14099e-04     -5.86e-13\npolygon 7               30  2.80002e+04      3.97e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.06e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.57e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.23e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.49e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.31e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.32e-13\npolygon 14 (hole)        3 -3.39794e-04     -4.81e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.40e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.53e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.36e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.60e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.07e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.10e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.46e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.16e-11\npolygon 23              71  8.18750e+03      1.16e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.19e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.10e-05\npolygon 26 (hole)        3 -3.41897e-05     -4.84e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.18e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.01e-11\npolygon 29              91  1.49663e+04      2.12e-05\npolygon 30 (hole)        3 -3.79135e-02     -5.37e-11\npolygon 31 (hole)        5 -2.92235e-04     -4.14e-13\npolygon 32 (hole)        3 -7.43616e-06     -1.05e-14\npolygon 33 (hole)      270 -1.21455e+03     -1.72e-06\npolygon 34 (hole)       19 -4.39650e+00     -6.23e-09\npolygon 35 (hole)       35 -1.38385e+02     -1.96e-07\npolygon 36 (hole)       23 -1.99656e+01     -2.83e-08\npolygon 37              40  1.38607e+04      1.96e-05\npolygon 38 (hole)       41 -6.00381e+03     -8.50e-06\npolygon 39 (hole)        7 -1.40546e-01     -1.99e-10\npolygon 40 (hole)       11 -8.36705e+01     -1.18e-07\npolygon 41 (hole)        3 -2.33435e-03     -3.31e-12\npolygon 42              45  2.51218e+03      3.56e-06\npolygon 43             139  3.22293e+03      4.56e-06\npolygon 44             148  3.10395e+03      4.40e-06\npolygon 45 (hole)        4 -1.72650e-04     -2.44e-13\npolygon 46              75  1.73526e+04      2.46e-05\npolygon 47              83  5.28920e+03      7.49e-06\npolygon 48             106  3.04104e+03      4.31e-06\npolygon 49             264  1.50631e+06      2.13e-03\npolygon 50              71  5.63061e+03      7.97e-06\npolygon 51              10  1.99717e+02      2.83e-07\npolygon 52 (hole)        3 -1.37223e-02     -1.94e-11\npolygon 53             487  2.06117e+06      2.92e-03\npolygon 54              65  8.42861e+04      1.19e-04\npolygon 55              47  3.82087e+04      5.41e-05\npolygon 56              22  6.74651e+03      9.55e-06\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 706156000 square units\nFraction of frame area: 0.46"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "6.0 Exploratory Spatial Data Analysis",
    "text": "6.0 Exploratory Spatial Data Analysis\n\n6.1 Measuring Central Tendency\nDescriptive statistics are used in point pattern analysis to summarise a point pattern’s basic properties, such as its central tendency and dispersion. The mean centre and the median centre are two often employed metrics for central tendency (Gimond, 2019).\n\n6.1.1 Mean Center\nMean center is the arithmetic average of the (x, y) coordinates of all point in the study area. Similar to mean in statistical analysis, mean center is influenced to a greater degree by the outliers. (Yuan et al.,2020)\n\norigin_xy &lt;- st_coordinates(origin_sf)\norigin_mc &lt;- apply(origin_xy, 2, mean)\n\ndestination_xy &lt;- st_coordinates(destination_sf)\ndestination_mc &lt;- apply(destination_xy, 2, mean)\n\norigin_mc\n\n       X        Y \n28490.57 36939.04 \n\ndestination_mc\n\n       X        Y \n28870.96 36590.49 \n\n\nThe results show that the origin and destination mean centres are, respectively, (28490.57, 36939.04) and (28870.96, 36590.49). The two mean centres appear to be situated in close proximity to one another.\n\n\n6.1.2 Median Center\nMedian center is the location that minimizes the sum of distances required to travel to all points within an observation window. It can be calculated using an iterative procedure first presented by Kulin and Kuenne (1962). The procedure begins at a predetermined point, such as the median center, as the initial point. Then, the algorithm updates the median center’s new coordinates (x’, y’) continually until the optimal value is reached. The median center, as opposed to the mean center, offers a more reliable indicator of central tendency as it is unaffected by outliers (Yuan et al., 2020).\n\norigin_medc &lt;- apply(origin_xy, 2, median)\n\ndestination_medc &lt;- apply(destination_xy, 2, median)\n\norigin_medc\n\n       X        Y \n28553.17 36179.05 \n\ndestination_medc\n\n       X        Y \n28855.04 35883.86 \n\n\nBased on the results, the median centres of origin and destination are, respectively, (28553.17, 36179.05) and (28855.04, 35883.86). The two median centres appear to be situated in close proximity to one another.\nMoreover, mean centers and median centers for each origin and destination points are similar. This may imply that the distribution of the data is relatively balanced and there is not a significant difference in the spatial patterns between the origin and destination points. Additionally, this indicates that both the mean center and median center are effective measures for analyzing the central tendency of the data in this context.\n\n\n6.1.3 Plotting Mean and Median Centers\nWe can try to plot both results obtained from previous section on the same plane for better comparison of the mean center and median center.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(origin_medc[1], origin_medc[2]), pch='*', col='purple', cex=3)\n\n\n\n\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='yellow', cex=3)\npoints(cbind(destination_medc[1], destination_medc[2]), pch='*', col='orange', cex=3)\n\n\n\n\n\n\n\n6.2 Measuring Dispersion\n\n6.2.1 Standard Distance\nStandard distances are defined similarly to standard deviations. This indicator measures how dispersed a group of points is around its mean center (Gimond, 2023).\n\norigin_sd &lt;- sqrt(sum((origin_xy[,1] - origin_mc[1])^2 + (origin_xy[,2] - origin_mc[2])^2) / nrow(origin_xy))\n\ndestination_sd &lt;- sqrt(sum((destination_xy[,1] - destination_mc[1])^2 + (destination_xy[,2] - destination_mc[2])^2) / nrow(destination_xy))\n\norigin_sd\n\n[1] 10187.88\n\ndestination_sd\n\n[1] 9545.69\n\n\nFrom the results, the origin and destination standard distances are 10187.88 and 9545.69, respectively. Hence, it appears that origin points are more dispersed than the origin points.\n\n\n\n\n\n\nReflection\n\n\n\nHowever, it would be challenging to discern why the origin points are more dispersed without further analysis. Further analysis would be needed to determine the factors contributing to the increased dispersion of destination points. Since it is out of scope for this exercise, we will not explore any further.\n\n\n\n\n6.2.2 Plotting Standard Distances\nIn this section, we will create bearing circles of origin and destination points using the standard distance values we have calculated earlier. This can provide visual representation of their dispersion and make intuitive comparison between them.\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey', main=\"standard distance of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- origin_mc[1] + origin_sd * cos(bearing)\ncy &lt;- origin_mc[2] + origin_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='red', lwd=2)\n\n\n\n\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey',main=\"standard distance of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ncy &lt;- destination_mc[2] + destination_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='purple', lwd=2)\n\n\n\n\nA better comparison of the standard distances between origin and destination points can also be achieved by trying to plot both results on the same plane.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey',main=\"standard distances of origin_sf & destination_sf\")\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\n\norigin_cx &lt;- origin_mc[1] + origin_sd * cos(bearing)\norigin_cy &lt;- origin_mc[2] + origin_sd * sin(bearing)\n\ndestination_cx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ndestination_cy &lt;- destination_mc[2] + destination_sd * sin(bearing)\n\norigin_circle &lt;- cbind(origin_cx, origin_cy)\ndestination_circle &lt;- cbind(destination_cx, destination_cy)\n\nlines(origin_circle, col='red', lwd=2)\nlines(destination_circle, col='purple', lwd=2)\n\n\n\n\n\n\n\n6.3 Spatial Randomness Test\nClark and Evans (1954) give a very simple test of spatial randomness called Clark and Evans aggregation index (R). It is the ratio of the observed mean nearest neighbour distance in the pattern to that expected for a Poisson point process of the same intensity. R-value &gt;1 suggests ordering, while R-value &lt;1 suggests clustering.\nWe will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\n\n6.3.1 Spatial Randomness Test for Origin Points\nThe test hypotheses are:\n\nH0 = The distribution of trajectory original points are randomly distributed.\nH1= The distribution of trajectory original points are not randomly distributed.\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(origin_ppp_sg,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origin_ppp_sg\nR = 0.27408, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe Clark-Evans test for the origin points shows an R-value of 0.27408, which is less than 1. This indicates a clustered distribution. The p-value is less than 2.2e-16, which is extremely small and less than the significance level of 0.05. This means that we will reject the null hypothesis (H~0) and accept the alternative hypothesis (H~1). Therefore, the statistical inference from this test is that the original points are not randomly distributed but are clustered. This suggests that there may be underlying factors influencing the spatial distribution of these points.\n\n\n6.3.2 Spatial Randomness Test for Destination Points\nThe test hypotheses are:\n\nH0 = The distribution of trajectory destination points are randomly distributed.\nH1= The distribution of trajectory destination points are not randomly distributed.\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(destination_ppp_sg,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  destination_ppp_sg\nR = 0.29484, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe Clark-Evans test for the destination points also shows an R-value of 0.29484, which is less than 1. This indicates a clustered distribution. The p-value is less than 2.2e-16, which is significantly smaller than the significance level of 0.05. Therefore, we reject the null hypothesis (H~0) and accept the alternative hypothesis (H~1). Therefore, the statistical inference from this test is that the destination points are not randomly distributed but are clustered. This suggests that there may be underlying factors influencing the spatial distribution of these points."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "7.0 First-Order Spatial Point Patterns Analysis",
    "text": "7.0 First-Order Spatial Point Patterns Analysis\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package. As we have discussed in Section 2.0., First-order properties concern the characteristics of individual point locations and their variations of their density across space and are mostly addressed by density-based techniques, such as quadrant analysis and kernel density estimation.\nInvestigation of the intensity of a point pattern is one of the first and most important steps in point pattern analysis (Baddeley et al., 2015). If the point process has an intensity function λ(u), this function can be estimated non-parametrically by kernel estimation (Baddeley et al., 2015). Kernel estimation allows for smoothing of the probability density estimation of a random variable (in this analysis a point event) based on kernels as weights.\n\n7.1 Rescaling origin_ppp_sg and destination_ppp_sg\nThe SVY21 Coordinate References System uses meters as the standard unit. Hence, the original_ppp_sg and destination_ppp_sg that we have prepared in the previous sections has “metres” as the unit. However, we will need to convert the measuring unit from metre to kilometeres when calculating the kernel density estimators for entirety of Singapore because kilometers provide a more appropriate scale for analyzing large areas.\n\norigin_ppp_sg.km &lt;- rescale(origin_ppp_sg, 1000, \"km\")\ndestination_ppp_sg.km &lt;- rescale(destination_ppp_sg, 1000, \"km\")\n\n\n\n7.2 Computing Default Kernel Density Estimation\nKernel Destiny Estimation (KDE) generates a surface (raster) representing the estimated distribution of point events over the observation window. Each cell in the KDE layer carries a value representing the estimated density of that location (Wilkin, 2020). Hence, this approach is also known as local density approach. To build the KDE layer, a localised density is calculated for multiple small subsets of the observation window. However, these subsets overlap throughout each iteration, resulting in a moving window defined by a kernel (Wilkin, 2020; Gimond, 2023).\nIn this section, we will focus on destination points as we would like to identify areas that exert a ‘pull’ effect on people, hence resulting in cluster of trajectory destinations. Analyzing the destination of Grab trajectories can provide interesting insights into pull factors within a given area and help identify popular destinations or areas of high mobility demand.\nKernel estimation is implemented in spatstat by the function density.ppp(), a method for the generic command density.\n\npar(mar = c(0,1,1,1))\nkde_default_destination &lt;- density(destination_ppp_sg.km)\nplot(kde_default_destination,main = \"Default Density KDE for Destination Points\")\ncontour(kde_default_destination, add=TRUE)\n\n\n\n\nsigma argument in density() function controls the bandwidth of kernel function. The choice of the bandwidth affects the kernel density estimation strongly. A smaller bandwidth will produce a finer density estimate with all little peaks and valleys. A larger bandwidth will result into a smoother distribution of point densities. Generally speaking, if the bandwidth is too small the estimate is too noisy, while if bandwidth is too high the estimate may miss crucial elements of the point pattern due to over-smoothing (Scott, 2009).\n\n\n\nDensity Estimates with Different Smoothing Bandwidth (Ref, Baddeley et al., 2015)\n\n\nWhen sigma value is not specified, an isotropic Gaussian kernel will be used, with a default value of sigma calculated by a simple rule of thumb that depends only on the size of the window. Hence, the KDE given by default argument may not be what we aim to achieve. Looking at the KDE plot we have created above, there are signs of oversmoothing where only a single spatial cluster in the CBD area being observable. This can severely limit our analysis as potential small-scale clusters and other interesting details are being masked by the oversmoothing effect.\nTo overcome this challenge, we can specify smoothing bandwidth through the argument sigma or kernel function through the argument kernel to compute and plot more intuitive and detailed KDE maps.\n\n\n7.3 Creating KDE Layers with Fixed Bandwidth\n\n7.3.1 Computing Fixed Bandwidths Using Different Bandwidth Selection Methods\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density: In this method, band-width σ is chosen to minimize the mean-square error criterion defined by Diggle (1985). The mean-square error is a measure of the average of the squares of the errors - that is, the average squared difference between the estimated values and the actual value.\nbw.CvL() Cronie and van Lieshout’s Criterion for Bandwidth Selection for Kernel Density: The bandwidth σ is chosen to minimize the discrepancy between the area of the observation window and the sum of reciprocal estimated intensity values at the points of the point process. This method aims to choose a bandwidth that best represents the underlying point process, taking into account both the observed points and the area they occupy.\nbw.scott() Scott’s Rule for Bandwidth Selection for Kernel Density: The bandwidth σ is computed by the rule of thumb of Scott (1992). The bandwidth is proportional to \\(n^{-1/(d-4)}\\) where n is the number of points and d is the number of spatial dimensions. This rule is very fast to compute. It typically produces a larger bandwidth than Diggle’s method. It is useful for estimating gradual trend.\nbw.ppl() Likelihood Cross Validation Bandwidth Selection for Kernel Density: This approach, explained by Loader (1999), uses likelihood cross-validation to determine the bandwidth (σ) by maximizing the point process likelihood. This method is beneficial when the aim is to maximize the likelihood of observing the given data.\n\n\nbw_diggle &lt;- bw.diggle(destination_ppp_sg.km)\nbw_diggle\n\n      sigma \n0.008317447 \n\nbw_CvL &lt;- bw.CvL(destination_ppp_sg.km)\nbw_CvL\n\n   sigma \n3.745658 \n\nbw_scott &lt;- bw.scott(destination_ppp_sg.km)\nbw_scott\n\n  sigma.x   sigma.y \n1.4763217 0.9063352 \n\nbw_ppl &lt;- bw.ppl(destination_ppp_sg.km)\nbw_ppl\n\n    sigma \n0.1913655 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe notice that bw_diggle, bw_CvL and bw_ppl all give a numeric sigma value, whereas bw_scott, by default, provides a separate bandwidth for each coordinate axis. In the code output above, sigma.x = 1.4763217 and sigma.y = 0.9063352 are the estimated bandwidths for the x and y coordinates, respectively. These values represent the amount of smoothing applied in each direction when estimating the kernel density.\nWe can specify isotropic=TRUE argument when calculating bw_scott() method to produce a single value bandwidth.\n\n\n\nbw_scott_single &lt;- bw.scott(destination_ppp_sg.km, isotropic=TRUE)\nbw_scott_single \n\n   sigma \n1.156738 \n\n\nThe optimized bandwidth values generated from above methods belongs to the special class bw.optim. The plot function can be used to see the objective function for the optimisation that leads to the result.\n\npar(mfrow = c(1,2))\nplot(bw_diggle, xlim=c(-0.02,0.05), ylim=c(-60,200))\nplot(bw_CvL)\n\n\n\npar(mfrow = c(1,2))\nplot(bw_scott, main=\"bw_scott\")\nplot(bw_ppl,  xlim=c(-1,5), ylim=c(70000,130000))\n\n\n\n\n\n\n7.3.2 Plotting Fixed-Bandwidth KDE Layers\nIn practice, there are no definite method to choose the KDE bandwidth. Many literature has outlined a diverse range of approaches for KDE bandwidth selection. According to Wolff and Asche (2009), the choice of bandwidth in many existing studies is mostly conducted by visually comparing different bandwidth setting.\nHence, we will now create KDE layers based on each bandwidth selection method and visualize them to have a better comparison of how distinct the resulting KDE layers are.\n\nkde_diggle &lt;- density(destination_ppp_sg.km, bw_diggle)\nkde_CvL &lt;- density(destination_ppp_sg.km, bw_CvL)\nkde_scott &lt;- density(destination_ppp_sg.km, bw_scott)\nkde_ppl &lt;- density(destination_ppp_sg.km, bw_ppl)\n\npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(kde_diggle,main = \"kde_diggle\")\nplot(kde_CvL,main = \"kde_CvL\")\nplot(kde_scott,main = \"kde_scott\")\nplot(kde_ppl,main = \"kde_ppl\")\n\n\n\n\nNext, we will try to plot histograms to compare the distribution of KDE values obtained from density() function using different bandwidth selection methods.\n\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_diggle,main = \"kde_diggle\")\nhist(kde_CvL,main = \"kde_CvL\")\nhist(kde_scott,main = \"kde_scott\")\nhist(kde_ppl,main = \"kde_ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can interpret the outputs as below:\n\nkde_diggle: The sharp peak at the beginning indicates that the Diggle method for bandwidth selection has indentified a high concentration of points in the first bin. The rest of the bins has little to no concentration. This may suggest that one specific area in our observation window has observed a relatively high spatial clustering than the rest of the window.\nkde_CvL: The more balanced distribution suggests that the CvL method for bandwidth selection is identifying a broader range of spatial point concentration. However, the bin sizes are quite small, which smooths out the overall distribution and masks some of the finer details.\nkde_scott: The wider range of values and less sharp peak compared to kde_diggle indicate that the Scott method is capturing a wider range of spatial point concentrations, including both densely concentrated locations and moderately concentrated ones.\nkde_ppl: The result is very similar to the Diggle method, the sharp peak at the beginning suggests a high concentration of points in a specific area, suggest that one specific area in our observation window has observed a relatively high spatial clustering than the rest of the area.\n\n\n\nAnother apporach to compare the KDE layers is to calculate the standard error of each density estimation. $SE is used to extract the standard error of the density estimate from the output of the density() function\n\ndse_diggle &lt;- density(destination_ppp_sg.km, bw_diggle, se=TRUE)$SE\ndse_CvL &lt;- density(destination_ppp_sg.km, bw_CvL, se=TRUE)$SE\ndse_scott &lt;- density(destination_ppp_sg.km, bw_scott, se=TRUE)$SE\ndse_ppl &lt;- density(destination_ppp_sg.km, bw_ppl, se=TRUE)$SE\n\n\npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(dse_diggle,main = \"standard error_diggle\")\nplot(dse_CvL,main = \"standard error_CvL\")\nplot(dse_scott,main = \"standard error_scott\")\nplot(dse_ppl,main = \"standard error_ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe standard error (SE) of the density estimate provides a measure of the uncertainty associated with the density estimate at each point. This can be useful for understanding the variability of density estimates, especially when comparing density estimates obtained using different bandwidths.\nHowever, in many applications of KDE, the focus is often on the shape of the density estimate rather than its absolute value. In such cases, the standard error might not be as relevant. Hence, in this analysis, we will not use standard error as a criteria for choosing the bandwidth.\n\n\n\n\n7.3.3 Choosing Fixed KDE Bandwidth Selection Method\nUpon the exploration of various fixed bandwidth selection methods for computing KDE vales, and subsequent plotting of the respective KDE estimates, their distributions and associated standard errors, we will now select the KDE bandwidth to be used in our analysis. As we have seen in Section 7.3.1.2, each KDE bandwidth method has produced a distinct KDE and there is no definite method to choose the KDE bandwidth.\nWe will proceed to choose bw_scott method for further analysis. This is because:\n\nbw_scott method provides a pair of bandwidth values for each coordinate axis. This allows it to capture the different levels of spatial clustering in each direction more accurately.\nbw_scott method capture the balance between bias and variance the best among all methods. If the bandwidth is too small, the estimate may be too skewed (high variance). The distribution histograms of KDE layers using bw_diggle and bw_ppl tend to indicate such nature. On the other hand, if the bandwidth is too large, the estimate may be oversmoothed, missing crucial elements of the point pattern (high bias). This is what we observed in the distribution histogram of KDE layer using bw_CvL.\n\nSince we have chosen to use bw_scott method, now we will plot the KDE layer using this method for further analysis.\n\npar(mar = c(0,1,1,1))\nbw_fixed_scott &lt;- bw.scott(destination_ppp_sg.km)\nbw_fixed_scott\n\n  sigma.x   sigma.y \n1.4763217 0.9063352 \n\nkde_fixed_scott &lt;- density(destination_ppp_sg.km, bw_fixed_scott)\nplot(kde_fixed_scott,main = \"Fixed-Bandwidth KDE for Grab Destination Points (Using bw_scott)\")\ncontour(kde_fixed_scott, add=TRUE)\n\n\n\n\nHowever, upon visual inspection, there are signs of a certian degree of over-smoothing when we directly use the bandwidth provide by bw_scott method. Automatic bandwidth selection methods provides a starting point for bandwidth selection, and further fine-tuning might be necessary based on the results of the plot we have created above.\nTo do so, we will use rule of thumb adjustment by diving the bandwidth value by 2, to reduce the bandwidth size, and hence possible over-smoothing effect.\n\npar(mar = c(0,1,1,1))\nkde_fixed_scott &lt;- density(destination_ppp_sg.km, sigma=bw_fixed_scott/2)\nplot(kde_fixed_scott,main = \"Fixed-Bandwidth KDE for Grab Destination Points (Using bw_scott)\")\ncontour(kde_fixed_scott, add=TRUE)\n\n\n\n\nLooking at the plot created, it appears that by reducing the bandwidth (thus making the point cluster buffers smaller), the over-smoothing effects have been minimized. However, we still can observe the Grab destination hotspot areas, which we can investigate further in subsequent sections.\n\n\n7.3.4 Kernel Function Selection for Fixed Bandwidth\n\n\n\n\n\nKernel functions are another consideration in KDE computation because they control how we weight points within the bandwidth radius. The default kernel in density.ppp() is the gaussian. alternatives such as epanechnikov, quartic, and disc are also available.\nIn this section, we will explore and experiment with different kernel functions and plot them together to see how it influences our KDE map results.\n\nkde_fixed_scott.gaussian &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_fixed_scott.epanechnikov &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_fixed_scott.quartic &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_fixed_scott.disc &lt;- density(destination_ppp_sg.km, \n                          sigma=bw_fixed_scott, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(kde_fixed_scott.gaussian, main=\"Gaussian\")\nplot(kde_fixed_scott.epanechnikov, main=\"Epanechnikov\")\nplot(kde_fixed_scott.quartic, main=\"Quartic\")\nplot(kde_fixed_scott.disc, main=\"Disc\")\n\n\n\n\nXie & Yan (2008) suggested that the choice of kernel function has minimal impact on density estimation outcomes. Shen et al. (2020) further identified the search bandwidth as a more influential factor in kernel estimation than the selection of different kernel functions. Empirically looking at the KDE maps we have created above using different kernel functions, similar conclusion can be made. Despite the slight variations in smoothness and spread, all four plots show similar patterns of density estimation. This underscores the argument that the choice of kernel function does not significantly impact KDE results. Consequently, we will not focus this aspect in our analysis.\n\n\n\n7.4 Creating KDE Layers with Spatially Adaptive Bandwidth\nFixed bandwidth kernels are commonly employed in statistical literature due to their ease of implementation. However, their application in spatial datasets often yields suboptimal estimations due to a lack of spatial and temporal adaptability (González & Moraga, 2022). Consequently, the more intuitive ‘adaptive smoothing’ approach has emerged. In this technique, the amount of smoothing is inversely related to the density of the points.\nIn spatstat packages, there are three main approaches (Pebesma & Bivand, 2023) in creating KDE layers with spatially adapative bandwidth.\n\nVoronoi-Dirichlet Adaptive Density Estimate: It computes the intensity function estimate of a point pattern dataset by creating tessellations. On default, the input point pattern data is used to construct a Voronoi/Dirichlet tessellation (Barr and Schoenberg, 2010). The intensity estimate at a given location equals the reciprocal of the size of the Voronoi/Dirichlet cell containing that location.\nAdaptive Kernel Density Estimate: It computes an estimate of the intensity function of a point pattern dataset using the partitioning technique of Davies and Baddeley (2018). It dynamically specifies the smoothing bandwidth to be applied to each of the points. The partitioning method of Davies and Baddeley (2018) accelerates this computation by partitioning the range of bandwidths into n-groups intervals, correspondingly subdividing the point patterns into n-groups, sub-patterns according to bandwidth, and applying fixed-bandwidth smoothing to each sub-pattern.\nNearest-Neighbour Adaptive Density Estimate: It computes an estimate of the intensity function of a point pattern dataset using the distance from each spatial location to the kth nearest points (Cressie, 1991: Silverman, 1986;Burman & Nolan, 1989). The default value of k is the square root of the number of points in the dataset. This estimator of intensity is relatively fast to compute and is spatially adaptive. Some studies suggest the use of the nearest neighbor distance as a suitable parameter for determining the bandwidth (Krisp et al., 2009).\n\n\n7.4.1 Voronoi-Dirichlet Adaptive Density Estimate\nThe Dirichlet-Voronoï estimator is computed in spatstat by the function adaptive.density() with argument method=\"voronoi\".\n\nkde_destination_dirichlet_adaptive &lt;- adaptive.density(destination_ppp_sg.km, f=1, method = \"voronoi\")\n\n\npar(mar = c(0,1,1,1))\nplot(kde_destination_dirichlet_adaptive,main = \"Voronoi-Dirichlet Adaptive Density Estimate\")\n\n\n\n\n\n\n7.4.2 Adaptive Kernel Density Estimate\nThe Adaptive Kernel estimator is computed in spatstat by the function adaptive.density() with argument method=\"kernel\".\n\nkde_destination_kernel_adaptive &lt;- adaptive.density(destination_ppp_sg.km, method = \"kernel\")\n\n\npar(mar = c(0,1,1,1))\nplot(kde_destination_kernel_adaptive,main = \"Adaptive Kernel Density Estimate\")\n\n\n\n\n\n\n7.4.3 Nearest-Neighbour Density Estimate\nThe Nearest-Neighbour estimator is computed in spatstat by the function nndensity().\n\nkde_adaptive_nn &lt;- nndensity(destination_ppp_sg.km, k=10)\n\n\npar(mar = c(0,1,1,1))\nplot(kde_adaptive_nn,main = \"Nearest-Neighbour Adaptive Density Estimate\")\n\n\n\n\n\n\n7.4.4 Choosing Adaptive KDE Method\nSimilar to what we did for fixed bandwidth, we can try to plot histograms to compare the distribution of KDE values obtained from density() function using different adaptive bandwidth selection methods.\n\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_destination_dirichlet_adaptive,main = \"Voronoi-Dirichlet Adaptive\")\nhist(kde_destination_kernel_adaptive,main = \"Adaptive Kernel\")\nhist(kde_adaptive_nn,main = \"Nearest-Neighbour Adaptive\")\n\n\n\n\nFrom the outputs, it seems that there is no significance difference between the distribution of KDE values obtained across different methods. All three methods identified a high concentration of points in a specific area. Hence, we will choose to go with Adapative Kernel method because it provides the most\n\n\n\n7.5 Plotting Interactive KDE Maps\n\nraster_kde_fixed_scott &lt;- raster(kde_fixed_scott)\nraster_kde_adaptive_nn &lt;- raster(kde_adaptive_nn)\nraster_kde_adaptive_kernel &lt;- raster(kde_destination_kernel_adaptive)\n\n\nprojection(raster_kde_fixed_scott) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(raster_kde_adaptive_nn) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(raster_kde_adaptive_kernel) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\n\n\ntmap_mode('view')\nkde_fixed_scott &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_fixed_scott) +\n  tm_raster(\"layer\",\n            n = 10,\n            title = \"KDE_Fixed_scott\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_mode('view')\nkde_adaptive_nn &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_adaptive_nn) +\n  tm_raster(\"layer\",\n            n = 7,\n            title = \"KDE_Adaptive_nn\",\n            style = \"pretty\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_mode('view')\nkde_adaptive_kernel &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_adaptive_kernel) +\n  tm_raster(\"layer\",\n            n = 7,\n            title = \"KDE_Adaptive_Kernel\",\n            style = \"pretty\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1,id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)\n\ntmap_arrange(kde_fixed_scott, kde_adaptive_nn, kde_adaptive_kernel ,ncol=1,nrow=3,sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.6 Analysis of Singapore-Level Kernel Density Estimation Maps\nFromThe highest cluster of Grab taxi drop-off points are seen near Changi Airport, where the concentrations reach up to 450.\nThe central and southern parts of Singapore, particularly the Central Business District (CBD) and Marina Bay areas, also exhibit substantial concentrations of Grab taxi drop-off points, peaking at 350. These areas have a high demand for taxi services, possibly due to a high concentration of businesses, tourist attractions.\nInterestingly enough, we found a few residential subzones outside of the CBD region, which is comparatively higher density values compared to the rest of the island. However, because of the smoothing effect, identifying these hotspots using a fixed-bandwidth KDE map is difficult. Hence, we proceeded to perform a cross inspection with adaptive KDE maps for better precision in hotspots identification.\nThrough adaptive nearest neighbour KDE map, we identified distinct hotspots in Jurong West (7000-8000), Woodlands (6000-7000), Tampines (4000-5000), and Toa Payoh (4000-5000). The Adaptive Kernel KDE map complements this finding by accentuating two additional hotspots, Jurong East (500-1000) and Punggol (500-1000).\n\n\n\nSource: URA Master Plan 2019\n\n\nIt is intriguing to see that the six residential subzones we have identified have higher concentrations of drop-off points, despite being predominantly classified as residential areas , according to the Master Plan 2019 (MP19) by Urban Redevelopment Authority Singapore. It will be interesting to discern underlying spatial process and pull factors that lead to this spatial cluster. Therefore, we will attempt to create comparable KDE maps at the planning subzone level.\n\n\n7.7 Planning Area-Level Kernel Density Estimation\nIn this section, we will create planning-area level KDE maps for six planning areas we have identified. In order to create such maps, we will carry out additional data wrangling as required.\nFirstly, we will filter out different planning areas as separate sf objects from mpsz_sf.\n\nwl = mpsz_sf %&gt;% filter(PLN_AREA_N == \"WOODLANDS\")\nje = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG EAST\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\ntn = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\ntp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TOA PAYOH\")\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\n\n\npar(mar = c(1,1,1,0),mfrow=c(2,3))\nplot(st_geometry(wl), main = \"Woodlands\")\nplot(st_geometry(je), main = \"Jurong East\")\nplot(st_geometry(jw), main = \"Jurong West\")\nplot(st_geometry(tn), main = \"Tampines\")\nplot(st_geometry(tp), main = \"Toa Payoh\")\nplot(st_geometry(pg), main = \"Punggol\")\n\n\n\n\nNext, we will create owin objects to represent the observation windows for respective planning area. Once owin objects are created, we will also filter Grab taxi drop-off points in each observation window from the original destination_ppp_sg ppp object.\n\nwl_owin = as.owin(wl)\nje_owin = as.owin(je)\njw_owin = as.owin(jw)\ntn_owin = as.owin(tn)\ntp_owin = as.owin(tp)\npg_owin = as.owin(pg)\n\ndestination_wl_ppp = destination_ppp_sg[wl_owin]\ndestination_je_ppp = destination_ppp_sg[je_owin]\ndestination_jw_ppp = destination_ppp_sg[jw_owin]\ndestination_tn_ppp = destination_ppp_sg[tn_owin]\ndestination_tp_ppp = destination_ppp_sg[tp_owin]\ndestination_pg_ppp = destination_ppp_sg[pg_owin]\n\nNow that we have prepared both owin and ppp objects for each planning area, we are ready to plot KDE maps. Similar to what we have done in previous section, we will try both fixed-bandwitdh and adaptive bandwidth KDE maps.\n\n7.7.1 Planning Area-Level Fixed-Bandwidth KDE Maps\n\nwl_kde_scott &lt;- density(destination_wl_ppp, sigma=bw.scott, main=\"Woodlands\")\nje_kde_scott &lt;- density(destination_je_ppp, sigma=bw.scott, main=\"Jurong East\")\njw_kde_scott &lt;- density(destination_jw_ppp, sigma=bw.scott, main=\"Jurong West\")\ntn_kde_scott &lt;- density(destination_tn_ppp, sigma=bw.scott, main=\"Tampines\")\ntp_kde_scott &lt;- density(destination_tp_ppp, sigma=bw.scott, main=\"Toa Payoh\")\npg_kde_scott &lt;- density(destination_pg_ppp, sigma=bw.scott, main=\"Punggol\")\npar(mar = c(1,1,1,1.5),mfrow = c(3,2))\n\nplot(wl_kde_scott,main = \"Fixed KDE Woodlands\")\ncontour(wl_kde_scott, add=TRUE)\nplot(je_kde_scott,main = \"Fixed KDE Jurong East\")\ncontour(je_kde_scott, add=TRUE)\nplot(jw_kde_scott,main = \"Fixed KDE Jurong West\")\ncontour(jw_kde_scott, add=TRUE)\nplot(tn_kde_scott,main = \"Fixed KDE Tampines\")\ncontour(tn_kde_scott, add=TRUE)\nplot(tp_kde_scott,main = \"Fixed KDE Toa Payoh\")\ncontour(tp_kde_scott, add=TRUE)\nplot(pg_kde_scott,main = \"Fixed KDE Punggol\")\ncontour(pg_kde_scott, add=TRUE)\n\n\n\n\n\n\n7.7.2 Planning Area-Level Adaptive-Bandwidth KDE Maps\n\nwl_kde_adaptive_kernel &lt;- adaptive.density(destination_wl_ppp, method = \"kernel\")\nje_kde_adaptive_kernel &lt;- adaptive.density(destination_je_ppp, method = \"kernel\")\njw_kde_adaptive_kernel &lt;- adaptive.density(destination_jw_ppp, method = \"kernel\")\ntn_kde_adaptive_kernel &lt;- adaptive.density(destination_tn_ppp, method = \"kernel\")\ntp_kde_adaptive_kernel &lt;- adaptive.density(destination_tp_ppp, method = \"kernel\")\npg_kde_adaptive_kernel &lt;- adaptive.density(destination_pg_ppp, method = \"kernel\")\n\npar(mar = c(1,1,1,1.5),mfrow = c(3,2))\n\nplot(je_kde_adaptive_kernel,main = \"Adaptive KDE Woodlands\")\ncontour(je_kde_adaptive_kernel, add=TRUE)\nplot(je_kde_adaptive_kernel,main = \"Adaptive KDE Jurong East\")\ncontour(je_kde_adaptive_kernel, add=TRUE)\nplot(jw_kde_adaptive_kernel,main = \"Adaptive KDE Jurong West\")\ncontour(jw_kde_adaptive_kernel, add=TRUE)\nplot(tn_kde_adaptive_kernel,main = \"Adaptive KDE Tampines\")\ncontour(tn_kde_adaptive_kernel, add=TRUE)\nplot(tp_kde_adaptive_kernel,main = \"Adaptive KDE Toa Payoh\")\ncontour(tp_kde_adaptive_kernel, add=TRUE)\nplot(pg_kde_adaptive_kernel,main = \"Adaptive KDE Punggol\")\ncontour(pg_kde_adaptive_kernel, add=TRUE)\n\n\n\n\n\n\n\n7.8 Analysis of Planning Area-Level Kernel Density Estimation Maps\nObservations from the planning area-level KDE maps reveal that while KDE maps excel in visualizing and analyzing spatial data, their application to micro-level analysis, like planning subzones, has limitations. Firstly, there are issues with over-smoothing (in fixed KDE maps) and undersmoothing (in adaptative KDE maps) which deters us from discerning meaningful insights. Furthermore, KDE values, generated on grid-pixels using Euclidean distance, result in grid blocks that limit our ability to discern fine details and variations within each subzone.\nFor example, let’s look at the picture below, which is a snapshot from KDE adaptive nearest-neighbor map. A hotspot is identified and represented by a concentrated red zone near Braddell Road in Toa Payoh. However, just by looking at this map, it’s really hard to understand more about this hotspot. We know there is a cluster of drop-off points, but why are they there? What are pull factors attracting people to this area? It’s almost impossible to answer these questions just by looking at this KDE map.\n\n\n\n\n\nThis limitation underscores the need for more advanced visualization techniques or analytical methods that can provide alternative approaches to understanding spatial point clusters. With this limitation in mind, we will apply a new analytical method called network constrained kernel density estimation (NKDE) to offer more detailed insights into the spatial distribution of Grab taxi drop-off points through the integration of road networks."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "8.0 Network Constrained Kernel Density Estimation (NKDE)",
    "text": "8.0 Network Constrained Kernel Density Estimation (NKDE)\nIn the real world, point events are often not randomly distributed. Instead, their distribution is constrained by networks. When carrying out spatial point pattern analysis, traditional Kernel Density Estimation (KDE) assumes an infinite, homogeneous, two-dimensional space, an approximation that is not accurate for network-based study areas. In such networks, movement is constrained by multiple one-dimensional lines (Gelb, 2021). Network Constrained Kernel Density Estimation (NKDE), therefore, is a widely used approach to identify the hotspots and evaluate origin-destination points along with the road network (Shen et al. 2020).\nThis approach estimates the intensity of the spatial process solely on the network. The network edges are divided into lixels (one-dimensional pixels), and the centers of the lixels serve as the locations for intensity estimation. Distances between events and sampling points are calculated as the shortest path distances on the network, instead of Euclidean distances. This adjustment slightly modifies the intensity function from the classical KDE function. The adapted formula makes the interpretation straightforward: it “estimates the density over a linear unit” rather than an area unit.\nIn this section, we will follow-up on the six planning areas we identified in Section 7.6 and attempt to create NKDE maps using spNetwork package.\n\n8.1 Extracting Road Networks for Focus Planning Areas\nBefore we carry out the anlysis, we will extract relevant datasets required for calculating NKDE values in each planning area:\n\nroad network\ndestination points\n\n\nwl_network = st_intersection(sg_driving_sf,st_union(wl))\nje_network = st_intersection(sg_driving_sf,st_union(je))\njw_network = st_intersection(sg_driving_sf,st_union(jw))\ntn_network = st_intersection(sg_driving_sf,st_union(tn))\ntp_network = st_intersection(sg_driving_sf,st_union(tp))\npg_network = st_intersection(sg_driving_sf,st_union(pg))\n\n\nwl_destination = st_intersection(destination_sf,st_union(wl))\nje_destination = st_intersection(destination_sf,st_union(je))\njw_destination = st_intersection(destination_sf,st_union(jw))\ntn_destination = st_intersection(destination_sf,st_union(tn))\ntp_destination = st_intersection(destination_sf,st_union(tp))\npg_destination = st_intersection(destination_sf,st_union(pg))\n\n\n\n8.3 Data Preparation\n\nFor illustrative purpose, I’ll use “Punggol” as a example to demonstrate step-by-step data preparation process. Upon completion of this demonstration, maps for the remaining planning areas will be created.\n\nThe spNetwork package contains nkde function that is specifically designed to implement Network Constrained Kernel Density Estimation (NetKDE).\nThe three main inputs are:\n\nlixels: a “SpatialLinesDataFrame”, representing the lines of the network.\nevents: a “SpatialPointsDataframe”, representing the realizations of the spatial process.\nsamples: a “SpatialPointsDataframe” providing the locations where the density must be estimated.\n\n\n8.3.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork.\n\npg_lixels &lt;- lixelize_lines(pg_network, \n                         700, \n                         mindist = 350)\n\nIn this code snippet, the length of a lixel is set to 700m, and the minimum length of a lixel, denoted as mindist, is set to 350m. This implies that during segmentation, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. The segments that are already shorter than the minimum distance are not modified.\n\n\n8.3.2 Generating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points. The points are located at center of the line based on the length of the line.\n\npg_samples &lt;- lines_center(pg_lixels)\n\n\n\n\n8.4 Performing NKDE\n\n8.4.1 Computing Network Constrained Kernel Density Estimation\nThe spNetwork package offers three methods for calculating NKDE values simple, discontinuous and continuous.\n\nSimple NKDE: Simple NKDE method was proposed by Xie and Yan (2008), extending the planar KDE to a network-based model. However, there are issues with this method due to statistical incorrectness. Specifically, at network intersections, the event’s mass is multiplied in each direction, leading to overestimation and inflated density estimates, which can result in misleading interpretations (Gelb, 2021). To address these issues, Okabe et al. (2009) proposed two unbiased estimators: Discontinuous NKDE and Continuous NKDE.\nDiscontinuous NKDE: Discontinuous NKDE aims to resolve density inflation by dividing the mass at intersections according to the number of directions minus one, resulting in an unbiased estimator (Gelb, 2021). However, the discontinuous nature of this method can be counter-intuitive in practical applications.\nContinuous NKDE: Continuous NKDE proposes to address the limitations of both simple and discontinuous NKDE. It adjusts the values of the NKDE at intersections and applies a backward correction to force the density values to be continuous (Gelb, 2021).\n\nIn this analysis, we will compute NKDE values using all three methods and compare the results.\n\npg_nkde_simple &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\npg_nkde_discontinuous &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"discontinuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\npg_nkde_continuous &lt;- nkde(pg_network, \n                  events = pg_destination,\n                  w = rep(1,nrow(pg_destination)),\n                  samples = pg_samples,\n                  kernel_name = \"quartic\",\n                  bw = 200, \n                  div= \"bw\", \n                  method = \"continuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n\n\n\nReflection\n\n\n\nbw argument refers to the bandwidth used for calculating KDE values. Xie and Yan (2008) suggested that narrow bandwidths (between 20 and 250 m) are more appropriate for identifying local effects at smaller scales. Hence, we use 200 here.\nagg argument allows the events to be aggregated, and their weights to be added within a threshold distance. Aggregating events can simplify networks and limit the number of iterations when calculating the NKDE, hence effectively reducing time complexity of computation.\n\n\n\n\n8.4.2 Visualising NKDE Maps Using Different Methods\nBefore we can visualise the NetKDE values, we will insert the computed density values into samples and lixels objects. To enhance the readability of the results, you will first multiply the obtained densities by the total number of drop-off points. This adjustment ensures that the spatial integral equals the number of events. Subsequently, you will multiply this value by 1000 to derive the estimated number of drop-off points per kilometer. This process will provide a more intuitive understanding of the density distribution along the network.\n\npg_samples$nkde_simple &lt;- pg_nkde_simple*nrow(pg_destination)*1000\npg_lixels$nkde_simple &lt;- pg_nkde_simple*nrow(pg_destination)*1000\n\npg_samples$nkde_discontinuous &lt;- pg_nkde_discontinuous*nrow(pg_destination)*1000\npg_lixels$nkde_discontinuous &lt;- pg_nkde_discontinuous*nrow(pg_destination)*1000\n\npg_samples$nkde_continuous &lt;- pg_nkde_continuous*nrow(pg_destination)*1000\npg_lixels$nkde_continuous &lt;- pg_nkde_continuous*nrow(pg_destination)*1000\n\nNow we can plot NKDE maps using tmap.\n\ntmap_mode('view')\npg_nkde_simple_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_simple\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\npg_nkde_discontinuous_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_discontinuous\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\npg_nkde_continuous_map &lt;- tm_basemap(server = \"Esri.WorldTopoMap\") +\n  tm_basemap(server = \"Esri.WorldGrayCanvas\")+\n  tm_basemap(server = \"OpenStreetMap\") +\ntm_shape(pg_lixels)+\n  tm_lines(col=\"nkde_continuous\", lwd = 2, palette =c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\"), id=\"nkde_continuous\")+\ntm_shape(pg_destination)+\n  tm_dots(size=0.01)\n\n\ntmap_arrange(pg_nkde_simple_map, pg_nkde_discontinuous_map, pg_nkde_continuous_map ,ncol=3,nrow=1,sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\nComparing Simple, Discontinuous and Continuous NKDE Maps\n\n\n\n\n\nUpon close examination to the intersection as shown above, we can clearly see the density inflation caused by simple NKDE method, as opposed to discontinuous and continuous methods. A comparison between the Discontinuous and Continuous NKDE maps reveals the disjointed nature of lixels resulting from the Discontinuous method, as indicated by the varying color bands representing each lixel segment. Out of all three maps, continuous NKDE map offers the most intuitive representation.\n\n\n\n\n\n8.5 Plotting NKDE for Different Planning Areas\nIn this section, we will create interactive maps of different planning areas that we have previousely identified as potential hotspots. These maps will incorporate several key elements: network kernel density estimation values (depicted through lixels), Grab drop-off points, and points of interest (POIs). The purpose of including these elements is to identify the pull factors within these areas. Understanding these factors can help in planning and decision-making processes, such as where to locate new facilities or how to improve transportation routes.\n\nPunggolTampinesJurong EastJurong WestWoodlandsToa Payoh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.6 Analysis of Network Constrained Kernal Density Estimation (NKDE) Maps\nOverall, It is intriguing to observe the interplay of similar, yet distinct, pull factors across different planning subzones.\n\nPunggol: In Punggol, all observed clusters are located near schools and parks. The most significant clusters are found in the vicinity of Edgefield Secondary School, Edgefield Primary School, and Punggol Green Primary School. Multiple smaller clusters are observed around neighbourhood parks.\nTampines: In Tampines, a significant cluster is observed at Tanah Merah Country Club and Laguna Country Club, suggesting a high demand for taxi services in these recreational areas. A smaller cluster is also noticeable near Simei Neighbourhood Park.\nJurong East: In Jurong East, a significant cluster is observed leading to IMM, a major outlet shopping center, which seems to attract multiple taxi trajectories. Additional clusters are observed near Yuhua Primary School and Crest Secondary School, as well as various neighbourhood parks.\nJurong West: In Jurong West, two significant clusters are observed, one near Westwood Secondary School and the other leading to Nanyang Technological University, highlighting the importance of educational institutions in this area. Another equally significant cluster is located near Masjid Maarof, one of Singapore’s oldest and most prominent mosques.\nWoodlands: In Woodlands, a significant cluster is observed at Innova Junior College and Singapore Sports School. Additional clusters are found at Woodland Civic Centre, a community centre with library and dining areas, and STELLAR@TE2, a lifestyle shopping mall. We also observed another cluster leads to Singapore Turf Club.\nToa Payoh: In Toa Payoh, a cluster is observed near Toa Payoh Townpark, indicating its popularity as a communal destination. Another cluster is found at St Andrew’s Junior College.\n\nIn conclusion, the results of NKDE analysis of Grab taxi drop-off points in six selected planning areas reveals a nuanced pattern of clusters associated with various types of amenities and institutions. These findings suggest that different planning areas have distinct characteristics that attract people to specific locations within them. Additionally, understanding these pull factors can help inform urban planning and development strategies to create anew or further enhance the attractiveness of these areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "9.0 Temporal Network Kernel Density Estimation (TNKDE)",
    "text": "9.0 Temporal Network Kernel Density Estimation (TNKDE)\nTemporal Network Kernel Density Estimation (TNKDE) is an extension of Network Constrained Kernel Density Estimation (NKDE) whereby the the temporal dimension is integrated to calculating density of events on a network (Gelb & Apparicio, 2023). This means that density estimation can be done along lines of the network and at different times. The spatio-temporal kernel is calculated as the product of the network kernel density and the time kernel density.\nTNKDE can be calculated in calculated in R environment using tnkde() function from spNetwork package.\n\n9.1 Visualising Frequency Distribution\nBefore we calculate TNKDE, we will conduct an exploratory analysis of our data from the Grab Posisi dataset. This will involve visualizing the frequency distribution of both our origin and destination point samples. This step will provide us with a preliminary understanding of our data distribution, which is crucial for effective TNKDE calculation.\nFirstly, we will analyze the frequency distribution of days from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories originating and ending on each day of the week\n\norigin_day &lt;- ggplot(data=origin_sf, \n              aes(x=weekday)) + \n              geom_bar()\n\ndestination_day &lt;-  ggplot(data=destination_sf, \n                    aes(x=weekday)) + \n                    geom_bar()\n\norigin_day + destination_day\n\n\n\n\nAs seen above, each bar graph displays a uniform distribution across all weekdays. The dataset is sampled; hence all days are equally represented in both origin and destination datasets.\n\n\n9.2 Plotting Frequency of Trip Origination By Hour\nNext, we will analyze the frequency distribution of trip origination by hour from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories originating points in each hour.\n\norigin_sf$pickup_hr_num &lt;- as.numeric(origin_sf$pickup_hr)\npickup_hr_num &lt;- origin_sf$pickup_hr_num\n\nggplot(data=origin_sf, \n                    aes(x=pickup_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = pickup_hr_num)\n\n\n\n\nWe observed noticeable peaks at 3, 5, 14, and 15 hours (3 AM, 5 AM, 2 PM and 3 PM respectively). There’s a significant drop in trip originations during 9 hour (9 AM), as well as from 19 to 22 hours (7 PM to 10 PM). These could be quieter periods in the day when fewer trips are originated. It is also interesting to see a sudden increase of trip origination at 23 hour (11 PM).\n\n\n9.3 Plotting Frequency of Trip Destination By Hour\nNext, we will analyze the frequency distribution of trip destination by hour from our trajectory data samples. To achieve this, we will plot bar graphs representing the count of trajectories destination points in each hour.\n\ndestination_sf$dropoff_hr_num &lt;- as.numeric(destination_sf$dropoff_hr)\ndropoff_hr_num &lt;- destination_sf$dropoff_hr_num\n\nggplot(data=destination_sf, \n                    aes(x=dropoff_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = dropoff_hr_num)\n\n\n\n\nSomewhat similar to origination points, we observed noticeable peaks at 3, 13, and 14 hours (3 AM, 1 PM, and 2 PM respectively). There’s a significant drop in trip desintation during 9, 18, 21 and 23 hours (9 AM, 6 PM, 9 PM and 11 PM respectively).\n\n\n\n\n\n\nReflection\n\n\n\nWhile the frequency plots of trip origination and destination points based on temporal segments reveal intriguing patterns, deriving meaningful insights from these can be challenging. This is primarily due to the nature of the dataset. As a sample dataset, the data points are randomly collected within a specific time window. Consequently, these data may not accurately represent the true temporal variations in traffic flows. Therefore, while the plots provide a snapshot of the data, they may not fully capture the complexity and variability of real-world traffic patterns.\n\n\n\n\n9.4 Time-Bandwidth Selection\nWe can now calculate the kernel density values of destination points in time for several bandwidths. When calculating TNKDE, two bandwidths are necessary, one for space and one for time. In this section, we will try to explore three different bandwidth selection methods:\n\nbw_bcv: implements biased cross-validation for bandwidth selection in kernel density estimation. The goal of cross-validation in this context is to find the bandwidth that minimizes the estimation error.\nbw_ucv: implements cross-validation for bandwidth selection, but in an unbiased manner.\nbw_SJ: implements the methods of Sheather & Jones (1991) to select the bandwidth using pilot estimation of derivatives.\n\nTo achieve this, we will follow the steps as below:\nWe will first create a vector w of length equal to the number of rows in the destination_sf data frame. Each element of the vector is set to 1. This vector will be used as weights in calculating TKDE.\n\nw &lt;- rep(1,nrow(destination_sf))\n\nWe will then create a sequence of numbers from 0 to the maximum value of the dropoff_hr_num column in the destination_sf data frame, with a step size of 0.5. These are the sample points where the density will be estimated.\n\nsamples &lt;- seq(0, max(destination_sf$dropoff_hr_num), 0.5)\n\nNext, we will calculate bandwidth values for dropoff_hr_num column of destination_sf using 3 bandwidth selection methods discussed above.\n\nbw_bcv &lt;- bw.bcv(destination_sf$dropoff_hr_num)\nbw_ucv &lt;- bw.ucv(destination_sf$dropoff_hr_num)\nbw_SJ &lt;- bw.SJ(destination_sf$dropoff_hr_num)\n\nOnce bandwidth values are calculated, we will proceed to implement TNKDE analysis using tkde() function from spPackage. Then, we will create a dataframe time_kernel_values to store these density values.\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_bcv, kernel_name = \"quartic\"),\n  bw_ucv = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_ucv, kernel_name = \"quartic\"),\n  bw_SJ = tkde(destination_sf$dropoff_hr_num, w = w, samples = samples, bw = bw_SJ, kernel_name = \"quartic\"),\n  time = samples\n)\n\nNext, we will use melt function from the reshape2 package to transform time_kernel_values dataframe into a new dataframe called df_time, using time as the identifier variable. We will also convert the variable column of the df_time data frame to a factor, for plotting in ggplot2.\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nFinally, we will use ggplot2 to create a line plot of the TKDE for each bandwidth.\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = dropoff_hr_num) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat does each line plot say about each bandwidth selection approach?\nIn all three plots, the line represents the estimated density of drop-offs at each time point. At a glance, bw_ucv and bw_SJ methods seem to give a more detailed and potentially noisier pattern than bw_bcv. Unlike the other two, bw_bcv approach shows a relatively smooth curve with minor fluctuations. The differences in these plots highlight how the choice of bandwidth can significantly affect the resulting density estimate.\n\n\n\n\n9.5 TNKDE Implementing at Punggol Planning Area\n\n9.5.1 Data Wrangling and Exploratory Analysis\nNow, we will try to construct a TNKDE map for Punggol Planning Area. Before running TNKDE, we will carry our some preliminary data wrangling and exploratory analysis to ensure our dataset is in the appropriate format.\n\npg_dropoff_hr_num &lt;- pg_destination$dropoff_hr_num\n\nggplot(data=pg_destination, \n                    aes(x=pg_dropoff_hr_num), bins = 24) + \n                    geom_bar() +\n    scale_x_continuous(breaks = pg_dropoff_hr_num)\n\n\n\n\nFrom the graph above, we observed noticeable peaks at 1, 6, 8, 13, and 20 hours (1 AM, 6 AM, 8 AM, 1 PM, and 8 PM respectively). These peaks suggest that these time intervals might be particularly active or important. To investigate further, we will filter out the destination points that fall within these specific hour intervals and use tmap to visualize these filtered data points.\n\npg_dropoff_1 &lt;- filter(pg_destination, dropoff_hr_num == '1')\npg_dropoff_6 &lt;- filter(pg_destination, dropoff_hr_num == '6')\npg_dropoff_8 &lt;- filter(pg_destination, dropoff_hr_num == '8')\npg_dropoff_13 &lt;- filter(pg_destination, dropoff_hr_num == '13')\npg_dropoff_20 &lt;- filter(pg_destination, dropoff_hr_num == '20')\n\n\ntmap_mode(\"view\")\ntm_basemap(server = \"Esri.WorldTopoMap\") +\ntm_shape(pg_lixels)+\n  tm_lines()+\ntm_shape(pg_dropoff_1)+\n  tm_dots(size=0.01) +\n  tm_shape(pg_dropoff_6)+\n  tm_dots(size=0.01) +\n    tm_shape(pg_dropoff_8)+\n  tm_dots(size=0.01) +\n    tm_shape(pg_dropoff_13)+\n  tm_dots(size=0.01) +  \n  tm_shape(pg_dropoff_20)+\n  tm_dots(size=0.01)\n\n\n\n\n\n\n\n\n9.5.2 Bandwidth Selection\nOnce, we have applied visualization techniques to briefly analyse the datasets, we will proceed with implementing TKNDE for Punggol planning area. For the steps, we will repeat the procedures in Section 9.4 for this implementation.\n\nw_pg &lt;- rep(1,nrow(pg_destination))\nsamples_pg &lt;- seq(0, max(pg_destination$dropoff_hr_num), 0.5)\n\nbw_bcv &lt;- bw.bcv(pg_destination$dropoff_hr_num)\nbw_ucv &lt;- bw.ucv(pg_destination$dropoff_hr_num)\nbw_SJ &lt;- bw.SJ(pg_destination$dropoff_hr_num)\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_bcv, kernel_name = \"quartic\"),\n  bw_ucv = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_ucv, kernel_name = \"quartic\"),\n  bw_SJ = tkde(pg_destination$dropoff_hr_num, w = w_pg, samples = samples_pg, bw = bw_SJ, kernel_name = \"quartic\"),\n  time = samples_pg\n)\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = dropoff_hr_num) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\n\n\nAs we notice that the bandwidths plotted above only accounts for temporal aspect. However, both spatial and temporal dimensions are required for implementation of TNKDE. To achieve this, the network and temporal bandwidths can be selected with the leave-one-out-cross validation method (Gelb & Apparicio, 2023). To do so, we can use bws_tnkde_cv_likelihood_calc() function to select the most appropriate bandwidths for the network and time dimensions. It computes the cross-validation likelihood for various bandwidths, enabling a data-driven selection of the most suitable bandwidths for both dimensions.\n\ncv_scores &lt;- bws_tnkde_cv_likelihood_calc(\n  bw_net_range = c(100,1000),\n  bw_net_step = 100,\n  bw_time_range = c(3,24),\n  bw_time_step = 3,\n  lines = pg_network,\n  events = pg_destination,\n  time_field = \"dropoff_hr_num\",\n  w = rep(1, nrow(pg_destination)),\n  kernel_name = \"quartic\",\n  method = \"discontinuous\", \n  diggle_correction = FALSE,\n  study_area = NULL,\n  max_depth = 10,\n  digits = 2,\n  tol = 0.1,\n  agg = 15,\n  sparse=TRUE,\n  grid_shape=c(1,1),\n  sub_sample=1,\n  verbose = FALSE,\n  check = TRUE)\n\n\n\n\n\n\n\nReflection\n\n\n\nThis code chunk was referenced from Gelb’s implementation of TNKDE (2023) as provided in this vignette. Below is a breakdown of some arguments used in this code chunk.\n\nbw_net_range = c(100,1000), bw_net_step = 100: These parameters define the range and step size for the network bandwidths that will be evaluated. The function will calculate the cross-validation likelihood for network bandwidths from 100 to 1000, in steps of 100.\nbw_time_range = c(3,24), bw_time_step = 3: Similarly, these parameters define the range and step size for the time bandwidths that will be evaluated. The function will calculate the cross-validation likelihood for time bandwidths from 3 to 24, in steps of 3.\nkernel_name = \"quartic\": This parameter specifies the kernel function to use for the calculation. In this case, the quartic kernel is used.\nmethod = \"discontinuous\": This parameter specifies the method to use when calculating the TNKDE. The discontinuous method is used similar to what we did in NKDE.\ndiggle_correction = FALSE: This parameter specifies whether to use the correction factor for edge effect. In this case, since we are only looking at the neighborhood level, edge correction is not necessary and the correction factor is not used.\nagg = 15: This parameter specifies a function to aggregate the events when they are too close.\n\n\n\nWe can see the outputs of bws_tnkde_cv_likelihood_calc() function in a table format using knitr package.\n\nknitr::kable(cv_scores)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n6\n9\n12\n15\n18\n21\n24\n\n\n\n\n100\n-250.91794\n-182.71584\n-155.27632\n-116.78531\n-113.26649\n-102.39516\n-93.35400\n-89.81173\n\n\n200\n-145.76563\n-90.84170\n-69.02605\n-61.88347\n-56.56890\n-49.40677\n-49.57432\n-49.73512\n\n\n300\n-100.00061\n-65.40228\n-47.32325\n-43.88449\n-40.43182\n-36.94576\n-37.12520\n-37.29583\n\n\n400\n-68.95819\n-41.82902\n-32.92989\n-31.35903\n-29.73603\n-29.93138\n-30.11827\n-30.29386\n\n\n500\n-54.37326\n-34.65375\n-29.47804\n-29.75000\n-28.14349\n-28.34934\n-28.54072\n-28.71939\n\n\n600\n-47.21774\n-34.83465\n-29.69234\n-29.97549\n-28.37665\n-28.58649\n-28.77993\n-28.96001\n\n\n700\n-36.42261\n-29.55281\n-28.07569\n-28.36425\n-28.59379\n-28.80594\n-29.00069\n-29.18167\n\n\n800\n-32.88500\n-27.88168\n-28.25843\n-28.55089\n-28.79093\n-29.00503\n-29.20083\n-29.38249\n\n\n900\n-31.20502\n-28.04346\n-28.43185\n-28.72756\n-28.97115\n-29.18680\n-29.38338\n-29.56554\n\n\n1000\n-31.33948\n-28.19916\n-28.59400\n-28.89218\n-29.13776\n-29.35455\n-29.55170\n-29.73421\n\n\n\n\n\nAccording to the “leave one out cross validation” method, the optimal set of bandwidths is 800 metres and 6 hrs (the combination that gives the least cv score). As expected, larger bandwidths are required because the density of the events are spread both in space and time.\n\n\n9.5.3 TNKDE Calculation\nNow that we have selected an appropriate bandwidth for both spatial and temporal dimensions, we will proceed to implement NKDE calculation. Our first step involves selecting a sample time-step for the calculation. Given that our analysis is based on a 24-hour window, we adopted a 3-hour time-step as a simple rule-of-thumb. Then, we proceed to calculate TNKDE densities.\n\npg_sample_time &lt;- seq(0, max(pg_destination$dropoff_hr_num), 3)\n\npg_tnkde_densities &lt;- tnkde(lines = pg_network,\n                   events = pg_destination,\n                   time_field = \"dropoff_hr_num\",\n                   w = rep(1, nrow(pg_destination)), \n                   samples_loc = pg_samples,\n                   samples_time = pg_sample_time, \n                   kernel_name = \"quartic\",\n                   bw_net = 800, bw_time = 6,\n                   adaptive = TRUE,\n                   trim_bw_net = 900,\n                   trim_bw_time = 8,\n                   method = \"discontinuous\",\n                   div = \"bw\", max_depth = 10,\n                   digits = 2, tol = 0.01,\n                   agg = 15, grid_shape = c(1,1), \n                   verbose  = FALSE)\n\n\n\n\n\n\n\nReflection\n\n\n\nThis code chunk was referenced from Gelb’s implementation of TNKDE (2023) as provided in this vignette. Below is a breakdown of some arguments used in this code chunk.\n\nbw_net = 800, bw_time = 6: These parameters specify the bandwidths for the network and time dimensions respectively. We use 800 and 6 respectively based on the results of “leave one out cross validation” we carried out in previous section.\ntrim_bw_net = 900, trim_bw_time = 8: These parameters specify the maximum value for the adaptive bandwidth in the network and time dimensions respectively. Here, we use simple rule-of-thumb values.\n\n\n\n\n\n9.5.4 Creating Animated TNKDE Map\nUpon completion of TNKDE densities calculation, we will visulise the results in form of animated GIF.\nFirstly, we will use classInt package to create color breaks for densities values. To determine the class intervals, k-means algorithm will be used.\n\nall_densities &lt;- c(pg_tnkde_densities$k)\ncolor_breaks &lt;- classIntervals(all_densities, n = 10, style = \"kmeans\")\n\nNext, we will generate respective maps for each sample time and compile them all under pg_tnkde_densities using lapply() function. Each map visualizes TNKDE densities for a specific time point. The densities are represented by dots on the map, with the color of the dots indicating the density value. We will use the color breaks and palette specified earlier.\n\nall_maps &lt;- lapply(1:ncol(pg_tnkde_densities$k), function(i){\n  time &lt;- pg_sample_time[[i]]\n  \n  pg_samples$tnkde_density &lt;- pg_tnkde_densities$k[,i]\n  map1 &lt;- tm_shape(pg_samples) + \n  tm_dots(col = \"tnkde_density\", size = 0.01,\n          breaks = color_breaks$brks, palette = viridis(10)) + \n    tm_layout(legend.show=FALSE, main.title = as.character(time), main.title.size = 0.5)\n  return(map1)\n})\n\nGelb (2023) suggested the use of animated maps (GIF or video) to analyse the results of a TNKDE. hence, we will create an animated GIF from the series of maps stored in all_maps and then display the GIF. To achieve this, we will use tmap_animation() function from the tmap package. Once the GIF image is created, we will use include_graphics() function from the knitr package to display the created GIF.\n\ntmap_animation(all_maps, filename = \"images/animated_pg.gif\", \n               width = 1000, height = 1000, dpi = 300, delay = 50)\n\n\nknitr::include_graphics(\"images/animated_pg.gif\")\n\n\n\n\nThe TNKDE map for Punggol provides much more nuanced insights into the spatial clustering patterns across various time intervals. This enables a better understanding of how events are distributed and concentrated within the network over different periods. This can be useful for identification of hotspots and trends both spatially and temporally. Overall, the TNKDE approach offers a comprehensive analysis of spatial and temporal clustering within a network, providing a deeper understanding of the dynamics at play."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01.html#conclusion",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "10.0 Conclusion",
    "text": "10.0 Conclusion\nThis analysis has explored different technqiues of spatial point pattern analysis using Grab’s trajectory datasets. We have carried out three different approaches - Traditional Kernel Density Estimation (KDE), Network Constrained Kernel Density Estimation (NKDE) and Temporal Network Kernel Density Estimation (TNKDE). The findings from these approaches provide valuable insights into the spatial and temporal distribution of Grab’s trajectory datasets for Singapore (and particularly the distribution of destination points).\nIn this study, we have experimented different bandwidth selection methods and kernel function approaches and compare the results of different combinations. We then developed 3 KDE maps using three distinct approaches: Scott’s rule of thumb for fixed bandwidth, adaptive nearest neighbor, and adaptive kernel density. These KDE maps identified six planning subzones as potential hotspots for spatial clustering of Grab destination points.\nWe have also carried out comparative study between KDE and NKDE, outlining the limitations of pixel-based traditional KDE against network-based NKDE. We then created NKDE maps for selected six planning areas: Woodlands, Jurong East, Jurong West, Punggol, Tampines, and Toa Payoh. By utilizing network-based NKDE, we were able to overcome these limitations and provide more accurate and insights into the spatial distribution of various factors within the selected planning areas. The results from NKDE showed significant improvements in precision and interpretability compared to traditional KDE maps.\nFinally, the implementation of TNKDE with a case study in Punggol further enhanced our understanding by incorporating temporal information, allowing us to identify how spatial clusters change over different time intervals.\nFurther research could focus on expanding the application of TNKDE to other planning areas in Singapore. Additionally, exploring the potential of incorporating other data sources, such as demographic or socioeconomic data, could provide a more comprehensive understanding of the spatial distribution of Grab taxi origin and destination points and their temporal dynamics. This could help in identifying patterns and trends related to specific demographics or socioeconomic factors that may influence human mobility patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01.html#references",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "References",
    "text": "References\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial Point Patterns: Methodology and Applications with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b19708.\nBarr, C., & Schoenberg, F.P. (2010). On the Voronoi estimator for the intensity of an inhomogeneous planar Poisson process. Biometrika, 97(4), 977–984.\nBoots, B.N., & Getis, A. (1988). Point Pattern Analysis. Reprint. Edited by G.I. Thrall. WVU Research Repository.\nCronie, O., & van Lieshout, M.N. (2018). A non-model-based approach to bandwidth selection for kernel estimators of spatial intensity functions. Biometrika, 105, 455–462.\nDavies, T.M., & Baddeley, A. (2018). Fast computation of spatially adaptive kernel estimates. Statistics and Computing, 28(4), 937-956.\nDiggle, P.J. (1985). A kernel method for smoothing point process data. Applied Statistics (Journal of the Royal Statistical Society, Series C), 34, 138–147.\nFloch, J.-M., Marcon, E., & Puech, F. (n.d.). Spatial distribution of points. In M.-P. de Bellefon (Ed.), Handbook of Spatial Analysis : Theory and Application with R (pp. 72–111). Insee-Eurostat.\nGelb, J., Apparicio, P.: Temporal Network kernel density estimation. Geographical Analysis. 56, 62–78 (2023).\nGimond (2023). Chapter 11 Point Pattern Analysis. Retrieved from https://mgimond.github.io/Spatial/index.html.\nGonzález, J. A., & Moraga, P. (2022). An adaptive kernel estimator for the intensity function of spatio-temporal point processes.\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app.\nKrisp, J.M., Peters, S., Murphy, C.E., & Fan, H. (2009). Visual bandwidth selection for kernel density maps. Photogrammetrie - Fernerkundung - Geoinformation, 2009, 445–454.\nLoader, C. (1999). Local Regression and Likelihood. Springer, New York.\nO’Sullivan, D., & Wong, D.W. (2007). A surface‐based approach to measuring spatial segregation. Geographical Analysis, 39, 147–168.\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\nRey, S.J., Arribas-Bel, D., & Wolf, L.J. (2023). Point Pattern Analysis. In: Geographic Data Science with python. CRC Press.\nScott, D.W. (1992). Multivariate Density Estimation. Theory, Practice and Visualization. New York: Wiley.\nShen, B., Xu, X., Li, J., Plaza, A., & Huang, Q. (2020). Unfolding spatial-temporal patterns of taxi trip based on an improved network kernel density estimation. ISPRS International Journal of Geo-Information, 9, 683.\nWilkin, J. (2020). Geocomputation 2020-2021 Work Book. University College London. Retrieved from https://jo-wilkin.github.io/GEOG0030/coursebook/analysing-spatial-patterns-iii-point-pattern-analysis.html.\nWolff, M., & Asche, H. (2009). Towards geovisual analysis of crime scenes – a 3D crime mapping approach. Advances in GIScience, 429–448.\nXie, Z., & Yan, J. (2008). Kernel density estimation of traffic accidents in a network space. Computers, Environment and Urban Systems, 32, 396–406.\nYuan, Y., Qiang, Y., Bin Asad, K., & Chow, T. E. (2020). Point Pattern Analysis. In J.P. Wilson (Ed.), The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition). DOI: 10.22224/gistbok/2020.1.13."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHands-On Exercise 01\n\n\n\n\n\nGeospatial Data Wrangling with R!\n\n\n\n\n\n\nJan 7, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 02\n\n\n\n\n\nThematic Mapping and GeoVisualisation with R\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 03\n\n\n\n\n\nSpatial Point Pattern Analysis\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 04\n\n\n\n\n\nSpatial Weights and Applications\n\n\n\n\n\n\nJan 26, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 05\n\n\n\n\n\nGlobal and Local Measures of Spatial Autocorrelation\n\n\n\n\n\n\nFeb 1, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 07\n\n\n\n\n\nGeographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n\n\n\n\nMar 1, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 08\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nJan 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 09\n\n\n\n\n\nGeographically Weighted Predictive Models\n\n\n\n\n\n\nMar 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Exercise 10\n\n\n\n\n\nModelling Geographical Accessibility\n\n\n\n\n\n\nMar 22, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 03\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 03 NKDE\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 04\n\n\n\n\n\nLocal Spatial Autocorrelation Statistics (LISA)\n\n\n\n\n\n\nJan 29, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 05\n\n\n\n\n\nLocal Spatial Autocorrelation Statistics using sfdep\n\n\n\n\n\n\nFeb 5, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 06\n\n\n\n\n\nGeographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n\n\n\n\nMar 1, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 07\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nMar 11, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 09\n\n\n\n\n\nGeographically Weighted Predictive Models\n\n\n\n\n\n\nMar 18, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Econometric Interaction Modelling\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore\n\n\n\n\n\nApplication of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore, implemented using spatstat package (Baddeley, Turner, and Rubak 2022) and spNetwork package (Gelb & Apparicio, 2023) in R environment\n\n\n\n\n\n\nJan 15, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan\n\n\n\n\n\nThis study aims to use spatio-temporal analysis techniques to analyze if the distri-bution of dengue fever outbreak in Tainan City are independent from space and time. The study also attempts to identify clusters and outliers, as well as emerging hot spots/cold spots within the study area. The initial phase of the study utilizes Global and Local Spatial Autocorrelation modelling to discern the spatio-temporal pattern of the disease outbreak. Subsequently, Emerging Hot Spots Analysis (EHSA) is applied to identify spatio-temporal clusters.\n\n\n\n\n\n\nFeb 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application\n\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html",
    "href": "Hands-on_Ex/hands_on08.html",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#overview",
    "href": "Hands-on_Ex/hands_on08.html#overview",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "href": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "title": "Hands-On Exercise 08",
    "section": "2.0 Importing Datasets and Packages",
    "text": "2.0 Importing Datasets and Packages\nFirstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "href": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "title": "Hands-On Exercise 08",
    "section": "3.0 Data Wrangling",
    "text": "3.0 Data Wrangling\n\n3.1 Geospatial Data Wrangling\nWe use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n3.1 Aspatial Data Wrangling\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 08",
    "section": "4.0 Exploratory Data Analysis (EDA)",
    "text": "4.0 Exploratory Data Analysis (EDA)\n\n4.1 EDA Using Statistical Graphics\nWe can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n4.2 EDA Using Multiple Histogram Plots Distribution of Variables\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n4.3 Drawing Statistical Point Map\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\n#tmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 08",
    "section": "5.0 Hedonic Pricing Modelling in R",
    "text": "5.0 Hedonic Pricing Modelling in R\nIn this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n5.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n5.2 Multiple Linear Regression Method\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n5.3 Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\n\n5.4 Checking for Multicollinearity\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n5.5 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n5.6 Test for Normality Assumption\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the Kolmogorov-Smirnov test\n\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n5.7 Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "href": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "title": "Hands-On Exercise 08",
    "section": "6.0 Building Hedonic Pricing Model using GWmodel",
    "text": "6.0 Building Hedonic Pricing Model using GWmodel\nAfter exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n6.1 Computing Bandwidth\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n6.2 Building Basic GWModel with Fixed and Adaptive Bandwidth\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-02 16:17:26.679824 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-03-02 16:17:28.159495 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-02 16:17:28.160216 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-03-02 16:17:29.80862 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n6.3 Visualisaing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\n#tmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\n\n#tmap_mode(\"view\")\n\n{= 1) +}   tm_view(set.zoom.limits = c(11,14))\nNext, we will visualise the coefficient estimates\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html",
    "href": "Hands-on_Ex/hands_on1.html",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#overview",
    "href": "Hands-on_Ex/hands_on1.html#overview",
    "title": "Hands-On Exercise 01",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#setup",
    "href": "Hands-on_Ex/hands_on1.html#setup",
    "title": "Hands-On Exercise 01",
    "section": "2.0 Setup",
    "text": "2.0 Setup\n\n2.1 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, I extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n\n2.2 Installing R-Packages\nIn this exercise, I will be using two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nI install the required packages using the code chunk below.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-geospatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "3.0 Importing Geospatial Data into R",
    "text": "3.0 Importing Geospatial Data into R\nIn this section, I will import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n3.1 Importing polygon feature data in shapefile format\nDataset used: MP14_SUBZONE_WEB_PL File format: shapefile Data frame type: polygon feature\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our mpsz simple feature data frame, there are 323 multipolygon features, 15 fields and is in the svy21 projected coordinates system.\n\n\n3.2 Importing polyline feature data in shapefile form\nDataset used: CyclingPathGazette File format: shapefile Data frame type: line feature\n\ncyclingpath = st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                      layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our cyclingpath linestring feature data frame, there are 1625 linestring features, 2 fields and is in the svy21 projected coordinates system.\n\n\n3.3 Importing GIS data in kml format\nDataset used: pre-schools-location-kml File format: kml Data frame type: point feature\n\npreschool = st_read(\"~/IS415-GAA/data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we can see that in our preschool point feature data frame, there are 1359 linestring features, 2 fields and is in the wgs84 projected coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "href": "Hands-on_Ex/hands_on1.html#importing-converting-aspatial-data-into-r",
    "title": "Hands-On Exercise 01",
    "section": "4.0 Importing + Converting Aspatial Data into R",
    "text": "4.0 Importing + Converting Aspatial Data into R\nFor aspatial data, such as the listings Airbnb datset, there’s an extra step in the importing process. We’ll import it into a tibble data frame, then convert it into a simple feature data frame.\n\n4.1 Importing aspatial data\nSince our listings data set is in a csv file format, we’ll use the read_csv() function from the readr package, like so:\n\nlistings &lt;- read_csv(\"~/IS415-GAA/data/aspatial/listings.csv\")\n\nRows: 3457 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(listings) \n\nRows: 3,457\nColumns: 18\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ latitude                       &lt;dbl&gt; 1.34537, 1.34754, 1.34531, 1.29015, 1.2…\n$ longitude                      &lt;dbl&gt; 103.9589, 103.9596, 103.9610, 103.8081,…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n\n\nFrom the output message, we can see that in our listing tibble data frame, there are 4252 rows and 16 columns (not features and fields like in our simple data feature frame!) Take note of the latitude and longitude fields - we’ll be using them in the next phase.\n\nAssumption: The data is in the wgs84 Geographic Coordinate System on account of its latitude/longtitude fields.\n\n\n\n4.2 Converting aspatial data\nNow, let’s convert our listing tibble data frame into a by using the st_as_sf() function from the sf package.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThis gives us the new simple feature data frame, listings_sf:\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\n\nNote that a new column called geometry has been added! In addition, longtitude and latitude have both been dropped."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/hands_on1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-On Exercise 01",
    "section": "5.0 Checking the Content of A Simple Feature Data Frame",
    "text": "5.0 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/hands_on1.html#plotting-the-geospatial-data",
    "title": "Hands-On Exercise 01",
    "section": "6.0 Plotting the Geospatial Data",
    "text": "6.0 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. I use plot() to quickly plot a sf object as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "href": "Hands-on_Ex/hands_on1.html#working-with-projection",
    "title": "Hands-On Exercise 01",
    "section": "7.0 Working with Projection",
    "text": "7.0 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, I project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nTo check the coordinate system of mpsz simple feature data frame, I use st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in SVY21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for SVY21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nI take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (25089.46 31299.16 0)\n\n\nPOINT Z (27189.07 32792.54 0)\n\n\nPOINT Z (28844.56 36773.76 0)\n\n\nPOINT Z (24821.92 46303.16 0)\n\n\nPOINT Z (28637.82 35038.49 0)\n\n\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/hands_on1.html#geoprocessing-with-sf-package",
    "title": "Hands-On Exercise 01",
    "section": "8.0 Geoprocessing with sf package",
    "text": "8.0 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n8.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\n8.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNext, I calculate the density of pre-school by planning subzone.\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on1.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 01",
    "section": "9.0 Exploratory Data Analysis (EDA)",
    "text": "9.0 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\nUsing ggplot2 method, I plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html",
    "href": "Hands-on_Ex/hands_on04.html",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates.\nComputing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights.\nParticularly, we will explore using spdep, an R package specially designed for spatial weight analysis.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep,\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyverse\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nIn this section, we will carry out exploratory spatial data anlysis.\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nIn this section, we will explore how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area.\nThe poly2nb() function accepts an argument named queen, which can be set to either TRUE or FALSE. This argument plays a pivotal role in determining the criteria used for identifying neighboring regions. If the queen argument is not explicitly specified, the function defaults to TRUE and the function will generate a list of first-order neighbors using the Queen’s contiguity criteria.\n\n\nWe will start out by computing Queen contiguity weight matrix\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. To see the neighbors for the first polygon in the object, we can just use the following code chunk:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class. We can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str() function.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n\nIn previous section, we have created neighbours based on QUEEN contiguity. In this section, we will try ROOK contiguity to create another set of neighbours which we will call wm_r.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.\n\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column. We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]]and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nThen, we will check the first few observations to see if the coordinates are formatted properly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nOnce we have extracted the coordinates of the centroid of each map unit, we will go ahead and plot neighbours maps.\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\n\n\n\n\n\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\npar(mar = c(0,0,1,0),mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\n\nIn this section, we will explore how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band. This can be achieved by following the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nAverage number of links: 3.681818” in the output refers to the average number of neighboring regions each region has within the distance range of 0-62km. In the context of spatial analysis, a “link” is a connection between two regions that are considered neighbors based on the criteria we have set (in this case, distance of up to 62km).\nSo, an average of 3.681818 means that, on average, each region in our study area has about 3 to 4 neighboring regions within a distance of 62 units.\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix. This time, we will combine table() and card() of spdep to display the matrix more neatly than simply using str().\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\n\n\n\n\nThe purple lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotably, it is observed that each county has exactly six neighbours.\n\n\nWe can plot the adoptive weight matrix we obtained in previous section using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"purple\")\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance Weighted (IDW) method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors, we will use the code chunk below.\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n\n\n\n\n\nTip\n\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by by its corresponding weight, (in this case, 0.125) before being tallied.\n\n\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\n\nIn this section, we will explore how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nSpatial lag with row-standardized weights is a measure that represents the average value of a given variable in the neighbouring regions, taking into account the weights assigned to each neighbour. The weights are usually row-standardized, meaning that the weights assigned to the neighbors of each region sum up to 1. This ensures that the spatial lag is not influenced by the number of neighbors a region has.\nIn this section, we will compute the average neighbor GDPPC value for each county.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nOnce the computation is done, we will retrieve the GDPPC values for the five neighboring regions of Anxiang county,as we have explored in section 6.1. To do so, first, we will save these 5 neighbours as nb1 then will extract their respective GDPPC values.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame through following steps.\n\nFirst, we will create a variable called lag.list\nThen, we will calculate the spatial lag value using lag.listw() function that takes in the spatial weights matrix rswm_q, and the GDPPC values for each region in the hunan dataframe, hunan$GDPPC.\nNext, we will convert lag.list into a dataframe named lag.res\nFinally, we will use left_join() function from the dplyr package to merge the hunan dataframe with lag.res dataframe.\n\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nWe will now look at a few values of the newly created column in hunan called lag GDPPC.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nThis spatial lag measure do not consider spatial weights, but simply sums up the values of the variable for all the neighbouring regions.\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist argument in the nb2listw function to explicitly assign these weights.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the binary weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nhead(lag_sum)\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan dataframe by using left_join.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNext, we will plot both the GDPPC and lag_sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSpatial window average is a measure that calculates the average value of a variable within a specified spatial window or area. The spatial window can be defined in various ways, such as a fixed distance from a point or a predefined geographic area.\nIn this section, we will use row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nThis line of code modifies the wm_q neighbor list by including the diagonal element, resulting in the updated neighbor list wm_qs.\n\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of Anxiang county (area [1]) by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nNext, we will plot both the lag_gdppc and w_avg_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSimilar to the spatial window average, this measure sums up the values of a variable within a specified spatial window. When calculating spatial window sum, we do not need to use row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw() function.\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\", fill.palette=\"plasma\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#overview",
    "href": "Hands-on_Ex/hands_on04.html#overview",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates.\nComputing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights.\nParticularly, we will explore using spdep, an R package specially designed for spatial weight analysis."
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#importing-packages",
    "href": "Hands-on_Ex/hands_on04.html#importing-packages",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep,\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyverse\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on04.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on04.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#exploratory-spatial-data-analysis",
    "href": "Hands-on_Ex/hands_on04.html#exploratory-spatial-data-analysis",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will carry out exploratory spatial data anlysis.\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/hands_on04.html#computing-contiguity-spatial-weights",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area.\nThe poly2nb() function accepts an argument named queen, which can be set to either TRUE or FALSE. This argument plays a pivotal role in determining the criteria used for identifying neighboring regions. If the queen argument is not explicitly specified, the function defaults to TRUE and the function will generate a list of first-order neighbors using the Queen’s contiguity criteria.\n\n\nWe will start out by computing Queen contiguity weight matrix\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. To see the neighbors for the first polygon in the object, we can just use the following code chunk:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class. We can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str() function.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n\nIn previous section, we have created neighbours based on QUEEN contiguity. In this section, we will try ROOK contiguity to create another set of neighbours which we will call wm_r.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.\n\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column. We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]]and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nThen, we will check the first few observations to see if the coordinates are formatted properly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nOnce we have extracted the coordinates of the centroid of each map unit, we will go ahead and plot neighbours maps.\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\n\n\n\n\n\n\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")\n\n\n\n\n\n\n\n\npar(mar = c(0,0,1,0),mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"grey\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"grey\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/hands_on04.html#computing-distance-based-neighbours",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band. This can be achieved by following the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nAverage number of links: 3.681818” in the output refers to the average number of neighboring regions each region has within the distance range of 0-62km. In the context of spatial analysis, a “link” is a connection between two regions that are considered neighbors based on the criteria we have set (in this case, distance of up to 62km).\nSo, an average of 3.681818 means that, on average, each region in our study area has about 3 to 4 neighboring regions within a distance of 62 units.\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix. This time, we will combine table() and card() of spdep to display the matrix more neatly than simply using str().\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\n\n\n\n\nThe purple lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"purple\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotably, it is observed that each county has exactly six neighbours.\n\n\nWe can plot the adoptive weight matrix we obtained in previous section using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"purple\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#spatial-weights-based-on-inverse-distance-weighted-idw-method",
    "href": "Hands-on_Ex/hands_on04.html#spatial-weights-based-on-inverse-distance-weighted-idw-method",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, you will learn how to derive a spatial weight matrix based on Inversed Distance Weighted (IDW) method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors, we will use the code chunk below.\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n\n\n\n\n\nTip\n\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by by its corresponding weight, (in this case, 0.125) before being tallied.\n\n\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/hands_on04.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/hands_on04.html#application-of-spatial-weight-matrix",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "In this section, we will explore how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nSpatial lag with row-standardized weights is a measure that represents the average value of a given variable in the neighbouring regions, taking into account the weights assigned to each neighbour. The weights are usually row-standardized, meaning that the weights assigned to the neighbors of each region sum up to 1. This ensures that the spatial lag is not influenced by the number of neighbors a region has.\nIn this section, we will compute the average neighbor GDPPC value for each county.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nOnce the computation is done, we will retrieve the GDPPC values for the five neighboring regions of Anxiang county,as we have explored in section 6.1. To do so, first, we will save these 5 neighbours as nb1 then will extract their respective GDPPC values.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame through following steps.\n\nFirst, we will create a variable called lag.list\nThen, we will calculate the spatial lag value using lag.listw() function that takes in the spatial weights matrix rswm_q, and the GDPPC values for each region in the hunan dataframe, hunan$GDPPC.\nNext, we will convert lag.list into a dataframe named lag.res\nFinally, we will use left_join() function from the dplyr package to merge the hunan dataframe with lag.res dataframe.\n\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nWe will now look at a few values of the newly created column in hunan called lag GDPPC.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nThis spatial lag measure do not consider spatial weights, but simply sums up the values of the variable for all the neighbouring regions.\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist argument in the nb2listw function to explicitly assign these weights.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the binary weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nhead(lag_sum)\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan dataframe by using left_join.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNext, we will plot both the GDPPC and lag_sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\", fill.palette = \"plasma\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\",fill.palette = \"plasma\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSpatial window average is a measure that calculates the average value of a variable within a specified spatial window or area. The spatial window can be defined in various ways, such as a fixed distance from a point or a predefined geographic area.\nIn this section, we will use row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nThis line of code modifies the wm_q neighbor list by including the diagonal element, resulting in the updated neighbor list wm_qs.\n\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of Anxiang county (area [1]) by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nNext, we will plot both the lag_gdppc and w_avg_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\", fill.palette = \"plasma\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSimilar to the spatial window average, this measure sums up the values of a variable within a specified spatial window. When calculating spatial window sum, we do not need to use row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw() function.\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\", fill.palette=\"plasma\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html",
    "href": "Hands-on_Ex/hands_on07.html",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.\n\n\n\nIn this hands-on exercise, we are interested in delineating homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\n\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(sp,spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\n\n\n\nshan_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\nThe csv file will be import using read_csv function of readr package.\n\nict &lt;- read_csv (\"../data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.\n\n\n\n\n\n\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"#ff7d04\")\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nNext, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n\n\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"../data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\n\n#shan_sf &lt;- read_rds(\"../data/rds/shan_sf.rds\")\n\n\n\n\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\ntm_shape(shan_sf) +\n  tm_fill(\"RADIO_PR\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                palette=\"plasma\",\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\n\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.\n\n\n\nIn this section, we will explore how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = plasma,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n::: {#fig-width : 10 .cell-output-display}\n\n\n\n:::\n\n\n\n\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\ntm_shape(shan_sf_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.\n\n\n\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(shan_sp, \n     border=grey(.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(shan_sp, \n     border=grey(.5))\n\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\n#| fig-width: 10\n\ntm_shape(shan_sf_spatialcluster) +\n  tm_fill(\"SP_CLUSTER\",\n          palette=\"-plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nIn this section, we will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n\n\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\ntm_shape(shan_sf_ngeo_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\ntm_shape(shan_sf_Gcluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#overview",
    "href": "Hands-on_Ex/hands_on07.html#overview",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#analytical-objective",
    "href": "Hands-on_Ex/hands_on07.html#analytical-objective",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this hands-on exercise, we are interested in delineating homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#importing-packages",
    "href": "Hands-on_Ex/hands_on07.html#importing-packages",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "The R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(sp,spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on07.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\n\n\n\nshan_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\nThe csv file will be import using read_csv function of readr package.\n\nict &lt;- read_csv (\"../data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html",
    "href": "Hands-on_Ex/hands_on05.html",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "“The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable. The variable can assume values either:\n\nat any point on a continuous surface (such as land use type or annual precipitation levels in a region); \nat a set of fixed sites located within a region (such as prices at a set of retail outlets); or \nacross a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region). \n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001):  \n\nthe difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation;\nmeasurement error;\nspatial diffusion, spillover, interaction, and dispersal processes;\ninheritance by one variable through causal association with another;\nmodel misspecification \n\nIn this hands-on exercise, we will explore how to compute Global and Local Measures of Spatial Autocorrelation by using spdep package.\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep for computing spatial weights, global and local spatial autocorrelation statistics\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyversefor wrangling attribute data in R\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\nMoran’s I and Geary’s c are well known tests for spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function. \nIn this section, we will explore how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation in R environment.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above!\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\n\n\nIn this section, we will explore how to perform Moran’s I statistics testing by using moran.test() of spdep package in R environment.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.300749970, which is significantly different from the expectation under the null hypothesis of -0.011494253. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is 4.7351. This is the value of the Moran’s I statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 1.095e-06, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘greater’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.30075, which is the same as in our previous test (See Section 5.3.1).\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Moran’s I test. There is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using hist() and abline() of R Graphics.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Moran’s I statistics is -0.01504572. This is close to zero, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Moran’s I statistics is 0.004371574. This gives us an idea of the spread of the Moran’s I statistics under the null hypothesis.\nThe minimum and maximum of the simulated Moran’s I statistics are -0.18339 and 0.27593, respectively. This gives us the range of Moran’s I statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are -0.06168, -0.02125, and 0.02611, respectively. These give us more detailed information about the distribution of the Moran’s I statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at 0 to represent the expected value under the null hypothesis.\n\n\nAlternatively, we can also run a similar plot using ggplot2 package.\n\n# Create a data frame with the simulated Moran's I values\ndf &lt;- data.frame(MoranI = bperm$res[1:999])\n\n# Create the histogram\nggplot(df, aes(x = MoranI)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(aes(xintercept = 0.05), color = \"red\", linetype = \"dashed\") +\n  xlab(\"Simulated Moran's I\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Histogram of Simulated Moran's I\")\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nhe Geary’s C statistic is 0.6907223, which is significantly different from the expectation under the null hypothesis of 1.0000000. This suggests that there is a significant local spatial structure in the data.\nThe standard deviate of the Geary’s C statistic is 3.6108. This is the value of the Geary’s C statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 0.0001526, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘Expectation greater than statistic’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Geary’s C statistic is 0.69072, which is the same as in your previous test.\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Geary’s C test. There is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nSimilart to what we did in Moran’s I test, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Geary’s C statistics is 1.004402. This is close to 1, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Geary’s C statistics is 0.007436493. This gives us an idea of the spread of the Geary’s C statistics under the null hypothesis.\nThe minimum and maximum of the simulated Geary’s C statistics are 0.7142 and 1.2722, respectively. This gives us the range of Geary’s C statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are 0.9502, 1.0052, and 1.0595, respectively. These give us more detailed information about the distribution of the Geary’s C statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at Geary’s C = 1 represents the expected value under the null hypothesis of no spatial autocorrelation.\n\n\n\n\n\n\n\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Moran’s I statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, 3, 5, and 6, indicating that the spatial autocorrelation at these distances is statistically significant. For lag 4, however, the p-value is greater than 0.05, indicating that the spatial autocorrelation at this distance is not statistically significant.\nThe Moran’s I statistic is positive for lags 1 to 4, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 5 and 6, the Moran’s I statistic is negative, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Geary’s C statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, and 5, indicating that the spatial autocorrelation at these distances is statistically significant. For lags 3, 4, and 6, however, the p-values are greater than 0.05, indicating that the spatial autocorrelation at these distances is not statistically significant.\nThe Geary’s C statistic is less than 1 for lags 1, 2, and 3, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 4, 5, and 6, the Geary’s C statistic is greater than 1, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\n\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will explore how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(z != E(Ii)): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth shown above is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values. The code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\n\n\nNext, we will plot Moran scatterplot with standardised variables. First we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\n\n\n\n\n\nReflection\n\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\n\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\nNow that we have tried plotting Moran scatterplot, we will proceed to carry out necessary steps for preparing LISA maps. The code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\nNow that we have prepared all necessary steps, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\",\n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\ntmap_arrange(gdppc, LISAmap,localMI.map, pvalue.map, asp=2, ncol=2)\n\n\n\n\n\n\n\n\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas. The term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). Getis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas. It measures the degree of association that results from the concentration of weighted points (or area represented by a weighted point) and all other weighted points included within a radius of distance d from the original weighted point. It identifies areas where high or low cluster in space. \nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\n\n\n\n\n\n\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\n\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n\n\n\n\nReflection\n\n\n\nthe code chunk above performs three tasks. First, it convert the output vector gi.fixed into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (fixed)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap_adaptive &lt;-tm_shape(hunan.gi) +\n  tm_fill(\"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (adaptive)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_adaptive, asp=1, ncol=2)\n\n\n\n\n\n\n\n\ntmap_arrange(Gimap, Gimap_adaptive, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#overview",
    "href": "Hands-on_Ex/hands_on05.html#overview",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "“The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable. The variable can assume values either:\n\nat any point on a continuous surface (such as land use type or annual precipitation levels in a region); \nat a set of fixed sites located within a region (such as prices at a set of retail outlets); or \nacross a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region). \n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001):  \n\nthe difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation;\nmeasurement error;\nspatial diffusion, spillover, interaction, and dispersal processes;\ninheritance by one variable through causal association with another;\nmodel misspecification \n\nIn this hands-on exercise, we will explore how to compute Global and Local Measures of Spatial Autocorrelation by using spdep package.\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#importing-packages",
    "href": "Hands-on_Ex/hands_on05.html#importing-packages",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspdep for computing spatial weights, global and local spatial autocorrelation statistics\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyversefor wrangling attribute data in R\nknitr\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on05.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on05.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/hands_on05.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Moran’s I and Geary’s c are well known tests for spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function. \nIn this section, we will explore how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation in R environment.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above!\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\n\n\nIn this section, we will explore how to perform Moran’s I statistics testing by using moran.test() of spdep package in R environment.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.300749970, which is significantly different from the expectation under the null hypothesis of -0.011494253. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is 4.7351. This is the value of the Moran’s I statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 1.095e-06, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘greater’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Moran’s I statistic is 0.30075, which is the same as in our previous test (See Section 5.3.1).\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.In conclusion, the test suggests that there is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Moran’s I test. There is significant positive spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using hist() and abline() of R Graphics.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Moran’s I statistics is -0.01504572. This is close to zero, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Moran’s I statistics is 0.004371574. This gives us an idea of the spread of the Moran’s I statistics under the null hypothesis.\nThe minimum and maximum of the simulated Moran’s I statistics are -0.18339 and 0.27593, respectively. This gives us the range of Moran’s I statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are -0.06168, -0.02125, and 0.02611, respectively. These give us more detailed information about the distribution of the Moran’s I statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at 0 to represent the expected value under the null hypothesis.\n\n\nAlternatively, we can also run a similar plot using ggplot2 package.\n\n# Create a data frame with the simulated Moran's I values\ndf &lt;- data.frame(MoranI = bperm$res[1:999])\n\n# Create the histogram\nggplot(df, aes(x = MoranI)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(aes(xintercept = 0.05), color = \"red\", linetype = \"dashed\") +\n  xlab(\"Simulated Moran's I\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Histogram of Simulated Moran's I\")\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nhe Geary’s C statistic is 0.6907223, which is significantly different from the expectation under the null hypothesis of 1.0000000. This suggests that there is a significant local spatial structure in the data.\nThe standard deviate of the Geary’s C statistic is 3.6108. This is the value of the Geary’s C statistic standardized to have zero mean and unit variance under the null hypothesis.\nThe p-value is 0.0001526, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\nThe alternative hypothesis is ‘Expectation greater than statistic’, suggesting that the test is one-sided and the observed pattern is more clustered than would be expected under the null hypothesis of spatial randomness.\n\nIn conclusion, the test suggests that there is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe Geary’s C statistic is 0.69072, which is the same as in your previous test.\nThe p-value is 0.001, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the Monte Carlo simulation confirms the results of the previous Geary’s C test. There is significant positive global spatial autocorrelation in the hunan$GDPPC data. This means that areas with similar values of GDPPC are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n\nSimilart to what we did in Moran’s I test, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\n\nThe mean of the simulated Geary’s C statistics is 1.004402. This is close to 1, which is what we would expect under the null hypothesis of spatial randomness.\nThe variance of the simulated Geary’s C statistics is 0.007436493. This gives us an idea of the spread of the Geary’s C statistics under the null hypothesis.\nThe minimum and maximum of the simulated Geary’s C statistics are 0.7142 and 1.2722, respectively. This gives us the range of Geary’s C statistics that we might expect to see by chance alone.\nThe 1st quartile, median, and 3rd quartile are 0.9502, 1.0052, and 1.0595, respectively. These give us more detailed information about the distribution of the Geary’s C statistics under the null hypothesis.\n\nThe histogram provide a visual representation of this distribution, with a vertical red line at Geary’s C = 1 represents the expected value under the null hypothesis of no spatial autocorrelation."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#spatial-correlogram-for-global-spatial-autocorrelation-statistics",
    "href": "Hands-on_Ex/hands_on05.html#spatial-correlogram-for-global-spatial-autocorrelation-statistics",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Moran’s I statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, 3, 5, and 6, indicating that the spatial autocorrelation at these distances is statistically significant. For lag 4, however, the p-value is greater than 0.05, indicating that the spatial autocorrelation at this distance is not statistically significant.\nThe Moran’s I statistic is positive for lags 1 to 4, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 5 and 6, the Moran’s I statistic is negative, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other).\n\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat statistical conclusion can we draw from the output above?\nIn the output above, the Geary’s C statistic for each lag (1 to 6) is given along with its expectation under the null hypothesis of spatial randomness, variance, standard deviate, and p-value.\n\nThe p-values are all less than 0.05 for lags 1, 2, and 5, indicating that the spatial autocorrelation at these distances is statistically significant. For lags 3, 4, and 6, however, the p-values are greater than 0.05, indicating that the spatial autocorrelation at these distances is not statistically significant.\nThe Geary’s C statistic is less than 1 for lags 1, 2, and 3, indicating positive spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located near each other). For lags 4, 5, and 6, the Geary’s C statistic is greater than 1, indicating negative spatial autocorrelation at these distances (i.e., areas with similar values of GDPPC are more likely to be located far from each other)."
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#local-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/hands_on05.html#local-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will explore how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(z != E(Ii)): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth shown above is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values. The code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"-plasma\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"plasma\", \n          title = \"local moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/hands_on05.html#creating-a-lisa-cluster-map",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\n\n\nNext, we will plot Moran scatterplot with standardised variables. First we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\n\n\n\n\n\nReflection\n\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\n\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\nNow that we have tried plotting Moran scatterplot, we will proceed to carry out necessary steps for preparing LISA maps. The code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\nNow that we have prepared all necessary steps, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#e9e9e9\", \"#eff635\", \"#61adab\", \"#5071ea\", \"#4225df\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\",\n          title = \"LISA classes\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\ntmap_arrange(gdppc, LISAmap,localMI.map, pvalue.map, asp=2, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on05.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/hands_on05.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas. The term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). Getis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas. It measures the degree of association that results from the concentration of weighted points (or area represented by a weighted point) and all other weighted points included within a radius of distance d from the original weighted point. It identifies areas where high or low cluster in space. \nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below.\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\n\n\n\n\n\n\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\n\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n\n\n\n\nReflection\n\n\n\nthe code chunk above performs three tasks. First, it convert the output vector gi.fixed into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (fixed)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nGimap_adaptive &lt;-tm_shape(hunan.gi) +\n  tm_fill(\"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-plasma\",\n          title = \"local Gi (adaptive)\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_adaptive, asp=1, ncol=2)\n\n\n\n\n\n\n\n\ntmap_arrange(Gimap, Gimap_adaptive, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html",
    "href": "Hands-on_Ex/hands_on03.html",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This hands-on exercise will explore the basic concepts and methods of Spatial Point Pattern Analysis.\nParticularly, we will explore using spatstat, an R package specially designed for Spatial Point Pattern Analysis.\n\n\nToday, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?\n\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, devtools,sp)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")%&gt;% st_transform(crs =3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"../data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 338\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.653796e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)\n\n\n\n\n\n\n\n\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n306.6986 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\ntm = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\nbp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"BUKIT PANJANG\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.51429, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.7747, p-value = 0.002306\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#overview",
    "href": "Hands-on_Ex/hands_on03.html#overview",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "Today, we will use Spatial Point Pattern Analysis to analyse the spatial point processes of childcare centers in Singapore.\nThe specific questions we would like to address through this exercise are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nWhere are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-packages",
    "href": "Hands-on_Ex/hands_on03.html#importing-packages",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, devtools,sp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "href": "Hands-on_Ex/hands_on03.html#importing-datasets-to-r-environment",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")%&gt;% st_transform(crs =3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nsg_sf &lt;- st_read(dsn = \"../data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "childcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n\nAfter checking and assigning correct referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns. We will use tmap to create an interactive point symbol map.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nMy original tmap version was 3.99 and tmap_mode('view') does not work with the version. Hence, we have to download an older version of tmap that is compatible with using the code chunk below:\ninstall_version(\"tmap\", \"3.3-4\")\nWhile it is a good practice to keep the packages updated, some functions might be unavailable in certain package versions. Using the code chunk above, we can pull older or achieved versions of the R-packages and apply in our code.\n\n\n\n\n\nspatstat requires the analytical data in ppp object form. Hence we will convert sf objects to ppp objects using as.ppp() function by providing the point coordinates and the observation window.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nplot(childcare_ppp)\n\n\n\n\nNext, we will take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nSince the above code chunk returns TRUE, we will use the multiplicity() function to count the number of co-indicence point.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nIf we want to know how many locations have more than one point event, we can use sum()\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 338\n\n\nThe output shows that there are 885 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nReflection\n\n\n\nCan we spot the duplicate points from the map shown above?\nYes, if we zoom in and look closely to the points, we may observe that some points has darker shade than others, despite using the same color for all points. Those points with darker shade may indicate the duplicated points, as a result of overlap.\n\n\n\nHow do we overcome the issue of data duplicates?\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNext, we will check if there is still any duplicate points in our dataset. We will use any(duplicated()) function to do so.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical observation boundary like Singapore boundary, for example. In spatstat, an object called owin is specially designed to represent a observation window.\nSince we have imported the Singapore boundary in previous section, we will now convert the sg_sf object into an owin object.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of this newly created owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.653796e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "href": "Hands-on_Ex/hands_on03.html#st-order-spatial-point-pattern-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "After data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package.\n\n\nKernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.\nIn this session, we will compute the kernel density estimation (KDE) layers for childcare services in Singapore.\n\n\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events. Particularly, bw.diggle() argument can be used to automatically select a bandwidth for computing the kernel density.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can customise the code chunk above based on different congifurations required.\n\nbw.diggle() (Cross Validated)is used for automatic bandwidth selection. Other methods such as bw.CvL() (Cronie and van Lieshout’s Criterion), bw.scott()(Scott’s Rule) or bw.ppl() (Likelihood Cross Validation)can also be used.\nBy default, the smoothing kernel used is gaussian. However, we can specify other smoothing methods such as: epanechnikov, quarticor disc.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nYou can also retrieve the bandwidth used to compute the KDE layer.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n306.6986 \n\n\nNow, we will try to plot the kernel density derived from automatic bandwidth selection approach.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nAnalyzing from the output map above, the density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”. Hence, we need to rescale the value, which will be explored in next session.\n\n\n\nIn this session, we will use rescale() function of spatstat package to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can try to re-run the same density() function we tried above, using the rescaled data set.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n\n\nAs we have briefly explored different bandwidth selection methods available in spatstat package, we will now try out each of them and compare the resulting KDE layer, using the same dataset.\n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\nkde_childcareSG.bw.CvL &lt;- density(childcareSG_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.CvL)\n\n\n\n\n\n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\nkde_childcareSG.bw.scott &lt;- density(childcareSG_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.scott)\n\n\n\n\n\n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\nkde_childcareSG.bw.ppl &lt;- density(childcareSG_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw.ppl)\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.bw.CvL, main = \"bw.CvL\")\nplot(kde_childcareSG.bw.scott, main = \"bw.scott\")\nplot(kde_childcareSG.bw.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nHow do we know which approach to use?\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\n\n\n\n\n\n\nAs we explored briefly, the kernel method used in density.ppp(), by default, is Gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\n\n\n\n\nNow, we will take a look at how different smoothing methods work by comparing the resultant KDE layers as below.\n\nkde_childcareSG.gaussian &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_childcareSG.epanechnikov &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_childcareSG.quartic &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_childcareSG.disc &lt;- density(childcareSG_ppp.km, \n                          sigma=bw.ppl, \n                          edge=TRUE, \n                          kernel=\"disc\")\n         \npar(mfrow=c(2,2))  \nplot(kde_childcareSG.gaussian, main=\"Gaussian\")\nplot(kde_childcareSG.epanechnikov, main=\"Epanechnikov\")\nplot(kde_childcareSG.quartic, main=\"Quartic\")\nplot(kde_childcareSG.disc, main=\"Disc\")\n\n\n\n\n\n\n\n\n\nIn this session, generation of a Kernel Density Estimation (KDE) layer is performed by specifying a bandwidth of 600 meters. It is noteworthy that within the following code snippet, a sigma value of 0.6 is utilized. This choice is deliberate and corresponds to the unit of measurement employed in the childcareSG_ppp.km object, which is expressed in kilometers. Consequently, the representation of 600 meters in the KDE calculation is accurately denoted as 0.6 kilometers.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\nWhen we used the fixed bandwidth, the result is very sensitive to highly skew distribution of spatial point patterns over across geographical units, such as the distinction between urban and rural areas. To address this inherent challenge, an alternative approach involves the adoption of an adaptive bandwidth.\nTo do so, we can use density.adaptive() from the spatstat package to compute adaptive kernel density estimation layer as follows.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")  \nplot(kde_childcareSG_adaptive)\n\n\n\n\nNow, we will compare the two output maps side-by-side.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\nConversion of KDE output into a grid object can be done to make it compatible with mapping applications. It is important to note that the result remains unchanged.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe will look at the properties of this new raster layer.\n\nkde_childcareSG_bw_raster\n\nYou will notice that the CRS information is missing in the raster layer output. Hence, we need to assign an appropriate CRS value to the layer before mapping.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/hands_on03.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The code chunk below will be used to extract the target planning areas.\n\npg = mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\ntm = mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\nbp = mpsz_sf %&gt;% filter(PLN_AREA_N == \"BUKIT PANJANG\")\njw = mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(pg), main = \"Punggol\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(bp), main = \"Bukit Panjang\")\nplot(st_geometry(jw), main = \"Jurong West\")\n\n\n\n\n\n\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nbp_owin = as.owin(bp)\njw_owin = as.owin(jw)\n\n\n\n\nIn this session, we will extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_bp_ppp = childcare_ppp_jit[bp_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_bp_ppp.km = rescale(childcare_bp_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nFinally, we will plot the locations of the childcare centres in our selected 4 study areas.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_bp_ppp.km, main=\"Bukit Panjang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\nOnce all the data wrangling is complete, we will follow the same method we explored in session 5 and plot KDE layers.\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\npar(mfrow=c(1,2))\nplot(density(childcare_bp_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Bukit Panjang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "In this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.51429, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\nclarkevans.test(childcare_bp_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_bp_ppp\nR = 0.7747, p-value = 0.002306\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/hands_on03.html#nd-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 03",
    "section": "",
    "text": "The G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will explore how to compute G-function estimation by using Gest() of spatstat package. We will also explore how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nIn this example, we will use Punggol Planning Area to compute G-function.\n\nG_PG = Gest(childcare_pg_ppp, correction= \"border\")\nplot(G_PG, xlim=c(0,500))\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Punggol are randomly distributed.\nH1= The distribution of childcare services at Punggol are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nG_PG.csr &lt;- envelope(childcare_pg_ppp, Gest, nsim= 900)\n\nGenerating 900 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........\n900.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(G_PG.csr)\n\n\n\n\n\n\n\n\nWe will carry out the same procedure above for Tampines planning area as well\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will explore how to compute F-function estimation by using Fest() of spatstat package.\n\n\nIn this example, we will use Jurong West Planning Area to compute F-function.\n\nF_JW = Fest(childcare_jw_ppp)\nplot(F_JW)\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Jurong West are randomly distributed.\nH1= The distribution of childcare services at Jurong West are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nF_JW.csr &lt;- envelope(childcare_jw_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(F_JW.csr)\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Bukit Panjang Planning Area to compute F-function.\n\nK_bp = Kest(childcare_bp_ppp, correction = \"Ripley\")\nplot(K_bp, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Bukit Panjang are randomly distributed.\nH1= The distribution of childcare services at Bukit Panjang are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nK_BP.csr &lt;- envelope(childcare_bp_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(K_BP.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, we will learn how to compute K-function estimates by using Kest() of spatstat package.\n\n\nIn this example, we will use Tampines Planning Area to compute L-function.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\n\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo test our hypotheses, we will run a Monte Carlo simulation.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nOnce the simulation is done, we will plot the results.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html",
    "href": "Hands-on_Ex/hands_on02.html",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#overview",
    "href": "Hands-on_Ex/hands_on02.html#overview",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-packages",
    "href": "Hands-on_Ex/hands_on02.html#importing-packages",
    "title": "Hands-On Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nSince readr, tidyr and dplyr are part of tidyverse package, we will only need to install and import tidyverse.\n\npacman::p_load(sf, tmap, tidyverse)\ndevtools::install_github(\"thomasp85/patchwork\")\n\nSkipping install of 'patchwork' from a github remote, the SHA1 (d9437579) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(patchwork)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "href": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "title": "Hands-On Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use two datasets as follows:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\nFor geospatial data, we will use st_read() function of sf package to import shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"~/IS415-GAA/data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "title": "Hands-On Exercise 02",
    "section": "4.0 Data Preparation and Wrangling",
    "text": "4.0 Data Preparation and Wrangling\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n4.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n4.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nHence, we will standard the data values in these two fields.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 02",
    "section": "5.0 Choropleth Mapping Geospatial Data Using tmap",
    "text": "5.0 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n5.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\",\n    fill.palette =\"plasma\")\n\n\n\n\n\n\n5.2 Plotting a choropleth map quickly by using qtm()\nHowever, in real-life application, the quick choropleth map produced in the previous session may not be sufficient enough to properly visualize geospatial data. However, tmap packages allow us to customise and control how we design our choropleth maps. We will exploit tmap’s drawing elements to create a high quality cartographic choropleth map that includes more accurate and informative information.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nNext, we will breakdown the different tmpa functions used to plot the additional elements in the map above.\n\n\n5.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e.DEPENDENCY)\n\n\n\n\n\n5.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n5.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "title": "Hands-On Exercise 02",
    "section": "6.0 Data Classification Methods of tmap",
    "text": "6.0 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher, and\njenks.\n\n\n6.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNext, we will try equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nNext, we will try other data classification methods.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\nAlso, we can try exploring using the same classification methods, but with different numbers of classes. As an example, we will use kmeans clustering method with different class sizes (2,6,10,20)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\noriginal &lt;- ggplot(data=mpsz_pop2020, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\n#Try to remove outliers\nmpsz_pop2020_no_outlier &lt;- subset(mpsz_pop2020, mpsz_pop2020$DEPENDENCY &lt;3)\n\nfiltered &lt;- ggplot(data=mpsz_pop2020_no_outlier, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\noriginal + filtered\n\n\n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 1.00. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we use the above-mentioned breaks?\nThe reason behind choosing those break points is mainly stemmed from the 1st quantile and 3rd quantile of the datasets. While the minimum value is 0.10 and maximum value is 19.0, the 1st quantile (the value under which 25% of data points are found) is 0.7147 and the 3rd quantile (the value under which 75% of data points are found) is 0.8763. Using these two values, we may assume that the dataset might have outliers on the right end, and the majority of the dataset might be scattered in the range of 0.7147 and 0.8763. Hence, we use the mentioned break points.\nOtherwise, we can use non-heuristic approach in this case as well. We can easily plot the data to see the distribution first (see above). As we assumed earlier, you can clearly see the outliers on the right-side of the histogram. After removing the outliers (temporarily), we can see the new plot (see above). Majority of the datasets are scattered within the range of 0.6 - 1.0. This is why we break the datasets into 0.6, 0.7, 0.8 and 0.9 respectively so that there is balanced quantity of data points in each break.\n\n\nUsing this information, we will now proceed to plot the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "href": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "title": "Hands-On Exercise 02",
    "section": "7.0 Controlling and Customizing Map Layots",
    "text": "7.0 Controlling and Customizing Map Layots\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n7.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-On Exercise 02",
    "section": "8.0 Drawing Small Multiple Choropleth Maps",
    "text": "8.0 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-On Exercise 02",
    "section": "9.0 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "9.0 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\nFor example, we have select the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#references",
    "href": "Hands-on_Ex/hands_on02.html#references",
    "title": "Hands-On Exercise 02",
    "section": "10. References",
    "text": "10. References\nTutorial provided by Professor Kam Tin Seong©, Singapore Management University\nReference: https://r4gdsa.netlify.app/chap02.html\n\n10.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n10.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n10.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "library_workship.html#why-are-you-here",
    "href": "library_workship.html#why-are-you-here",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Why are you here?",
    "text": "Why are you here?\n\nBecause you are :\n\ntired of creating PowerPoint slides\ninterested to create awesome web slides using R"
  },
  {
    "objectID": "library_workship.html#hello-plot",
    "href": "library_workship.html#hello-plot",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Hello Plot",
    "text": "Hello Plot\n\n\n\n\npacman::p_load(tidyverse)\nexam_data &lt;- read_csv(\"data/library/Exam_data.csv\")\n\nggplot(data=exam_data, aes(x=MATHS))+\n  geom_histogram(bins=10,\n                 boundary =100,\n                 color=\"white\",\n                 fill =\"grey\")+\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "library_workship.html#working-with-tabsets",
    "href": "library_workship.html#working-with-tabsets",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "Working with Tabsets",
    "text": "Working with Tabsets\n\nCode Chunk (Highlighted Lines)Plot\n\n\n\n\npacman::p_load(tidyverse)\nexam_data &lt;- read_csv(\"data/library/Exam_data.csv\")\n\nggplot(data=exam_data, aes(x=MATHS))+\n  geom_histogram(bins=10,\n                 boundary =100,\n                 color=\"white\",\n                 fill =\"grey\")+\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "library_workship.html#section",
    "href": "library_workship.html#section",
    "title": "R Workshop Series:  RevealJS Presentation",
    "section": "",
    "text": "Adding Table Static html table\n\nhead(exam_data) %&gt;%\n  knitr::kable(format=\"html\")\n\n\n\n\nID\nCLASS\nGENDER\nRACE\nENGLISH\nMATHS\nSCIENCE\n\n\n\n\nStudent321\n3I\nMale\nMalay\n21\n9\n15\n\n\nStudent305\n3I\nFemale\nMalay\n24\n22\n16\n\n\nStudent289\n3H\nMale\nChinese\n26\n16\n16\n\n\nStudent227\n3F\nMale\nChinese\n27\n77\n31\n\n\nStudent318\n3I\nMale\nMalay\n27\n11\n25\n\n\nStudent306\n3I\nFemale\nMalay\n31\n16\n16"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I am Khant Min Naing, an undergraduate student at Singapore Management University. This course website is developed to document my learning journey at IS415: Geospatial Analytics and Applications under Professor Kam Tin Seong. Follow my journey as I venture into the world of big data, geospatial analysis and urban planning.\nConnect with me!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "",
    "text": "Dengue Hemorrhagic Fever, also referred to as dengue fever in short, is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. Historically, the majority of outbreaks during the first half of the 20th century were reported in East Asia and Western Pacific countries (Chen, 2018). In 2015, Taiwan experienced its most severe outbreak of dengue fever, recording over 43,000 cases and 228 fatalities (Yi-ning & Kao, 2023). Subsequently, the annual reported cases of dengue fever remained below 200. However, in 2023, there was a significant increase with 26,703 recorded cases. Notably, the majority of these outbreaks were reported in Tainan City, situated in the southern part of the island.\nThis study aims to use spatio-temporal analysis techniques to analyze if the distribution of dengue fever outbreak in Tainan City are independent from space and time. The study also attempts to identify clusters and outliers, as well as emerging hot spots/cold spots within the study area. The initial phase of the study utilizes Global and Local Spatial Autocorrelation modelling to discern the spatio-temporal pattern of the disease outbreak. Subsequently, Emerging Hot Spots Analysis (EHSA) is applied to identify spatio-temporal clusters. The results of this study will provide valuable insights into the spatial and temporal dynamics of dengue fever outbreak in Tainan City. By identifying clusters and outliers, as well as emerging hot spots and cold spots, it will help inform targeted interventions and prevention strategies to effectively control the spread of the disease."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatial-autocorrelation",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.1 Spatial Autocorrelation",
    "text": "2.1 Spatial Autocorrelation\nThe first law of geography states that everything is related to everything else but near things are more related than distant things (Tobler, 1970). Spatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable, as explained by Tobler’s first law. The concept of spatial autocorrelation, although it may be viewed as a special case of correlation, has a meaning all its own. Whereas correlation statistics were designed to show relationships between or among variables, spatial autocorrelation shows the correlation within variables across georeferenced space (Getis, 2009).\nThe variable can assume values either at any point on a continuous surface (such as land use type or annual precipitation levels in a region); at a set of fixed sites located within a region (such as prices at a set of retail outlets); or across a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region).\n\n\n\nExamples of configurations of areas showing different types of spatial autocorrelation (Ref: Moraga, 2023)\n\n\nWhere adjacent observations have similar data values the map shows positive spatial autocorrelation. Where adjacent observations tend to have very contrasting values then the map shows negative spatial autocorrelation. Spatial autocorrelation in a variable can be exogenous or endogenous. Spatial autocorrelation may arise from any one of the following situations (Haining, 2001): a) the difference between the scale of variation of a phenomenon and the scale of the spatial framework used to capture or represent that variation; b) measurement error; c) spatial diffusion, spillover, interaction, and dispersal processes; d) inheritance by one variable through causal association with another; e) model misspecification.\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas). Cliff and Ord (1973) demonstrated statistically how one can test residuals of regression analysis for spatial randomness by using spatial autocorrelation statistics. These spatial statistics are well suited for a number of geospatial analysis such as:\n\ndetecting clusters or outliers: spatial clustering algorithms are dependent on the conjecture that there is spatial autocorrelation among some nearby values of one or more variables of interest.\nmeasuring the strength of spatial effect on variable: spatial autocorrelation cofficients in regression models help us to understand the strength of spatial effects (Haining, 1990; Anselin and Rey, 1991). \nassessing the assumptions of stationarity: spatial autocorrelation measures allow for tests on hypotheses of no spatial differences in distribution parameters such as the mean and variance. (Haining, 1977; Leung et al., 2000).\ndesigning an appropriate spatial sample: if the goal is to avoid, as much as possible, spatial autocorrelation in the sample, then a reasonable sample design would benefit from a study of spatial autocorrelation in the region where the sample is selected (:egendre & Fortin, 1989; Legendre et al., 2002; Griffith, 2005).\n\nMeasures of spatial autocorrelation can be categorized as global or local indicators of spatial association (LISA). Moran’s I (Moran, 1950) and Geary’s C (Geary, 1954) are well known tests for global spatial autocorrelation. They represent two special cases of the general cross-product statistic that measures spatial autocorrelation. Moran’s I is produced by standardizing the spatial autocovariance by the variance of the data. Geary’s c uses the sum of the squared differences between pairs of data values as its measure of covariation. Both statistics depend on a spatial structural specification such as a spatial weights matrix or a distance related decline function.\nLISA statistics quantify the degree to which points in proximity to a given point exhibit similar values, based on a measure of contiguity within a defined radius. This makes them instrumental in identifying local spatial autocorrelation (Anselin, 1995). Global spatial autocorrelation statistics such as Moran’s I and Geary’s C can be further decomposed and used as LISA statistics to analyse local spatial autocorrelation as well. Another widely employed LISA statistic in spatial research is the Getis-Ord Gi* statistic (Getis & Ord, 1992). The Gi* statistic is essentially a ratio of the sum of values within a specified area to the global total, providing a measure of local concentration relative to the overall distribution."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.2 Emerging Hotspot Analysis",
    "text": "2.2 Emerging Hotspot Analysis\nA hotspot is defined as an area exhibiting a higher concentration of events than would be expected under a random distribution. This concept, initially rooted in the study of point distributions and spatial arrangements, has significantly evolved (Chakravorty, 1995). There are different methods for analyzing spatial patterns and detecting hotspots including spatial autocorrelation and cluster analysis(). Emerging Hot Spot Analysis (EHSA) is a specialised spatio-temporal technique for analysing hotspots over observation time period. It combines two established methods: the traditional Getis-Ord Gi* statistic for hotspot analysis and the time-series Mann-Kendall test for monotonic trends. EHSA’s primary objective is to evaluate the temporal changes in hot and cold spots, specifically addressing whether these spots are intensifying, diminishing, or remaining stable (Parry & Locke, 2022).\nEHSA works by calculating the Gi* for each time period. The series of Gi* at each location is treated as a time-series and evaluated for a trend using the Mann-Kendall statistic. The Gi* and the Mann-Kendall are compared together to create 17 unique classifications base on ESRI’s emerging hot spot classification criteria to help better understand how the locations have changed over time.\n\n\n\n\n\n\n\nPattern Name\nDefinition\n\n\n\n\nNo Pattern Detected\nDoes not fall into any of the hot or cold spot patterns defined below.\n\n\nNew Hot Spot\nA location that is statistically significant hot spot for the final time step and has never been a statistically significant hot spot before.\n\n\nConsecutive Hot Spot\nA location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.\n\n\nIntensifying Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of high counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Hot Spot\nA location that has been statistically hot spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering over time.\n\n\nDiminishing Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Hot Spot\nA statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically cold spots.\n\n\nOscillating Hot Spot\nA statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nHistorical Hot Spot\nThe most recent time period is not hot, but at least 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nNew Cold Spot\nA location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.\n\n\nConsecutive Cold Spot\nA location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.\n\n\nIntensifying Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.\n\n\nDiminishing Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Cold Spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\nOscillating Cold Spot\nA statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.\n\n\nHistorical Cold Spot\nThe most recent time period is not cold, but at least 90 percent of the time-step intervals have been statistically significant cold spots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatio-temporal-analytics-in-epidemiological-studies",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#spatio-temporal-analytics-in-epidemiological-studies",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "2.3 Spatio-Temporal Analytics in Epidemiological Studies",
    "text": "2.3 Spatio-Temporal Analytics in Epidemiological Studies\nSpatial autocorrelation and hotspot analysis play a crucial role in epidemiological studies by enabling to discover and analyse the distribution and clustering of transmissible diseases.These diseases, due to their transmission dynamics, often exhibit distinct spatial patterns and commonly occur in spatial clusters (Mergenthaler et al., 2022). Understanding and quantifying spatial autocorrelation is a pivotal aspect of epidemiological research. It provides insights into the dynamics and spread of diseases. Moreover, it is essential for the statistical testing of epidemiological risk factors (Mergenthaler et al., 2022).\nSpatial autocorrelation can also be leveraged to detect spatiotemporal clustering and outliers, thereby aiding in the identification of disease hotspots and the targeting of control measures. Numerous works in this area integrate the temporal component of emerging hotspot analysis with the spatial component of autocorrelation analysis. Emerging hotspot analysis helps in identifying areas that are experiencing a significant increase in disease incidence over time. Singh et al. (2023) highlighted in particular how understanding the spatiotemporal patterns of infectious illnesses fever provides important insights on the spread of the disease and how it relates to local risk factors.\nOverall, spatial autocorrelation and emerging hotspot analysis provide valuable insights into the spatial patterns and dynamics of epidemiological studies of communicable diseases. Consequently, this understanding holds promise for developing forecasting models to optimise resource allocation and plan effective vector control measures (Singh et al., 2023) in low- and middle-income countries."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#datasets",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.1 Datasets",
    "text": "4.1 Datasets\nIn this exercise, we will use two datasets. The first dataset, TAIWAN_VILLAGE_2020, is a geospatial dataset that delineates the village boundaries of Taiwan. This data is presented in the ESRI shapefile format and is based on the Taiwan Geographic Coordinate System. This data is extracted from Historical map data of the village boundary: TWD97 longitude and latitude.\nThe second dataset, Dengue_Daily.csv, is an aspatial dataset that contains records of reported dengue cases in Taiwan since 1998. This data is extracted from Dengue Daily Confirmed Cases Since 1998."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-geospatial-data",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.2 Importing Geospatial Data",
    "text": "4.2 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import TAIWAN_VILLAGE_2020 dataset into R environment.\n\ntainan &lt;- st_read(\"~/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.shp\") \n\nReading layer `TAINAN_VILLAGE' from data source \n  `/Users/khantminnaing/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\n\n\nWe will verify the coordinate reference systems of the tainan object to ensure the assignment of the correct CRS value.\n\nst_crs(tainan)\n\nCoordinate Reference System:\n  User input: TWD97 \n  wkt:\nGEOGCRS[\"TWD97\",\n    DATUM[\"Taiwan Datum 1997\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.\"],\n        BBOX[17.36,114.32,26.96,123.61]],\n    ID[\"EPSG\",3824]]\n\n\nNext, we will generate a plot of the tainan object to visualise its structure.\n\ntmap_mode(\"plot\")\ntm_shape(tainan)+\n  tm_fill(col=\"white\")+\n  tm_borders(col = \"black\", lwd=0.3, alpha=0.6)+\n  tm_layout(\n    main.title = \"Villages (Tainan City)\",\n    main.title.size = 1,\n    main.title.position = \"center\",\n    legend.show = FALSE,\n     frame = FALSE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-aspatial-data",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "4.3 Importing Aspatial Data",
    "text": "4.3 Importing Aspatial Data\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\ndengue_daily &lt;- read_csv(\"~/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/aspatial/Dengue_Daily.csv\")\ndengue_daily\n\n# A tibble: 106,861 × 26\n   發病日     個案研判日 通報日     性別  年齡層 居住縣市 居住鄉鎮 居住村里\n   &lt;date&gt;     &lt;chr&gt;      &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1 1998-01-02 None       1998-01-07 男    40-44  屏東縣   屏東市   None    \n 2 1998-01-03 None       1998-01-14 男    30-34  屏東縣   東港鎮   None    \n 3 1998-01-13 None       1998-02-18 男    55-59  宜蘭縣   宜蘭市   None    \n 4 1998-01-15 None       1998-01-23 男    35-39  高雄市   苓雅區   None    \n 5 1998-01-20 None       1998-02-04 男    55-59  宜蘭縣   五結鄉   None    \n 6 1998-01-22 None       1998-02-19 男    20-24  桃園市   蘆竹區   None    \n 7 1998-01-23 None       1998-02-02 男    40-44  新北市   新店區   None    \n 8 1998-01-26 None       1998-02-19 女    65-69  台北市   北投區   None    \n 9 1998-02-11 None       1998-02-13 女    25-29  台南市   南區     None    \n10 1998-02-16 None       1998-02-24 男    20-24  高雄市   楠梓區   None    \n# ℹ 106,851 more rows\n# ℹ 18 more variables: 最小統計區 &lt;chr&gt;, 最小統計區中心點X &lt;chr&gt;,\n#   最小統計區中心點Y &lt;chr&gt;, 一級統計區 &lt;chr&gt;, 二級統計區 &lt;chr&gt;,\n#   感染縣市 &lt;chr&gt;, 感染鄉鎮 &lt;chr&gt;, 感染村里 &lt;chr&gt;, 是否境外移入 &lt;chr&gt;,\n#   感染國家 &lt;chr&gt;, 確定病例數 &lt;dbl&gt;, 居住村里代碼 &lt;chr&gt;, 感染村里代碼 &lt;chr&gt;,\n#   血清型 &lt;chr&gt;, 內政部居住縣市代碼 &lt;chr&gt;, 內政部居住鄉鎮代碼 &lt;chr&gt;,\n#   內政部感染縣市代碼 &lt;chr&gt;, 內政部感染鄉鎮代碼 &lt;chr&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#understanding-administrative-division-in-taiwan",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#understanding-administrative-division-in-taiwan",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.1 Understanding Administrative Division in Taiwan",
    "text": "5.1 Understanding Administrative Division in Taiwan\nBefore we carry out data wrangling, it is of utmost importance to contexualise ourselves with the nature of datasets. This is particularly important when conducting spatial analysis, as understanding the nature of local spatial information, such as the administrative division, is key to accurate and meaningful results.\nIn this section, we will explore administrative division of Taiwan.\n\n\n\n\n\nAccording to the Local Government Act, the local government in the ROC (Taiwan) is subdivided into provinces and special municipalities.\n\n\n\n\n\n\n5.1.1 First Level: Special municipalities, counties, and cities\nCurrently there are three types and in total 22 administrative divisions are directly governed by the central government.\n\n\n\n\n\n\n\n\n\nSpecial Municipality (直轄市) : Currently, Taiwan comprises 6 special municipalities - Taipei, New Taipei, Taoyuan, Taichung, Tainan, and Kaohsiung. [Colored in Red]\n\nProvinces are sub-divided into County (縣) & City (市) |\n\nCounty (縣): Currently, Taiwan comprises 13 counties - Hsinchu, Miaoli, Changhua, Nantou, Yunlin, Chiayi, Pingtung, Yilan, Hualien, Taitung, Penghu, Kinmen, and Lienchiang. [Colored in Green]\nCity (市) : Currently, Taiwan comprises 3 cities - Keelung, Hsinchu, and Chiayi. [Colored in Purple]\n\n\n\n\nThe 22 main divisions in the country are further divided into 368 subdivisions. These 368 divisions can be categorized as the following.\n\n\n5.1.2 Second Level: Townships, county-administered cities and districts\nThe 22 main divisions in previous level are further divided into 368 subdivisions. These 368 divisions can be categorized as the following.\n\n\n\n\n\n\n\n\n\nRural Township (鄉) : Currently, Taiwan comprises 122 rural townships.\nMountain Indigenous Township (山地鄉) : Currently, Taiwan comprises 24 mountain indigenous townships.\nUrban Township (鎮) : Currently, Taiwan comprises 38 urban townships.\nCounty-administered City (縣轄市) : Currently, Taiwan comprises 14 county-administered cities.\nMountain Indigenous District (原住民區) : Currently, Taiwan comprises 6 mountain indigenous district.\nDistrict (區): Currently, Taiwan comprises 164 districts.\n\n\n\n\n\n\n5.1.3 Lower-Level Administrative Divisions\nThe 368 divisions in previous level are further divided into villages and neighborhoods.\n\n\n\n\n\n\n\n\n\nRural/Urban Village (村里) : Currently, Taiwan comprises 7,835 rural and urban villages.\nNeighbourhood (鄰) : Currently, Taiwan comprises 147,877 neighbourhoods."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#translating-dataset-columns-of-dengue_daily-object",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#translating-dataset-columns-of-dengue_daily-object",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.2 Translating Dataset Columns of dengue_daily object",
    "text": "5.2 Translating Dataset Columns of dengue_daily object\nNotice that the data columns are currently represented in traditional Chinese. To enhance clarity in subsequent analysis, we will update the column names to English. It is an essential step in making data analysis more reproducible. In R, the colnames() function can be utilized to rename the columns. Before that, we will explicitly print out the column names.\n\ncolnames(dengue_daily)\n\n [1] \"發病日\"             \"個案研判日\"         \"通報日\"            \n [4] \"性別\"               \"年齡層\"             \"居住縣市\"          \n [7] \"居住鄉鎮\"           \"居住村里\"           \"最小統計區\"        \n[10] \"最小統計區中心點X\"  \"最小統計區中心點Y\"  \"一級統計區\"        \n[13] \"二級統計區\"         \"感染縣市\"           \"感染鄉鎮\"          \n[16] \"感染村里\"           \"是否境外移入\"       \"感染國家\"          \n[19] \"確定病例數\"         \"居住村里代碼\"       \"感染村里代碼\"      \n[22] \"血清型\"             \"內政部居住縣市代碼\" \"內政部居住鄉鎮代碼\"\n[25] \"內政部感染縣市代碼\" \"內政部感染鄉鎮代碼\"\n\n\nNext we will translate these column names into English using references from administrative divisions of Taiwan as well as the language translation of Google Translate (Chinese Traditional to English). Each literal translation is cross-referenced and cross-checked to be accurate and intuitive.\n\n\n\n\n\n\n\n\nColumn Name (Chinese)\nData Type\nColumn Name To-Be-Converted (English)\n\n\n\n\n發病日\n&lt;date&gt;\nOnset_Date\n\n\n個案研判日\n&lt;chr&gt;\nTesting_Date\n\n\n通報日\n&lt;date&gt;\nNotification_Date\n\n\n性別\n&lt;chr&gt;\nGender\n\n\n年齡層\n&lt;chr&gt;\nAge_Group\n\n\n居住縣市\n&lt;chr&gt;\nCountyCity_Residence\n\n\n居住鄉鎮\n&lt;chr&gt;\nRuralUrbanTownship_Residence\n\n\n居住村里\n&lt;chr&gt;\nRuralUrbanVillage_Residence\n\n\n最小統計區\n&lt;chr&gt;\nGrid_Area\n\n\n最小統計區中心點X\n&lt;chr&gt;\nX_Coordinate\n\n\n最小統計區中心點Y\n&lt;chr&gt;\nY_Coordinate\n\n\n一級統計區\n&lt;chr&gt;\nPrimary_Statistical_Area\n\n\n二級統計區\n&lt;chr&gt;\nSecondary_Statistical_Area\n\n\n感染縣市\n&lt;chr&gt;\nCountyCity_Exposure\n\n\n感染鄉鎮\n&lt;chr&gt;\nRuralUrbanTownship_Exposure\n\n\n感染村里\n&lt;chr&gt;\nRuralUrbanVillage_Exposure\n\n\n是否境外移入\n&lt;chr&gt;\nImported_Case\n\n\n感染國家\n&lt;chr&gt;\nCountry_Infection_Origin\n\n\n確定病例數\n&lt;num&gt;\nNo_Infected_Cases\n\n\n居住村里代碼\n&lt;chr&gt;\nCode_RuralUrbanVillage_Residence\n\n\n感染村里代碼\n&lt;chr&gt;\nCode_RuralUrbanTownship_Residence\n\n\n血清型\n&lt;chr&gt;\nSerotype\n\n\n內政部居住縣市代碼\n&lt;chr&gt;\nMOI_Code_CountyCity_Residence\n\n\n內政部居住鄉鎮代碼\n&lt;chr&gt;\nMOI_Code_RuralUrbanTownship_Residence\n\n\n內政部感染縣市代碼\n&lt;chr&gt;\nMOI_Code_CountyCity_Exposure\n\n\n內政部感染鄉鎮代碼\n&lt;chr&gt;\nMOI_Code_RuralUrbanTownship_Exposure\n\n\n\n\ncolnames(dengue_daily) &lt;- c(\"Onset_Date\",\"Testing_Date\",\"Notification_Date\",\"Gender\",\"Age_Group\",\"CountyCity_Residence\",\"RuralUrbanTownship_Residence\",\"RuralUrbanVillage_Residence\",\"Grid_Area\",\"X_Coordinate\",\"Y_Coordinate\",\"Primary_Statistical_Area\",\"Secondary_Statistical_Area\",\"CountyCity_Exposure\",\"RuralUrbanTownship_Exposure\",\"RuralUrbanVillage_Exposure\",\"Imported_Case\",\"Country_Infection_Origin\",\"No_Infected_Cases\",\"Code_RuralUrbanVillage_Residence\",\"Code_RuralUrbanTownship_Residence\",\"Serotype\",\"MOI_Code_CountyCity_Residence\",\"MOI_Code_RuralUrbanTownship_Residence\",\"MOI_Code_CountyCity_Exposure\n\",\"MOI_Code_RuralUrbanTownship_Exposure\")\n\n\nhead(dengue_daily)\n\n# A tibble: 6 × 26\n  Onset_Date Testing_Date Notification_Date Gender Age_Group\n  &lt;date&gt;     &lt;chr&gt;        &lt;date&gt;            &lt;chr&gt;  &lt;chr&gt;    \n1 1998-01-02 None         1998-01-07        男     40-44    \n2 1998-01-03 None         1998-01-14        男     30-34    \n3 1998-01-13 None         1998-02-18        男     55-59    \n4 1998-01-15 None         1998-01-23        男     35-39    \n5 1998-01-20 None         1998-02-04        男     55-59    \n6 1998-01-22 None         1998-02-19        男     20-24    \n# ℹ 21 more variables: CountyCity_Residence &lt;chr&gt;,\n#   RuralUrbanTownship_Residence &lt;chr&gt;, RuralUrbanVillage_Residence &lt;chr&gt;,\n#   Grid_Area &lt;chr&gt;, X_Coordinate &lt;chr&gt;, Y_Coordinate &lt;chr&gt;,\n#   Primary_Statistical_Area &lt;chr&gt;, Secondary_Statistical_Area &lt;chr&gt;,\n#   CountyCity_Exposure &lt;chr&gt;, RuralUrbanTownship_Exposure &lt;chr&gt;,\n#   RuralUrbanVillage_Exposure &lt;chr&gt;, Imported_Case &lt;chr&gt;,\n#   Country_Infection_Origin &lt;chr&gt;, No_Infected_Cases &lt;dbl&gt;, …\n\n\nAfter translating the column names, we can better understand what each data column is. To reduce the memory load, we can drop columns which are not relevant for this study and store only relevant columns.\n\ndengue_daily &lt;- subset(dengue_daily, select = c(Onset_Date, CountyCity_Residence, RuralUrbanTownship_Residence, RuralUrbanVillage_Residence, X_Coordinate, Y_Coordinate))\n\ndengue_daily\n\n# A tibble: 106,861 × 6\n   Onset_Date CountyCity_Residence RuralUrbanTownship_R…¹ RuralUrbanVillage_Re…²\n   &lt;date&gt;     &lt;chr&gt;                &lt;chr&gt;                  &lt;chr&gt;                 \n 1 1998-01-02 屏東縣               屏東市                 None                  \n 2 1998-01-03 屏東縣               東港鎮                 None                  \n 3 1998-01-13 宜蘭縣               宜蘭市                 None                  \n 4 1998-01-15 高雄市               苓雅區                 None                  \n 5 1998-01-20 宜蘭縣               五結鄉                 None                  \n 6 1998-01-22 桃園市               蘆竹區                 None                  \n 7 1998-01-23 新北市               新店區                 None                  \n 8 1998-01-26 台北市               北投區                 None                  \n 9 1998-02-11 台南市               南區                   None                  \n10 1998-02-16 高雄市               楠梓區                 None                  \n# ℹ 106,851 more rows\n# ℹ abbreviated names: ¹​RuralUrbanTownship_Residence,\n#   ²​RuralUrbanVillage_Residence\n# ℹ 2 more variables: X_Coordinate &lt;chr&gt;, Y_Coordinate &lt;chr&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-subsetting-to-study-area",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-subsetting-to-study-area",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.3 Data Subsetting to Study Area",
    "text": "5.3 Data Subsetting to Study Area\nFor this study, we have identified a total of 8 townships/district that we want to investigate as below.\n\n\n\nTOWNID\nTOWNCODE\nChinese\n\n\n\n\nD01\n67000320\n東區\n\n\nD02\n67000330\n南區\n\n\nD04\n67000340\n北區\n\n\nD06\n67000350\n安南區\n\n\nD07\n67000360\n安平區\n\n\nD08\n67000370\n中西區\n\n\nD32\n67000270\n仁德區\n\n\nD39\n67000310\n永康區\n\n\n\nNow, we will filter the tainan data frame to only include selected townships we have identified above. Firstly, we will create a character vector named township_list that contains the codes of selected counties. Then, we will use filter function to filter the rows of the tainan data frame where the TOWNID is in the township_list and create a new sf object called study_area_sf.\n\ntownship_list &lt;- c(\"D01\", \"D02\", \"D04\", \"D06\", \"D07\", \"D08\", \"D32\", \"D39\") \n\nstudy_area_sf &lt;- tainan %&gt;%\n  filter(TOWNID %in% township_list)\n\nstudy_area_sf\n\nSimple feature collection with 258 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\nFirst 10 features:\n      VILLCODE COUNTYNAME TOWNNAME VILLNAME       VILLENG COUNTYID COUNTYCODE\n1  67000350032     臺南市   安南區   青草里  Qingcao Vil.        D      67000\n2  67000270011     臺南市   仁德區   保安里   Bao'an Vil.        D      67000\n3  67000370005     臺南市   中西區   赤嵌里  Chihkan Vil.        D      67000\n4  67000330004     臺南市     南區   大成里  Dacheng Vil.        D      67000\n5  67000350028     臺南市   安南區   城北里 Chengbei Vil.        D      67000\n6  67000350030     臺南市   安南區   城南里 Chengnan Vil.        D      67000\n7  67000370009     臺南市   中西區   法華里    Fahua Vil.        D      67000\n8  67000350017     臺南市   安南區   海南里   Hainan Vil.        D      67000\n9  67000350049     臺南市   安南區   國安里   Guo'an Vil.        D      67000\n10 67000350018     臺南市   安南區   溪心里    Xixin Vil.        D      67000\n   TOWNID TOWNCODE NOTE                       geometry\n1     D06 67000350 &lt;NA&gt; POLYGON ((120.1176 23.08387...\n2     D32 67000270 &lt;NA&gt; POLYGON ((120.2304 22.93544...\n3     D08 67000370 &lt;NA&gt; POLYGON ((120.2012 22.99966...\n4     D02 67000330 &lt;NA&gt; POLYGON ((120.1985 22.98147...\n5     D06 67000350 &lt;NA&gt; POLYGON ((120.1292 23.06512...\n6     D06 67000350 &lt;NA&gt; POLYGON ((120.1246 23.06904...\n7     D08 67000370 &lt;NA&gt; POLYGON ((120.2094 22.98452...\n8     D06 67000350 &lt;NA&gt; POLYGON ((120.175 23.02218,...\n9     D06 67000350 &lt;NA&gt; POLYGON ((120.1866 23.02766...\n10    D06 67000350 &lt;NA&gt; POLYGON ((120.1834 23.06086...\n\n\n\nclass(study_area_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nLikewise, we will also need to subset the dengue_daily layer to only include dengue cases that are within the selected townships we have identified above. In dengue_daily layer, TOWNID information is not included. Hence we will use RuralUrbanTownship_Residence column, which represents the township name (in Chinese) to filter the data.\n\ntownship_name &lt;- c(\"東區\",\"南區\",\"北區\",\"安南區\",\"安平區\",\"中西區\",\"仁德區\",\"永康區\")\n\ndengue_tainan &lt;- dengue_daily %&gt;%\n  filter(CountyCity_Residence==\"台南市\", RuralUrbanTownship_Residence %in% township_name)\n\n\n\n\n\n\n\nTip\n\n\n\nIn the code chunk above, I have to include CountyCity_Residence==\"台南市\" (台南市 = Tainan) because there are many data entry errors observed in the dengue_daily dataset.\n\n\n\n\n\nFor example, East District (東區) is located in Tainan City and is one of the selected townships for our study. In dengue_daily dataset, there are data instances like in the snapshot above, where data ponts in other counties and cities have been mis-classified as East District (東區).\nHence, we need to make sure that the data instances we will filter our for the analysis has correct information for both CountyCity_Residence and RuralUrbanTownship_Residence columns .\n\n\nNow we will use check the number of rows (data instances) in newly created dengue_tainan object.\n\nnrow(dengue_tainan)\n\n[1] 44786"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#updating-township-names-in-datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#updating-township-names-in-datasets",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.4 Updating Township Names in Datasets",
    "text": "5.4 Updating Township Names in Datasets\nstudy_area_sf object that we created in previous section only has VILLENG which represents the name of village in English. We did not have corresponding English translations for township names, which are crucial for a comprehensive understanding of our analysis results. To address this, we have prepared a list of English translations for each Chinese township name as follows.\n\n\n\nChinese (Origin)\nEnglish (Translation)\n\n\n\n\n東區\nEast District\n\n\n南區\nSouth District\n\n\n北區\nNorth District\n\n\n安南區\nAnnan District\n\n\n安平區\nAnping District\n\n\n中西區\nWest Central District\n\n\n仁德區\nRende District\n\n\n永康區\nYongkang District\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe have obtained the English translations for each township and district name directly from Google Maps. Initially, we input the Chinese names into Google Maps. This leads us to the respective township information panel, as shown below. From this point, we directly adopt the English name for each township.\n\n\n\nBased on the English translation we have tabulated above, we will proceed to create a new data column called TOWNENG which represents the name of township in English.\n\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"東區\"] &lt;- \"East District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"南區\"] &lt;- \"South District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"北區\"] &lt;- \"North District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"安南區\"] &lt;- \"Annan District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"安平區\"] &lt;- \"Anping District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"中西區\"] &lt;- \"West Central District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"仁德區\"] &lt;- \"Rende District\"\nstudy_area_sf$TOWNENG[study_area_sf$TOWNNAME == \"永康區\"] &lt;- \"Yongkang District\"\n\nSimilarly, in dengue_tainan object that we created in previous section, the values for RuralUrbanTownship_Residence column are in Chinese. To enhance interpretability, we will replace these values with their corresponding English translations.\n\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"東區\"] &lt;- \"East District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"南區\"] &lt;- \"South District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"北區\"] &lt;- \"North District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"安南區\"] &lt;- \"Annan District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"安平區\"] &lt;- \"Anping District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"中西區\"] &lt;- \"West Central District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"仁德區\"] &lt;- \"Rende District\"\ndengue_tainan$RuralUrbanTownship_Residence[dengue_tainan$RuralUrbanTownship_Residence == \"永康區\"] &lt;- \"Yongkang District\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-study-area-of-tainan-city",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-study-area-of-tainan-city",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.5 Plotting Study Area of Tainan City",
    "text": "5.5 Plotting Study Area of Tainan City\nOnce subsetting of the study area is completed, we will now plot a choropleth map of the newly created study_area_sf object.\n\ntm_shape(study_area_sf)+\n  tm_fill(\"TOWNENG\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          title = \"Township Name\") +\n  tm_borders(col = \"white\")+\n  tm_layout(main.title = \"Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-epidemiology-week-for-dengue-cases",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-epidemiology-week-for-dengue-cases",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.6 Deriving Epidemiology Week for Dengue Cases",
    "text": "5.6 Deriving Epidemiology Week for Dengue Cases\nEpidemiology week is a standardised method of counting weeks to allow for the comparison of data year over year. Each epidemiological week begins on a Sunday and ends on a Saturday (i.e. The first epidemiological week of the year ends on the first Saturday of January).\nHowever, the current dengue_tainan object do not has epidemiology week information yet. It only has &lt;date&gt; type data under Onset_Date column. Hence, we will need to calculate and derive epidemiology week information from Onset_Date. To achieve this, we will take the following data wrangling steps:\n\nWe will add a new column year to the dengue_tainan data frame. The year() function will be used to extract the year (numerical) from the Onset_Date value.\nNext, we will adds one more new column Epidemiol_Week to the dengue_tainan data frame. We will use strftime() function to format the time of the Onset_Date column. The \"%V\" format is used to extract the week of the year as decimal number (01–53) as defined in ISO 8601. This newly created Epidemiol_Week column contains the epidemiology week required for our analysis.\n\n\ndengue_tainan &lt;- dengue_tainan %&gt;% mutate(year = year(dengue_tainan$Onset_Date))\ndengue_tainan &lt;- dengue_tainan %&gt;% mutate(Epidemiol_Week = strftime(dengue_tainan$Onset_Date, format = \"%V\"))\n\n\nclass(dengue_tainan$Epidemiol_Week)\n\n[1] \"character\"\n\n\nThe data values in Epidemiol_Week column are observed to be &lt;character&gt;. Hence, we will change these values into &lt;numeric&gt; data type using as.numeric() function.\n\ndengue_tainan$Epidemiol_Week &lt;- as.numeric(dengue_tainan$Epidemiol_Week)\n\nFor this study, we are interested to investigate the dengue fever that occurred during epidemiological weeks 31 through 50 of 2023. Next, we will proceed with filtering the dataset, Hence, we will create a new dengue fever layer called dengue_tainan_fever. This layer will include dengue fevers cases within Tainan City which occurred between epidemiological weeks 31 and 50 of 2023.\n\ndengue_tainan_fever &lt;- dengue_tainan %&gt;%\n  filter(year == \"2023\", Epidemiol_Week&gt;30, Epidemiol_Week&lt;51)\n\nPlotting histograms is a good practice in data wrangling to visualise and understand the underlying distribution as well as patterns, trends and outliers. Hence, we will plot a distribution histogram for dengue_tainan_fever using mapping techniques from ggplot package.\n\nggplot(dengue_tainan_fever, aes(x=Epidemiol_Week)) +\n  geom_histogram(bins = 20,color=\"white\") +\n  labs(x=\"Epidemiology Week\", y=\"Count\", title=\"Histogram of Epidemiology Weeks 31-50 in 2023\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#converting-dengue_tainan_fever-object-to-sf-object",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#converting-dengue_tainan_fever-object-to-sf-object",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.7 Converting dengue_tainan_fever object to sf Object",
    "text": "5.7 Converting dengue_tainan_fever object to sf Object\ndengue_tainan_fever object we created in previous section is of &lt;tibble dataframe&gt; type. Hence, we will need to convert it into a sf object to perform spatial analysis.\n\nclass(dengue_tainan_fever)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nBefore we convert to sf object, it is required to check if any NULL values exist in X_Coordinate and Y_Coordinate column values. A NULL value could mean that the location data for a particular record was not collected or is not available\n\nany(is.na(dengue_tainan_fever[,\"X_Coordinate\"]))\n\n[1] FALSE\n\nany(is.na(dengue_tainan_fever[,\"Y_Coordinate\"]))\n\n[1] FALSE\n\n\nHowever, sometimes Null values may be represented in other forms, such as “None”, “NA”, “NaN”, or even a blank space. Hence, we did a visual inspection of the dataset and found that NULL values are denoted as None as shown below.\n\n\n\n\n\nHence, we need to do manual filtering of all the data instances which has None values for X_Coordinate and Y_Coordinate. To achieve this, we will use the subset function to filter the dengue_tainan_fever data frame. We will use X_Coordinate!=\"None\" & Y_Coordinate!=\"None\" to specify that our data frame only includes the rows where both X_Coordinate and Y_Coordinate are not “None”.\n\ndengue_tainan_fever&lt;-subset(dengue_tainan_fever, X_Coordinate!=\"None\" & Y_Coordinate!=\"None\")\n\nNow that we have cleaned the dataset and removed NULL values, we will proceed to convert this to a sf object called dengue_tainan_fever.sf using column values from X_Coordinate and Y_Coordinate .\n\ndengue_tainan_fever.sf &lt;- st_as_sf(dengue_tainan_fever,\n                            coords = c(\"X_Coordinate\", \"Y_Coordinate\"))\ndengue_tainan_fever.sf\n\nSimple feature collection with 18800 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 120.091 ymin: 22.9 xmax: 120.2803 ymax: 23.08108\nCRS:           NA\n# A tibble: 18,800 × 7\n   Onset_Date CountyCity_Residence RuralUrbanTownship_R…¹ RuralUrbanVillage_Re…²\n   &lt;date&gt;     &lt;chr&gt;                &lt;chr&gt;                  &lt;chr&gt;                 \n 1 2023-07-31 台南市               Yongkang District      埔園里                \n 2 2023-07-31 台南市               East District          大智里                \n 3 2023-07-31 台南市               Yongkang District      五王里                \n 4 2023-07-31 台南市               Rende District         成功里                \n 5 2023-07-31 台南市               Yongkang District      中興里                \n 6 2023-07-31 台南市               Yongkang District      復華里                \n 7 2023-07-31 台南市               Rende District         仁德里                \n 8 2023-07-31 台南市               East District          崇善里                \n 9 2023-07-31 台南市               East District          崇學里                \n10 2023-07-31 台南市               Annan District         鳳凰里                \n# ℹ 18,790 more rows\n# ℹ abbreviated names: ¹​RuralUrbanTownship_Residence,\n#   ²​RuralUrbanVillage_Residence\n# ℹ 3 more variables: year &lt;dbl&gt;, Epidemiol_Week &lt;dbl&gt;, geometry &lt;POINT&gt;\n\n\nThe resulting dengue_tainan_fever.sf object is a simple feature with POINT geometry. It is also noted that the CRS information is absent in this sf object. Hence, we will assign the relevant CRS value for Taiwan, which is ESPG:3824.\n\ndengue_tainan_fever.sf &lt;- st_set_crs(dengue_tainan_fever.sf,3824)\n\nNow that we have assigned the correct CRS value for dengue_tainan_fever.sf, we will try to plot the points from dengue_tainan_fever.sf to ensure all the points are contained within the boundaries of study_area_sf object.\n\ntmap_mode('plot')\ntm_shape(study_area_sf)+\n  tm_fill(\"TOWNENG\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          title = \"Township Name\") +\n  tm_borders(col = \"white\")+\ntm_shape(dengue_tainan_fever.sf)+\n  tm_dots()+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculating-overall-number-of-cases-for-each-village",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculating-overall-number-of-cases-for-each-village",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.8 Calculating Overall Number of Cases for Each Village",
    "text": "5.8 Calculating Overall Number of Cases for Each Village\nIn previous sections, we have created and processed two data layers study_area_sf (which contains polygons representing each village in our study area) and dengue_tainan_fever.sf (which contains points representing recorded cases of dengue fever within our study area). Now, we will calculate the number for cases for each village. To achieve this, we will use st_intersects() and lengths() functions from the sf package.\n\nIdentify Intersecting Points: We will use the st_intersects() function to identify which points from dengue_tainan_fever.sf intersect with each village polygon in study_area_sf.\nCalculate Case Counts: Next, we will use the lengths() function to count the number of intersecting points (i.e., dengue fever cases) within each village polygon.\nRecord Results: Finally, we will record these counts in a new column, CASE_COUNT, in the study_area_sf data frame.\n\n\n(study_area_sf$CASE_COUNT &lt;- lengths(st_intersects(study_area_sf, dengue_tainan_fever.sf)))\n\n  [1]   2  19 111  29   1  10  38  44 112  65  28   2   3  11  24  20  84  24\n [19] 198  96  74  59 166 112  12   6  83  89  52  51  34 194  38 124 106 173\n [37]  27 166  97  32 140 117 139 205  66  71  78  86 113 196 123 106  27  82\n [55]  41 149  53  89  21  42  88  22  79 136 114 104  43  56  72  96  63 100\n [73] 100 163  83  84  47  13  15   7   5  21  17  78 126 157 184 137 112 191\n [91]  15  74 118 204  31 130  78  37  15  16 100  24 170  81  81  75  71  72\n[109] 132 211 167  91 250 311  11 169   3   2  74  61  79  63  55  59 242  30\n[127]  41  62   9  17 105  24  19  15  39 109 102  13  51  96  44 110   3  27\n[145]  57  61  15 150  72 351 189  59  80  78  55  57   4  67  25   5  21  73\n[163]   6  75   0  36  15   8  23  25 178  82  50  62  49  52  84  68 154  48\n[181]  89 101 118  67  86  36 125  83 102 249  54  79  26  83  74  58  63  49\n[199]  91  76  32  31  52  56  83  44  59  60  16  24  39  65   4  78  66 121\n[217]  87  70  33 130  82  14   9 163  15  37 110 145  73 120 111  97 118  34\n[235]  17  18  18   8 243  27 118  27  23 113  32  50  23  11   4  19  12  19\n[253]  18   8  12  28  75  95"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-spacetime-cubes",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-spacetime-cubes",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.9 Creating Spacetime Cubes",
    "text": "5.9 Creating Spacetime Cubes\nsfdep introduces a new s3 class called spacetime by Edzer Pebesma (2012) to represent spatio-temporal data. The spacetime class links a flat data set containing spatio-temporal information with the related geometry. As spacetime class can encapsulate both the spatial and temporal aspects of data, it is suitable for spatio-temporal analysis.\nThere are four important data required to create the spacetime object:\n\ndata: a tibble data.frame object\ngeometry: an sf object\nlocation identifier: a common column between data and geometry\ntime: a column in data that includes temporal information.\n\nBefore creating a spacetime object, we need to carry out data wrangling to prepare the requred data. The data object will ideally include the number of dengue fever cases by village and by epidemiology week. To prepare the data, we will carry out the following steps:\n\nInitialize vectors : We will initializes six empty vectors to store the epidemiology week epi_week_vector, village code village_code_vector, village name village_name_vector, township/district name town_name_vector, village geomtery village_vector, and number of cases num_cases.\nLoop over epidemiology weeks: We will create a loop ver each epidemiology week from 31 to 50. We will create a new variable epi_week to represent the epidemiology week in each iteration.\nFilter points for the current week: For each epidemiology week epi_week, we will filter the dengue_tainan_fever.sf data frame to include only the points (i.e., cases) that occurred during the current week. The filtered points will be temporarily stored in a temporary data frame called dengue_tainan_fever_epiwk.\nLoop over polygons (villages): Under each iteration of epideiology week, we will loop over each polygon (i.e., village) in the study_area_sf spatial data frame. For each polygon, we will extract the polygon itself, VILLCODE value, VILLENG values, TOWNENG values, and geometry values, and store them in temporary variables - village, village_code, village_name, town_name, village_geometry.\nFind intersecting points: We will find the points from dengue_tainan_fever_epiwk that intersect with the current polygon using st_intersects() and lengths() functions. The number of intersecting points is the number of dengue fever cases in the current village for the current week. We will store the result in a temporary variable called fever_count.\nStore the results: We will then append the values of six temporary variables each representing epidemiology week epi_week, village code village_code, village name village_name, township/district name town_name, village geometry village_geometry, and number of cases to their respective vectors fever_count into empty vectors that we initialised earlier.\nCreate a data frame: After looping over all epidemiology weeks and polygons, the script creates a data frame st_data from the six vectors.\nSet column names: Finally, we will update the column names of the st_data data frame to match with the nomenclature of study_area_sf object.\n\n\n# Initialize an empty list to store the results\n# Initialize an empty list to store the results\nepi_week_vector &lt;- c()\nvillage_code_vector &lt;- c()\nvillage_name_vector &lt;- c()\ntown_name_vector &lt;- c()\nvillage_vector &lt;- c()\nnum_cases &lt;- c()\n\n# Loop over each epidemiology week\nfor(epi_week in 31:50) {\n  # Filter the points for the current week\n  dengue_tainan_fever_epiwk &lt;- dengue_tainan_fever.sf %&gt;% \n    filter(Epidemiol_Week == epi_week)\n  \n  # Calculate the number of points in each polygon\n  for(i in 1:nrow(study_area_sf)) {\n    # Get the current polygon\n    village &lt;- study_area_sf[i, ]\n    village_code &lt;- village$VILLCODE\n    village_name &lt;- village$VILLENG\n    town_name &lt;- village$TOWNENG\n    village_geometry &lt;- village$geometry\n    # Find the points that intersect with the current polygon\n    fever_count &lt;- lengths(st_intersects(village, dengue_tainan_fever_epiwk))\n    \n    # Store the results\n    epi_week_vector &lt;- append(epi_week_vector, as.double(epi_week))\n    village_code_vector &lt;- append(village_code_vector, village_code)\n    village_name_vector &lt;- append(village_name_vector,village_name)\n    town_name_vector &lt;- append(town_name_vector,town_name)\n    village_vector &lt;- append(village_vector,village_geometry)\n    num_cases &lt;- append(num_cases, as.double(fever_count))\n  }\n}\n\n#Create a data frame\nst_data &lt;- data.frame(epi_week_vector,village_code_vector,village_name_vector,town_name_vector,num_cases, village_vector)\n\ncolnames(st_data) &lt;- c(\"Epidemiol_Week\", \"VILLCODE\",\"VILLENG\",\"TOWNENG\",\"CASE_COUNT\", \"geometry\")\n\nBefore we create spacetime cube, we will first save the dataframe into an sf object called st_data.sf using st_as_sf(). This data will capture the both geometry and case count per epidemiology week for all villages in our study and will be used later for visualisation purpose.\n\nst_data.sf &lt;- st_as_sf(st_data)\n\nAfter creating st_data.sf, we will drop the geometry of st_data object as we do not need for creating spacetime cube. To do so, we will use select() function to select only first 4 columns - Epidemiol_Week, VILLCODE, VILLENG, TOWNENG and CASE_COUNT. Then, we will sort the st_data by VILLCODE.\n\nst_data &lt;- st_data %&gt;% select(c(1:5))\nst_data &lt;- st_data[order(st_data$VILLCODE),]\n\nLastly, we will convert st_data into a tibble data.frame object for creating spacetime cube. To do so, we will use as_tibble() function.\n\nst_data &lt;- as_tibble(st_data)\n\n\n\n\n\n\n\nReflection\n\n\n\nAlthough the sfdep document mention that the data context for spacetime cube has to be a data.frame​ object, it did not work when I use a data.frame​ object to create spacetime cube and used it to create neighbour list. I ran into the error such this.\n\n\n\n\n\nI figured out that I needed to use a tibble​ object and create a new spacetime cube after many trials and errors.\n\n\nWith all the necessary data prepared, we can now create a spacetime object called dengue_tanan_spt using the spacetime function, using the following data:\n\ndata: st_data\ngeometry: study_area_sf\nlocation identifier: VILLCODE (from st_data and study_area_sf)\ntime column: Epidemiol_Week (from st_data)\n\n\ndengue_tainan_spt &lt;- spacetime(st_data, study_area_sf,\n                                     .loc_col = \"VILLCODE\",\n                                     .time_col = \"Epidemiol_Week\",\n                               active = \"data\")\n\nA spacetime object is known as a spacetime cube if every location has a value for every time index. In other words, each location is associated with a regular time-series. Spacetime cubes are particularly useful in the analysis of emerging hot spots because they allow for the comprehensive tracking of changes over both space and time.\n\n\n\n\n\nWe will use is_spacetime_cube() function to check if our newly created dengue_tainan_spt object is a spacetime cube or not.\n\nis_spacetime_cube(dengue_tainan_spt)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-village-level-dengue-distribution-maps",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-village-level-dengue-distribution-maps",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.10 Creating Village-Level Dengue Distribution Maps",
    "text": "5.10 Creating Village-Level Dengue Distribution Maps\nIn this section, we will create a village-level dengue distribution maps utilising the data we have processed and prepared for epidemiology weeks 31-50. To do so, we will employ relevant tmap functions.\n\nstudy_area_count &lt;- tm_shape(study_area_sf)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Dengue Cases\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Villege-Level Distribution of Dengue Cases \\n from Epidemiology Week 31-50 in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.8,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nstudy_area_count"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-time-series-animated-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-time-series-animated-map",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "5.11 Creating Time-Series Animated Map",
    "text": "5.11 Creating Time-Series Animated Map\nIn addition to static map we created for total cumulative case counts, we can also create a time-series animated map to visualise the temporal distribution of dengue cases, which will effectively illustrate the changes over each epidemiology week.\nFor visualization purpose, we will mutate an additional column called Epidemiol_Week_Label as follows.\n\nst_data.sf &lt;- st_data.sf %&gt;% mutate(\n  Epidemiol_Week_Label = paste(\"Epidemiology Week\", as.character(Epidemiol_Week)))\n\nNow that we have prepared the data, we will use the along = \"Epidemiol_Week_Label\" argument within the tm_facets function to generate facets for each epidemiological week. Subsequently, we will employ the tmap_animation function to create an animation from the temporal map. This animation will be saved as a GIF file, named dengue_tainan_temporal.gif.”\n\ntemporal_maps &lt;- tm_shape(st_data.sf)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\",\"#e1ecbb\", \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          style = \"pretty\", \n          title = \"Dengue Cases\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(legend.title.size = 1.8,\n            legend.text.size = 1.5)+\n  tm_facets(along = \"Epidemiol_Week_Label\", free.coords = FALSE)\n\ntmap_animation(temporal_maps, filename = \"dengue_tainan_temporal.gif\", delay = 150, width = 1000, height = 1200)\n\n\nknitr::include_graphics(\"dengue_tainan_temporal.gif\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#defining-contiguity-neighbours",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#defining-contiguity-neighbours",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.1 Defining Contiguity Neighbours",
    "text": "6.1 Defining Contiguity Neighbours\nSpatial relationships are multidirectional and multilateral. To systematically transcribe the complexity of a geographic space into a final set of data analysable by a computer, first the study zone is divided into mutually exclusive areas. Each area contains a reference point (often its centroid). Then, the spatial relationships can be specified by a neighbourhood graph connecting the areas considered to be neighbouring, or by a matrix containing the geographical coordinates of the reference points. The third step consists in coding the graph in a neighbourhood matrix or transforming the geographic coordinates into a distance matrix.\n\n\n\n\n\nThere are multiple approaches in defining spatial neighbors. Some of the most commonly used approaches include a) adjacency measure where neighbourhood graphs are constructed by materialising the links between different points which are immediately adjacent to each other; and b) distance measure where neighbours are selected based on \\(k\\)-closet points adjacent to a spatial unit.\nAnselin discussed a new approach in identifying spatial neighbours called contiguity Measure (Anselin, 2020). Neighbors based on contiguity are constructed by assuming that neighbours of a given area are other areas that share a common boundary. Operationally, we can further distinguish between a rook, a queen and a bishop criterion of contiguity as described below.\n\n\n\n\n\n\nRook criterion (Common Edge) defines neighbours by the existence of a common edge between two spatial units.\nBishop criterion (Common Vertex) defines neighbours by the existence of a common vertex between two spatial units.\nQueen criterion (Either Common Edge or Vertext) defines neighbours as spatial units sharing a common edge or a common vertex. Therefore, the number of neighbours according to the queen criterion will always be at least as large as for the rook or bishop criterion."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-neighbours",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-neighbours",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.2 Computing Contiguity Neighbours",
    "text": "6.2 Computing Contiguity Neighbours\nWe will use st_contiguity() function from sfdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. However, this function only support rook and queen’s criterion. For this study, we will use queen criteria to calculate our neighbour list.\n\ntainan_nb_q &lt;- st_contiguity(study_area_sf, queen=TRUE)\nsummary(tainan_nb_q)\n\nNeighbour list object:\nNumber of regions: 258 \nNumber of nonzero links: 1526 \nPercentage nonzero weights: 2.29253 \nAverage number of links: 5.914729 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 10 11 12 14 \n 4 17 47 49 49 41 26 14  6  3  1  1 \n4 least connected regions:\n77 117 138 238 with 2 links\n1 most connected region:\n128 with 14 links"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-row-standardised-weight-matrix",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-row-standardised-weight-matrix",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.3 Computing Row-Standardised Weight Matrix",
    "text": "6.3 Computing Row-Standardised Weight Matrix\nNext, we need to assign spatial weights to each neighboring polygon.\nst_weights() function from sfdep pacakge can be used to supplement a neighbour list with spatial weights based on the chosen coding scheme. There are as least 5 different coding scheme styles supported by this function:\n\nB is the basic binary coding\nW is row standardised (sums over all links to n)\nC is globally standardised (sums over all links to n)\nU is equal to C divided by the number of neighbours (sums over all links to unity)\nS is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. (1999) (sums over all links to n).\n\nIn this study, we will use row-standardised weight matrix (style=\"W\"). Row standardisation of a matrix ensure that the sum of the values across each row add up to 1. This is accomplished by assigning the fraction 1/(# of neighbors) to each neighboring county then summing the weighted income values. Row standardisation ensures that all weights are between 0 and 1. This facilities the interpretation of operation with the weights matrix as an averaging of neighboring values, and allows for the spatial parameter used in our analyses to be comparable between models.\n\ntainan_wm_rs &lt;- st_weights(tainan_nb_q, style=\"W\")\n\nWe will mutate the newly created neighbour list object tainan_nb_q and weight matrix tainan_wm_rs into our existing study_area_sf. The result will be a new object, which we will call wm_q.\n\nwm_q &lt;- study_area_sf %&gt;%\n  mutate(nb = tainan_nb_q,\n         wt = tainan_wm_rs,\n         .before = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.4 Global Moran’s \\(I\\)",
    "text": "6.4 Global Moran’s \\(I\\)\n\n6.4.1 Computing Global Moran’s \\(I\\)\nMoran’s \\(I\\) is the correlation coefficient for the relationship between a variable and its neighbouring values. Moran’s \\(I\\) describes how features differ from the values in the study area as a whole and quantifies how similar each region is with its neighbors and averages all these assessments. Moran’s \\(I\\) values usually range from –1 to 1. We can test spatial autocorrelation by following these hypotheses:\n\nNull Hypothesis \\(H_0:I=E[I]\\). This suggests there is no spatial autocorrelation.\nAlternative Hypothesis \\(H_1:I≠E[I]\\). This indicates the presence of spatial autocorrelation.\n\n\nmoranI &lt;- global_moran(wm_q$CASE_COUNT,\n                        wm_q$nb,\n                        wm_q$wt)\n\n\n\n6.4.2 Global Moran’s \\(I\\) test\nThe Global Moran’s I test, which can be implemented using the global_moran_test() function from the sfdep package, is a method for testing spatial autocorrelation. The primary goal of this test is to determine whether the spatial autocorrelation is positive, negative, or non-existent. The hypotheses for this test are as follows:\n\nNull Hypothesis \\(H_0:I≤E[I]\\). This suggests that there is either no spatial autocorrelation ( \\(I=E[I]\\)). or negative spatial autocorrelation ( \\(I&lt;E[I]\\)).\nAlternative Hypothesis \\(H_1:I&gt;E[I]\\). This indicates the presence of positive spatial autocorrelation.\n\nWe will set alternative = \"greater\" based on our alternative hypothesis.\n\nglobal_moran_test(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  alternative = \"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 12.865, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.468214327      -0.003891051       0.001346663 \n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can draw the following statistical inferences:\n\nThe Moran’s I statistic is 0.468214327, which is significantly different from the expectation under the null hypothesis of -0.003891051. This suggests that there is a significant spatial autocorrelation in the data.\nThe standard deviate of the Moran’s I statistic is -0.003891051. This is the variance of the Moran’s I statistic.\nThe p-value is &lt;2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the test suggests that there is significant positive spatial autocorrelation in our study area. This means that areas with similar values of CASE_COUNT are more likely to be located near each other than would be expected if the data were randomly distributed.\n\n\n\n\n6.4.3 Performing Global Moran’s \\(I\\) permutation test\nThe assumptions underlying the test are sensitive to the form of the graph of neighbour relationships and other factors, and results may be checked against those of Monte Carlo permutations. Monte Carlo randomization creates random patterns by reassigning the observed values among the areas and calculates the Moran’s \\(I\\) for each of the patterns (Moraga, 2023).\nWe will use global_moran_perm() function from sfdep package with nsim = 999 which represent 1000 Monte Carlo simulations to be carried out.\n\nset.seed(1234)\ngmoranMC &lt;- global_moran_perm(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 999)\ngmoranMC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.46821, observed rank = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can observe that the Moran’s I statistic (after 1000 permutations) is 0.46821 with a p-value of &lt; 2.2e-16, which is almost identical to our previous result using global_moran_test(). It confirms that our result is stable and statistically significant.\n\n\nWe can also create a histogram to visualise the permutation results and compare them to the expectation value under null hypotheses.\n\nhist(gmoranMC$res, main=\"Histogram of Global Moran's I Monte-Carlo Simulation Results\", xlab=\"Monte-Carlo Results\", ylab=\"Frequency\")\n\nabline(v = gmoranMC$statistic, col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "6.5 Global Geary’s \\(C\\)",
    "text": "6.5 Global Geary’s \\(C\\)\nIntroduced by Geary, Geary’s \\(c\\) statistic studies the degree of intensity of a given feature in spatial objects described with the use of a weight matrix. Similarly to Moran’s analysis, Geary’s \\(c\\) can be used to quantify the extent of spatial autocorrelation in the data.\n\n6.4.2 Global Geary’s \\(C\\) test\nThe Global Geary’s \\(C\\) test, which can be implemented using the global_c_test() function from the sfdep package.\n\nglobal_c_test(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  alternative = \"greater\")\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw \n\nGeary C statistic standard deviate = 11.317, p-value &lt; 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.495364563       1.000000000       0.001988186 \n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can draw the following statistical inferences:\n\nThe Geary’s C statistic is 0.495364563, which is significantly different from the expectation under the null hypothesis of 1. This suggests the presence of spatial autocorrelation in the data.\nThe p-value is &lt;2.2e-16, which is less than 0.05, indicating that the spatial pattern observed is very unlikely to be the result of random chance. Therefore, we reject the null hypothesis of no spatial autocorrelation.\n\nIn conclusion, the test suggests that there is significant spatial autocorrelation in our study area. This means that areas with similar values of CASE_COUNT are more likely to be spatially clustered near each other than would be expected if the data were randomly distributed.\n\n\n\n\n6.4.3 Performing Global Geary’s \\(C\\) Permutation Test\nSimilar to what we did in Moran’s \\(I\\) test, we will use global_c_perm() function from sfdep package with nsim = 999 which represent 1000 Monte Carlo simulations to be carried out.\n\nset.seed(1234)\nbperm &lt;- global_c_perm(wm_q$CASE_COUNT,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw \nnumber of simulations + 1: 1000 \n\nstatistic = 0.49536, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the output above, we can observe that the Geary’s C statistic (after 1000 permutations) is 0.4953 with a p-value of 0.01, which is comparable to our previous result using global_c_test(). It confirms that our result is stable and statistically significant.\n\n\nWe can also create a histogram to visualise the permutation results and compare them to the expectation value under null hypotheses.\n\nhist(bperm$res, main=\"Histogram of Global Geary's C Monte-Carlo Simulation Results\", xlab=\"Monte-Carlo Results\", ylab=\"Frequency\")\n\nabline(v = bperm$statistic, col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i_i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i_i",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "7.1 Local Moran’s \\(I_i\\)",
    "text": "7.1 Local Moran’s \\(I_i\\)\nWe can decompose Global Moran’s \\(I_i\\) that we calculate in previous section into a localized measure of autocorrelation, called Local Moran’s \\(I_i\\). To compute Local Moran’s \\(I_i\\), the local_moran() function of sfdep will be used.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    CASE_COUNT, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\nlisa\n\nSimple feature collection with 258 features and 26 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\n# A tibble: 258 × 27\n        ii      eii  var_ii    z_ii    p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  1.39   -0.0482  0.563    1.92   0.0552      0.02         0.01   -0.684\n 2  0.747   0.00231 0.192    1.70   0.0895      0.04         0.02   -0.547\n 3  0.0391  0.0305  0.0553   0.0363 0.971       0.92         0.46    0.492\n 4 -0.217   0.0220  0.0821  -0.836  0.403       0.34         0.17   -0.357\n 5  1.42    0.0466  0.292    2.54   0.0111      0.02         0.01   -0.478\n 6  1.29   -0.00600 0.157    3.27   0.00108     0.02         0.01   -0.217\n 7  0.319  -0.0191  0.0956   1.09   0.274       0.24         0.12   -0.431\n 8  0.0174  0.00346 0.0277   0.0840 0.933       0.96         0.48   -0.264\n 9  0.411   0.0418  0.101    1.16   0.247       0.22         0.11    0.747\n10  0.0820 -0.00329 0.00368  1.41   0.160       0.18         0.09   -0.438\n# ℹ 248 more rows\n# ℹ 19 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, VILLCODE &lt;chr&gt;, COUNTYNAME &lt;chr&gt;, TOWNNAME &lt;chr&gt;,\n#   VILLNAME &lt;chr&gt;, VILLENG &lt;chr&gt;, COUNTYID &lt;chr&gt;, COUNTYCODE &lt;chr&gt;,\n#   TOWNID &lt;chr&gt;, TOWNCODE &lt;chr&gt;, NOTE &lt;chr&gt;, geometry &lt;POLYGON [°]&gt;,\n#   TOWNENG &lt;chr&gt;, CASE_COUNT &lt;int&gt;\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations\np_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative=\np_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\n\n\n7.1.1 Visualising Local Moran’s \\(I_i\\)\nThe best approach to describe and explain Local Moran’s \\(I_i\\) values is tthrough visualization on a map. In this section, we will employ relevant tmap functions to visualize the Local Moran’s \\(I_i\\) values across the study area.\n\ntm_shape(lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe local spatial autocorrelation (Moran’s I test) results in a total of 6 different zones with varying level of Moran’s I values.\n\nBlue Zone : Villages in blue zone of the map has local Moran’s I values ranging from -1 to 0. This may suggests that these villages tend to be outliers which has dissimilar values compared to neighbouring villages.\nGreen & Yellow, Light Orange, Dark Orange and Red Zones: Villages in these zones of the map has local Moran’s I values ranging from 1 to 5. This may suggest that these villages tend to be spatial clusters which has similar values compared to neighbouring villages.\n\nLooking at the distribution histogram, the majority of the villages fall within Green Zone, followed by Blue Zone and Yellow Zone. The number of villages within Light Orange, Dark Orange and Red Zones is significantly smaller.\nOverall, the spatial autocorrelation map reveals a concentration of dengue cases in the central region of the study area, as indicated by the relatively high Local Moran’s I values. This pattern suggests that the central region of our study area is a high-risk zone for dengue. However, a comprehensive understanding of this pattern requires an analysis of the statistical significance associated with each Local Moran’s I value. Without this, our interpretation of the spatial distribution and clustering of dengue cases remains incomplete.\n\n\n\n\n7.1.2 Visualising Local Moran’s \\(I_i\\) p-value\nAs mentioned in the analysis section above, we will need to examine the Local Moran’s \\(I_i\\) value alongside their corresponding p-values. Following a similar approach, we will use relevant tmap functions to visualise p-values associated wtih Local Moran’s \\(I_i\\) values across the study area.\n\ntm_shape(lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#b7dce9\",\"#c9e3d2\",\"#f5f3a6\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n7.1.3 Visualising Statistically Significant Local Spatial Autocorrelation Map\nFrom the p-value map above, it appears that not every village exhibits a statistically significant Local Moran’s \\(I_i\\) value (i.e., p-value &lt; 0.05). Consequently, our analysis will focus solely on villages with statistically significant local Moran’s \\(I_i\\) values. To achieve this, we will filter out all local Moran’s \\(I_i\\) values with a p-value greater than 0.05. Subsequently, we will use relevant tmap functions to create a statistically significant local spatial autocorrelation map for our study area.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;% mutate(label = paste(VILLENG,TOWNENG))\n\ntm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level Spatial Autocorrelation Map \\n of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe local spatial autocorrelation (Moran’s I test) results after the exclusion of statistically significant Moran’s I values in a total of 6 different zones with varying levels of Moran’s I values.\n\nBlue Zone : Villages in blue zone of the map has local Moran’s I values ranging from -1 to 0. This may suggests that these villages tend to be outliers which has dissimilar values compared to neighbouring villages.\nGreen & Yellow, Light Orange, Dark Orange and Red Zones: Villages in these zones of the map has local Moran’s I values ranging from 1 to 5. This may suggest that these villages tend to be spatial clusters which has similar values compared to neighbouring villages.\n\nLooking at the distribution histogram, the majority of the villages fall within Green Zone, followed by Yellow Zone. The number of villages within Blue, Light Orange, Dark Orange and Red Zones is comparatively smaller.\nIn our previous analysis, we hypothesized that the central part of our study area is a high-risk zone for dengue, given the concentration of elevated Moran’s I values. However, after the exclusion of statistically significant Moran’s I values, it turns out that majority of Moran’s I values in this central region lack statistical significance. Hence, we cannot conclude that the central part of our study area is a high-risk zone, based on statistical evidence.\nContrasting Moran’s I Values Within Close Proximity?\nInterestingly, it is observed that one village falls into Red Zone (4~5) and shows highest Moran’s I values (4.9070), signifying a single, significant spatial cluster. Adjacent to this, on the right, a total of four villages seems to fall under Blue Zone (-1~0) and have negative Moran’s I values, indicating spatial outliers. Such a contrast in spatial autocorrelation within close proximity suggests the presence of localized factors influencing the transmission of dengue case. Further investigation into these specific villages may provide insights into the underlying reasons for such a contrasting result.\n\n\n\n\n7.1.4 Visualising Statistically Significant Local Spatial Autocorrelation Map for Each Township\nIn our previous section, we have prepared and conducted analysis of local spatial autocorrelation across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the spatial patterns and correlations within each individual district.\nFirstly, we will filter study_area_sf, lisa and lisa_sig objects to each districts within our study area. Following this, we will generate Local Moran’s I maps for each district. These maps will serve as the basis for our subsequent discussions and analyses of the findings.\n\n\nShow the code\nannan &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Annan District\")\nrende &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang &lt;- study_area_sf %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping&lt;- study_area_sf %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral &lt;- study_area_sf %&gt;% filter(TOWNENG == \"West Central District\")\nsouth &lt;- study_area_sf %&gt;% filter(TOWNENG == \"South District\")\neast &lt;- study_area_sf %&gt;% filter(TOWNENG == \"East District\")\nnorth &lt;- study_area_sf %&gt;% filter(TOWNENG == \"North District\")\n\n\nannan_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Annan District\")\nrende_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"South District\")\neast_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"East District\")\nnorth_lisa &lt;- lisa %&gt;% filter(TOWNENG == \"North District\")\n\nannan_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Annan District\")\nrende_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"South District\")\neast_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"East District\")\nnorth_lisa_sig &lt;- lisa_sig %&gt;% filter(TOWNENG == \"North District\")\n\n\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nannan_local &lt;- tm_shape(annan_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nannan_sig &lt;- tm_shape(annan_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(annan_local,annan_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nThe single, significant spatial cluster that we identified in previous analysis is revealed to be Xiqi Village of Annan District. Apart from this, the Annan District does not exhibit any spatial outliers, as all villages with statistically significant Local Moran’s I values display positive values. This suggests a consistent pattern of dengue cases within these villages. There is a comparable number of villages that fall within the Green (0~1), Yellow (1~2), and Light Orange (2~3) Zones, each representing varying degrees of positive spatial autocorrelation. Notably, no villages were observed in the Dark Orange Zone (3~4).\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nrende_local &lt;- tm_shape(rende_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\", \"#c9e3d2\",\"#f5f3a6\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nrende_sig &lt;- tm_shape(rende_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(rende_local,rende_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nThe Rende District displays a total of three spatial clusters - Chenggong Village, Bao’an Village, and Shanglun Village - all exhibiting Local Moran’s I values that range from 0 to 1. This range suggests a moderate level of spatial autocorrelation, indicating that the distribution of dengue cases in this district is less clustered and not as strong as in other districts.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nyongkang_local &lt;- tm_shape(yongkang_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nyongkang_sig &lt;- tm_shape(yongkang_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(yongkang_local,yongkang_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nYongkang District observes a total of 7 spatial clusters and 3 spatial outliers with varying levels of local Moran’s I values, suggesting heterogeneous distribution of dengue cases across the district. This suggests a heterogeneous distribution of dengue cases across the district. Erwang Village appears to exhibit the highest spatial autocorrelation with local Moran’s I value of 2.254. The rest of the spatial clusters have local Morna’s I values ranging between 0.5 and 2, indicating moderate to strong positive spatial autocorrelation. Three spatial ouliers observed in this district are Wangliao Village, Sanhe Village and Zhongxing Village, all exhibiting a negative local Moran’s I value between -0.5 to 1. This implies these villages have dissimilar dengue case counts compared to their neighboring villages.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nanping_local &lt;- tm_shape(anping_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nanping_sig &lt;- tm_shape(anping_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          n =5,\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(anping_local,anping_sig, asp=1, ncol=2)\n\n\n\n\n\n\nAnalysis & Discussion\nIn Anping District, no village exhibits a statistically significant Local Moran’s I value. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\nwestcentral_local &lt;- tm_shape(westcentral_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#c9e3d2\",\"#f5f3a6\",\"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nwestcentral_sig &lt;- tm_shape(westcentral_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            legend.position = c(\"LEFT\",\"TOP\"),\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(westcentral_local,westcentral_sig, asp=1, ncol=2)\n\n\n\n\n\n\nAnalysis & Discussion\nIn the West Central District, no village exhibits a statistically significant Local Moran’s I value. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nsouth_local &lt;- tm_shape(south_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#c9e3d2\",\"#f5f3a6\",\"#f8d887\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nsouth_sig &lt;- tm_shape(south_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(south_local,south_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\",\"#ec9a64\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSouth District did not observe any spatial outliers, but a total of 7 spatial clusters, all exhibiting Local Moran’s I values that range from 0 to 4. This range suggests varying degrees of spatial autocorrelation, indicating that the distribution of dengue cases in this district is not uniformly clustered. Mingliang Village appears to exhibit the highest spatial autocorrelation with local Moran’s I value of 3.550. The rest of the spatial clusters have local Moran’s I values ranging between 0 and 3, indicating small to moderate positive spatial autocorrelation.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\neast_local &lt;- tm_shape(east_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          bin = 4) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\neast_sig &lt;- tm_shape(east_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(east_local,east_sig, asp=1, ncol=2)\n\n\n\n\n\n\nLocal Moran’s I Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\",\n                      \"#f8d887\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSimilar to South District, East District did not observe any spatial outliers, but a total of 4 spatial clusters, exhibiting Local Moran’s I values that range from 0 to 2.5. Ziqiang Village exhibit the highest spatial autocorrelation with local Moran’s I value of 2.456. Still, this results is comparatively smaller than the other districts, suggesting a moderate spatial autocorrelation pattern across the district.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nnorth_local &lt;- tm_shape(north_lisa)+\n  tm_fill(\"ii\", \n          palette = c(\"#e1ecbb\",\"#f5f3a6\", \"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Local Moran's I\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level Spatial Autocorrelation \\nof Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nnorth_sig &lt;- tm_shape(north_lisa)+\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"#d21b1c\",\"#ec9a64\",\"#f5f3a6\",\"#c9e3d2\",\"#b7dce9\"),\n          n =5,\n          title = \"p-value\",\n          midpoint = 0.5) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\n of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(north_local,north_sig, asp=1, ncol=2)\n\n\n\n\n\n\nInteractive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"ii\", \n          palette = c(\"#b7dce9\",\"#e1ecbb\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nNorth District observed one spatial outlier - Huade Village with local Moran’s I value of -0.397 and one spatial cluster - Wenyuan Village with local Moran’s I value of 0.318. While both villages shows statistically significant autocorrelation effect, the degree of autocorrelation is significantly small compared to patterns observed in other districts."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-classification",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-classification",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "7.2 LISA Classification",
    "text": "7.2 LISA Classification\nThe local indicator of spatial association (LISA) for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation. In general, the analysis will calculate a local statistic value, a z-score, a pseudo p-value, and a code representing the cluster type for each statistically significant feature. LISA map is a categorical map showing type of outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters.\nSpecific to our study, we may infer LISA classifications as below.\n\nHigh-Low Outliers: Villages with a high value of dengue cases, surrounded by neighbouring villages with low values of dengue cases.\nLow-High Outliers: Villages with a low value of dengue cases, surrounded by neighbouring villages with high values of dengue cases.\nHigh-High Clusters: Villages with a high value of dengue cases, surrounded by neighbouring villages with high values of dengue cases.\nLow-Low Clusters: Villages with a low value of dengue cases, surrounded by neighbouring villages with low values of dengue cases.\n\n\n7.2.1 Visualising Statistically Significant LISA Map for Study Area\nIn lisa sf data.frame we created when calculating local Moran’s \\(I_i\\) , we can find three fields contain the LISA categories. They are mean, median and pysal. We will use mean column to visualise LISA classification maps with relevant tmap functions.\n\ntmap_mode(\"plot\")\nstudy_area_lisa &lt;- tm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#e1ecbb\", \"#d21b1c\"),\n          title = \"LISA class\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Village-Level LISA Map of Dengue Cases in Study Area (Tinan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nstudy_area_lisa\n\n\n\n\n\ntmap_mode(\"view\")\ntm_shape(lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#e1ecbb\", \"#d21b1c\"),\n          title = \"LISA class\",\n          midpoint = NA,\n          id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nLISA Classification map for the study area has revealed a total of three LISA classes, each representing different spatial autocorrelation patterns. Low-Low Class has the highest number of villages, followed by High-High Class. Low-High Class has relatively smaller number of villages.\n\nLow-Low Class: Villages classified into the Low-Low class are primarily situated in the upper and lower periphery of the study area, suggesting a clustering of low dengue cases. Interestingly, there is a single Low-Low class village observed in the central part of the study area, which could be an exception or an indication of a potential transition zone.\nHigh-Low Class: The absence of any High-Low classification suggests that there are no high-value villages surrounded by low-value villages.\nHigh-High Class: Villages classified into the High-High class are scattered across the central part of the study area. These villages exhibit similar high values of dengue cases, and are hence “dengue hotspots”.\nLow-High Class: All villages classified into the Low-High class are located in the central part of the study area. Interestingly, these villages tend to be in close proximity to villages in the High-High class. These are the villages which has reportedly lower values depsite their neighbouring villages being dengue hotspots. Investigation into these villages may allow for more concrete insights on the underlying causes leading to such spatial outlier - either due to effective vector control measures, or increased immunity of the population living in these villages.\n\n\n\n\n\n7.2.2 Visualising Statistically Significant LISA Map for Each Township\nIn our previous section, we have prepared and conducted analysis of LISA classfication across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the spatial patterns and correlations within each individual district.\nSince we already have filtered study_area_sf, lisa and lisa_sig objects to each districts in previous section, we will proceed with visualisations and analysis dicsussion.\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\nannan_dengue_map &lt;- tm_shape(annan)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nannan_sig_map &lt;- \ntm_shape(annan_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(annan_dengue_map,annan_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nThe Annan District presents an intriguing spatial autocorrelation pattern characterized by the co-existence of Low-Low and High-High clusters. This suggests a diverse distribution of dengue cases across the district.\nThe Northern part of the Annan District predominantly exhibits Low-Low clusters. On the other hand, the Southern part of the Annan District, bordering the North District, displays High-High clusters.\nThis may suggest that villages in northern part of Annan District might serve as dengue hotspots and potentially transmit the disease to the villages in Northern part of Annan District which currently have low dengue cases. The movement of people between these villages could potentially facilitate the spread of the disease.\n\n\n\n\n\nShow the code\nrende_dengue_map &lt;- tm_shape(rende)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nrende_sig_map &lt;- \ntm_shape(rende_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Rende District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(rende_dengue_map,rende_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nRende District only observe Low-Low class villages. This result may suggest that while dengue cases are detected in the Rende District, the outbreak is well under control, limiting the rise of any potential High-Low or High-High class villages. This could be due to effective public health interventions, or other underlying factors that limit the breeding grounds for mosquitoes or transmission of the disease.\n\n\n\n\n\nShow the code\nyongkang_dengue_map &lt;- tm_shape(yongkang)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nyongkang_sig_map &lt;- \ntm_shape(yongkang_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(yongkang_dengue_map,yongkang_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nYongkang District observe a mixture of Low-Low clusters, Low-High outliers and High-High clusters. Interestingly, all Low-Low class villages are located in the northeast part of Yongkang District. This suggests that these areas have been successful in controlling the spread of dengue.\nLow-High and High-High class are located in close proximity in the western part of Yongkang District. Based on these findings, three Low-High class villages observed has high risk of transforming into High-High class. This is because they are currently low-value areas surrounded by high-value areas, and the continued spread of dengue could potentially increase their case counts. Given the risk of transformation, these Low-High class villages require proper vector control measures to prevent an increase in dengue cases.\n\n\n\n\nAnalysis & Discussion\nNo statistically significant LISA classes have been detected in the Anping District. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\nAnalysis & Discussion\nNo statistically significant LISA classes have been detected in the West Central District. This suggests an absence of discernible spatial autocorrelation patterns in the distribution of dengue cases within this district.\n\n\n\n\n\nShow the code\nsouth_dengue_map &lt;- tm_shape(south)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nsouth_sig_map &lt;- \ntm_shape(south_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(south_dengue_map, south_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nSouth District observes the co-existence of both High-High class villages and Low-Low class villages. Majority of the Low-Low class villages tend to located in the southern part of South District and all High-High class villages are in the northen part. This indicates a significant hotspot of dengue cases in the northern part of the district, while the southern part have been successful in controlling the spread of dengue.\nInterestingly, Xinsheng Village, which is currently a Low-Low class village, lies in close proximity to the High-High class villages. This suggests that while Xinsheng Village currently has a low number of dengue cases, it is at risk due to its location near high-risk areas. As a result, intervention efforts should indeed be maintained and strengthened to control the spread of dengue cases to this village.\n\n\n\n\n\nShow the code\neast_dengue_map &lt;- tm_shape(east)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\neast_sig_map &lt;- \ntm_shape(east_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(east_dengue_map,east_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nEast District observed a total of 4 High-High cluster villages in the southeastern part of the district, bordering Rende District. This suggests a potential risk of dengue spread to neighboring districts like Rende through movement of people or mosquitoes. Given the risk, intervention efforts should indeed be maintained and strengthened in East District.\n\n\n\n\n\nShow the code\nnorth_dengue_map &lt;- tm_shape(north)+\n  tm_fill(\"CASE_COUNT\", \n          palette = c(\"#f3f3f3\",\"#b7dce9\", \"#c9e3d2\",\"#e1ecbb\",\"#f5f3a6\", \"#f8d887\",\"#ec9a64\",\"#de573e\",\"#d21b1c\"),\n          title = \"Case Count\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Distribution of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\nnorth_sig_map &lt;- \ntm_shape(north_lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Village-Level LISA Map\\n of Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.3,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1,\n            legend.text.size = 0.8,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\ntmap_arrange(north_dengue_map,north_sig_map, asp=1, ncol=2)\n\n\n\n\n\n\nLISA Interactive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_lisa)+\n  tm_polygons(id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_lisa_sig)+\n  tm_fill(\"mean\", \n          palette = c(\"#b7dce9\",\"#ec9a64\",\"#f5f3a6\", \"#d21b1c\"),\n          title = \"LISA Class\",\n          midpoint = NA,\n          id=\"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nNorth District observed one High-High cluster and one Low-High outlier right next to each other. The observed High-High cluster is Wenyuan Village, and it is neighbouring to other High-High clusters in Annan District. This spatial pattern suggests a potential spread of dengue cases from Annan to North District.\nThe observed Low-High class outlier, Huade Village is also at risk of becoming a dengue hotspot due to its close proximity to numerous High-High clusters. Intervention efforts may be strengthened or inter-village movements may be temporarily limited to prevent the village from becoming a hotspot."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-hot-spot-and-cold-spot-area-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-hot-spot-and-cold-spot-area-analysis",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.1 Local Getis-Ord \\(G_i^*\\) for Hot Spot and Cold Spot Area Analysis",
    "text": "8.1 Local Getis-Ord \\(G_i^*\\) for Hot Spot and Cold Spot Area Analysis\nLocal Getis-Ord \\(G_i\\) and \\(G_i^∗\\) are one of the earliest LISAs. The Gi and Gi* measures are typically reported as a z-score where high values indicate a high-high cluster, and negative z-scores indicate a low-low cluster. There are no high-low and low-high classifications like the local Moran.\n\\(G_i\\) statistic consist of a ratio of the weighted average of the values in the neighbouring locations, to the sum of all values, not including the value at location (\\(x_i\\)). The \\(G_i^∗\\) statistic includes the focal (or self, or \\(i^{th}\\)) observation in the neighbourhood. Spatial weights used in calculating \\(G_i\\) and \\(G_i^*\\) statistics used a distance-based approach - i.e., spatial weights identify locations of statistically significant hot spots and cold spots that are in proximity to one another based on a calculated distance.\nIn this study, we will To compute local \\(G_i\\) statistic in R, we will use st_contiguity() to create a neighbour list, then include_self() to include the focal observation in the neighbour list. Then, we use the neighbour list to create a weight list using st_inverse_distance() function.\n\nwm_idw &lt;- study_area_sf %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nNext, we will calculate local \\(G_i^∗\\) using local_gstart_perm() function from sfdep package. This function uses a neighbour list nb and a weight list wt as an input and generate \\(G_i^∗\\) statistics through a Monte Carlo permutation with specified nsim. The results will then be stored into a new object called HCSA.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    CASE_COUNT, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\nNext, we will use relevant tmap functions to visualise the result of local \\(G_i^*\\) values for our study area. For visualisation purpose, we will create a new column label similar to what we did in Local Moran’s I.\n\nHCSA &lt;- HCSA%&gt;%\n  mutate(label = paste(VILLENG,TOWNENG))\n\n\ntmap_mode(\"plot\")  \ntm_shape(HCSA)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \" Hotspots & Coldspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\nSimilar to what we have done for LISA, we will only focus on villages with statistically significant Local Getis-Ord \\(G_i^∗\\) values. To achieve this, we will filter out all Local Getis-Ord \\(G_i^∗\\) values with a p-value &gt; 0.05. Subsequently, we will use relevant tmap functions to create a statistically significant local spatial autocorrelation map for our study area.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(HCSA) +\n  tm_polygons(id=\"label\") +\ntm_shape(HCSA_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\", \"#7977f3\",\"#f8d673\",\"#f8b675\",\"#f67774\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          id=\"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nThe hotspot coldspot map after the exclusion of statistically significant Local Gi* values result in different coldspot and hotpots with varying levels of Local Gi* values. The interpretation of the Local Gi* statistics is very straightforward - a positive value suggests a High-High cluster or hot spot and a negative value indicates a Low-Low cluster or cold spot.\nA visual inspection suggests that the majority of the coldspots are situated on the periphery of the study areas, while the hotspots tend to be concentrated in the centre. This observation aligns with the findings from the LISA maps.\n\n\n\n\n\n\n\nNext, we will retrieve three villages with highest local \\(G_i^∗\\) values and lowest local \\(G_i^∗\\) values respectively. These villages will serve as focus areas for Mann-Kendall Trend Test later.\n\nset.seed(123)\n\nthree_hotspots &lt;- (head((HCSA_sig[HCSA_sig$gi_star &gt; 4,]), 3)$label)\nthree_coldspots &lt;-  (head((HCSA_sig[HCSA_sig$gi_star &gt; -2,]), 3)$label)\n\nthree_hotspots\n\n[1] \"Haidian Vil. Annan District\" \"Xiqi Vil. Annan District\"   \n[3] \"Da'an Vil. Annan District\"  \n\nthree_coldspots\n\n[1] \"Qingcao Vil. Annan District\" \"Guo'an Vil. Annan District\" \n[3] \"Xuedong Vil. Annan District\"\n\n\nLet’s proceed to visualise the three most significant hotspots and the three most significant coldspots that we have identified, by plotting them on the map using the appropriate tmap functions.\n\n\nShow the code\nHCSA_three_hotspots &lt;- HCSA_sig %&gt;% filter(label %in% three_hotspots)\nHCSA_three_coldspots &lt;- HCSA_sig %&gt;% filter(label %in% three_coldspots)\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_three_hotspots)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#f67774\")) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_text(\"label\",auto.placement = T)+\n  tm_layout(main.title = \"Three Most Significant Hotspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\nShow the code\ntm_shape(HCSA) +\n  tm_polygons() +\ntm_shape(HCSA_three_coldspots)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#57bfc0\")) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_text(\"label\", auto.placement = T)+\n  tm_layout(main.title = \"Three Most Significant Coldspots \\nof Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nBased on the output above, we can draw the following insights\nHighest Local Gi* Hotspots: Haidian Village (Annan District), Xiqi Village (Annan District) and Da’an Village (Annan District) are three villages that exhibit highest local Gi* values and are hence most significant hotspots. Interestingly, all villages are from Annan District. This concentration of hotspots in Annan District suggests a localized outbreak of dengue cases, which warrants immediate attention and intervention.\nLowest Local Gi* Coldspots: Qingcao Village (Annan District), Guo’an Village (Annan District) and Xuedong Village (Anan District) are three villages that exhibit lowest local Gi* values and hence are most significant coldspots. Surprisingly, all villages are from Annan District, similar to hosspots. This concentration of coldspots in Annan District present a stark contrast from what we observed in Hotspots. In subsequent analysis, we will try to investigate in this matter."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-each-epidemiology-week",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-getis-ord-g_i-for-each-epidemiology-week",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.2 Local Getis-Ord \\(G_i^*\\) for Each Epidemiology Week",
    "text": "8.2 Local Getis-Ord \\(G_i^*\\) for Each Epidemiology Week\nIn previous section, we calculate local \\(G_i^*\\) values using the overall count of cases for each village. In this section, we are interested to analyse the dynamics of dengue case count over each epidemiology week from 31 to 50. To do so, we will have to calculate the local \\(G_i^*\\) that accounts for both spatial and temporal aspects. To do so, we will make use of the spacetime cube dengue_tainan_spt that we created.\n\ndengue_nb &lt;- dengue_tainan_spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n    set_nbs(\"nb\") %&gt;%\n    set_wts(\"wt\")\n\n\nhead(dengue_nb)\n\n# A tibble: 6 × 7\n  Epidemiol_Week VILLCODE    VILLENG       TOWNENG        CASE_COUNT nb    wt   \n           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;\n1             31 67000350032 Qingcao Vil.  Annan District          0 &lt;int&gt; &lt;dbl&gt;\n2             31 67000270011 Bao'an Vil.   Rende District          1 &lt;int&gt; &lt;dbl&gt;\n3             31 67000370005 Chihkan Vil.  West Central …          0 &lt;int&gt; &lt;dbl&gt;\n4             31 67000330004 Dacheng Vil.  South District          0 &lt;int&gt; &lt;dbl&gt;\n5             31 67000350028 Chengbei Vil. Annan District          0 &lt;int&gt; &lt;dbl&gt;\n6             31 67000350030 Chengnan Vil. Annan District          0 &lt;int&gt; &lt;dbl&gt;\n\n\nSince we are interested in understanding the spatio-temporal dynamics of dengue cases over each epidemiology week, we will groups the data by the Epidemiol_Week variable using group_by() function. By doing so, subsequent calculation of gi_star will be performed separately for each epidemiological week. As a result, local_gstart_perm() function will return a list-column fo each epidemiology week, where each element is gi_star value for each village in the particular epidemiology week. Hence, we will use unnest() function to convert list-column to a regular column.\n\ngi_stars_epiweek &lt;- dengue_nb %&gt;%\n  group_by(Epidemiol_Week) %&gt;%\n  mutate(gi_star = local_gstar_perm(CASE_COUNT, nb, wt)) %&gt;%\n  unnest(gi_star)\n\nhead(gi_stars_epiweek)\n\n# A tibble: 6 × 15\n# Groups:   Epidemiol_Week [1]\n  Epidemiol_Week VILLCODE VILLENG TOWNENG CASE_COUNT nb    wt    gi_star    e_gi\n           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1             31 6700035… Qingca… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -0.790 0.00243\n2             31 6700027… Bao'an… Rende …          1 &lt;int&gt; &lt;dbl&gt;   0.128 0.00395\n3             31 6700037… Chihka… West C…          0 &lt;int&gt; &lt;dbl&gt;  -0.530 0.00333\n4             31 6700033… Dachen… South …          0 &lt;int&gt; &lt;dbl&gt;  -0.339 0.00304\n5             31 6700035… Chengb… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -0.867 0.00277\n6             31 6700035… Chengn… Annan …          0 &lt;int&gt; &lt;dbl&gt;  -1.07  0.00313\n# ℹ 6 more variables: var_gi &lt;dbl&gt;, p_value &lt;dbl&gt;, p_sim &lt;dbl&gt;,\n#   p_folded_sim &lt;dbl&gt;, skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test-for-trends",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test-for-trends",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.3 Mann-Kendall Test for Trends",
    "text": "8.3 Mann-Kendall Test for Trends\nNext step of our analysis will be Mann-Kendall Trend Test. The Mann-Kendall statistical test for trend is used to assess whether a set of data values is increasing or decreasing over time, and whether the trend in either direction is statistically significant. It is a non-parametric test and compares the relative magnitudes of sample data rather than the data values themselves (Gilbert, 1987).\nMann-Kendall trend test genereate two values - Kendall’s Tau tau and Kendall Score S.\nKendall’s Tau (τ) is a correlation coefficient and is a measure of the relationship between two variables. The Tau correlation coefficient returns a value of 0 to 1. Kendall’s Tau can be calculate by the formula \\((C-D/C+D)\\) where C is the number of concordant pairs and D is the number of discordant pairs. Kendall’s Tau is used to test the following hypotheses.\n\nNull Hypothesis: the correlation coefficient τ = 0 (There is no correlation.)\nAlternative Hypothesis: the correlation coefficient τ ≠ 0 (There is a correlation.)\n\nKhambhammettu (2005) explained the methodology of Kendall Score S as follows. The initial value of the Mann-Kendall statistic, S, is assumed to be 0 (e.g., no trend). If a later data point is higher than an earlier one, S is incremented. Conversely, if a later data point is lower, S is decremented. The final value of S indicates the overall trend. A strongly positive value of S suggests an upward trend, while a deeply negative value indicates a downward trend. To statistically assess the significance of the trend, the associated p-value is calculated.\nTo implement Mann-Kendall trend testing in R, MannKendall() function from Kendall package can be used. In this section, we will run Mann-Kendall Test for three most significant hotspots and three most significant coldspots that we identified in our HCSA analysis.\n\n8.3.1 Trend Test of Three Most Significant Hotspots\nWe will first filter out all \\(G_i^*\\) values for each village.\n\nHaidian Village (Annan District)Xiqi Village (Annan District)Da’an Village (Annan District)\n\n\n\ncbg_hd &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Haidian Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_xq &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Xiqi Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_da &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Da'an Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\nWe are now set to visualize the trend of the \\(G_i^*\\) values of three most significant hotspots across epidemiological weeks 31 to 50. To achieve this, we will use relevant ggplot2 functions to create an interactive plot.\n\np_hotspots &lt;- ggplot() +\n  geom_line(data = cbg_hd, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Haidian Village\")) +\n  geom_line(data = cbg_xq, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Xiqi Village\")) + \n  geom_line(data = cbg_da, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Da'an Village\")) +\n  labs(x = \"Epidemiology Week\", y = \"Gi* Value\", \n       title = \"Gi* of Three Most Significant Hotspots Over Epidemiology Week 31-50\",\n       color = \"Village\")\n\nplotly::ggplotly(p_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nIt is interesting to see that all three villages have very similar trend pattern. In fact, these three villages are neighbouring villages and it appears that they all experience almost same outbreak pattern. The Gi* values of all three villages peaked at Week 34 before declining dramatically by Week 39. Afterwards, they all experience wild fluctuations in Gi* values and in fact end up as coldspots by the end of Week 50!! These villages were identified as significant hotspots due to the cumulative count of cases. However, this approach overlooks the temporal nuances and changes in disease clustering over time. Unless we conduct Mann-Kendall trend test, we would not realise that they end up as coldspots by the end.\n\n\nWe will now calculate the Kendall’s tau and Kendall score S for each village using MannKendall() function from Kendall package.\n\nHaidian Village (Annan District)Xiqi Village (Annan District)Da’an Village (Annan District)\n\n\n\ncbg_hd %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.579 0.000406  -110  190.   950\n\n\n\n\n\ncbg_xq %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau        sl     S     D  varS\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.642 0.0000865  -122  190.   950\n\n\n\n\n\ncbg_da %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.568 0.000517  -108  190.   950\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFor all three villages, the Kendall’s tau values are negative, indicating a negative association over time. This suggests that as time progresses, the spatial clustering of disease cases tends to decrease in these villages. The negative S scores for all villages further support this decreasing trend. The p-values sl for all three villages are less than 0.05, indicating that these results are statistically significant.\n\n\n\n\n8.3.2 Trend Test of Three Most Significant Coldspots\nWe will first filter out all \\(G_i^*\\) values for each village.\n\nQingcao Village (Annan District)Guo’an Village (Annan District)Xuedong Village (Annan District)\n\n\n\ncbg_qc &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Qingcao Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_ga &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Guo'an Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\ncbg_xd &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Xuedong Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\n\n\nWe are now set to visualize the trend of the \\(G_i^*\\) values of three most significant coldspots across epidemiological weeks 31 to 50. To achieve this, we will use relevant ggplot2 functions to create an interactive plot.\n\np_coldspots &lt;- ggplot() +\n  geom_line(data = cbg_qc, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Qingcao Village\")) +\n  geom_line(data = cbg_ga, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Guo'an Village\")) + \n  geom_line(data = cbg_xd, mapping = aes(x = Epidemiol_Week, y = gi_star, color = \"Xuedong Village\")) +\n  labs(x = \"Epidemiology Week\", y = \"Gi* Value\", \n       title = \"Gi* of Three Most Significant Coldspots Over Epidemiology Week 31-50\",\n       color = \"Village\")\n\nplotly::ggplotly(p_coldspots)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFrom the plot, it appears that Xuedong Village and Qingcao Village shares very similar trend, likely due to their spatial proximity. Both remained as coldspots throughout the entirety of epidemiology weeks 31-50. What is interesting was Guo’an Village, which initially started at Gi* value of as low as 0.6 at Week 31 and suddenly reached a peak of around 6.5 at week 34, indicating a sudden outbreak. However, this was followed by a rapid decline, falling below 0 by Week 40 and remaining as a coldspot until the end of the period.\n\n\nWe will now calculate the Kendall’s tau and Kendall score S for each village using MannKendall() function from Kendall package.\n\nQingcao Village (Annan District)Guo’an Village (Annan District)Xuedong Village (Annan District)\n\n\n\ncbg_qc %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau    sl     S     D  varS\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.105 0.538   -20  190.   950\n\n\n\n\n\ncbg_ga %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau      sl     S     D  varS\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.526 0.00132  -100  190.   950\n\n\n\n\n\ncbg_xd %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n      tau    sl     S     D  varS\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0105 0.974    -2  190.   950\n\n\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\n\n\n\nFor all three villages, the Kendall’s tau values are negative, indicating a negative association over time. This suggests that as time progresses, the spatial clustering of disease cases tends to decrease in these villages.\nHowever, for Qingcao Village, the p-value sl is 0.538, which is greater than 0.05, indicating that this result is not statistically significant. Similary, Xuedong Village has the p-value is 0.974, which is much greater than 0.05, indicating that this result is not statistically significant. This means that the observed trend in these two villages could be due to random chance.\nGuo’an Village shows a strong negative association over time, with a tau value of -0.526 and an S score of -100. The p-value is 0.00132, which is less than 0.05, indicating that this result is statistically significant. This suggests a significant decrease in disease spread over time in this village.\n\n\nWe can replicate the operation of MannKendall test for each location by using group_by() function of dplyr package. The results of the MannKendall test will then be stored in a new object called trend_combined.\n\ntrend_combined &lt;- gi_stars_epiweek %&gt;%\n  group_by(VILLENG) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\ntrend_combined\n\n# A tibble: 249 × 6\n   VILLENG         tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Andong Vil.  0.432  0.00859      82  190.   950\n 2 Anfu Vil.   -0.611  0.000191   -116  190.   950\n 3 Anhe Vil.    0.579  0.000406    110  190.   950\n 4 Ankang Vil. -0.0316 0.871        -6  190.   950\n 5 Anqing Vil.  0.337  0.0410       64  190.   950\n 6 Anshun Vil.  0.0526 0.770        10  190.   950\n 7 Anxi Vil.    0.137  0.417        26  190.   950\n 8 Bao'an Vil.  0.0316 0.871         6  190.   950\n 9 Beihua Vil.  0.674  0.0000378   128  190.   950\n10 Beimen Vil.  0.0316 0.871         6  190.   950\n# ℹ 239 more rows\n\n\nAs the next step, we will remove the results that are not statistically significant. To do so, we will filter the data frame to include only those rows where the sl value is less than 0.05.\n\ntrend_combined_sig &lt;- trend_combined %&gt;% filter(sl &lt; 0.05)\ntrend_combined_sig\n\n# A tibble: 90 × 6\n   VILLENG            tau        sl     S     D  varS\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Andong Vil.      0.432 0.00859      82  190.   950\n 2 Anfu Vil.       -0.611 0.000191   -116  190.   950\n 3 Anhe Vil.        0.579 0.000406    110  190.   950\n 4 Anqing Vil.      0.337 0.0410       64  190.   950\n 5 Beihua Vil.      0.674 0.0000378   128  190.   950\n 6 Chengda Vil.     0.474 0.00388      90  190.   950\n 7 Chengde Vil.     0.505 0.00205      96  190.   950\n 8 Chenghuang Vil.  0.516 0.00165      98  190.   950\n 9 Chihkan Vil.     0.653 0.0000659   124  190.   950\n10 Chongcheng Vil. -0.526 0.00132    -100  190.   950\n# ℹ 80 more rows\n\n\nNext, we will identify the top 5 villages with the most significant emerging trends in \\(G_i^*\\) values, either upward or downward.\n\nemerging_vill &lt;- trend_combined_sig %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)\nemerging_vill\n\n# A tibble: 5 × 6\n  VILLENG          tau         sl     S     D  varS\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Fuqian Vil.    0.768 0.00000250   146  190.   950\n2 Chongxin Vil. -0.737 0.00000649  -140  190.   950\n3 Wuwang Vil.   -0.716 0.0000119   -136  190.   950\n4 Chongde Vil.  -0.695 0.0000214   -132  190.   950\n5 Chongxue Vil. -0.695 0.0000214   -132  190.   950\n\n\n\nFuqian Village (West Central District)Chongxin Village (East District)Wuwang Village (Yongkang District)Chongde Village (East District)Chongxue Village (East District)\n\n\n\ncbg_fuqian &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Fuqian Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_fuqian &lt;- ggplot() +\n  geom_line(data = cbg_fuqian, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_fuqian)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Fuqian Village exhibit noticeable fluctuations across the epidemiological weeks The Gi* values tends to fall below zero, indicating a coldspot during the early weeks until Epidemiology Week 41 when it first reaches a positive value. A sharp peak is observed at Week 49 where Gi* value soared beyond 3. This suggests a strong clustering of high values (hotspot) during this week. It could indicate a sudden outbreak or spread of dengue cases in certain areas. However, by the conclusion of Week 50, the Gi∗ value recedes to approximately 1.4.\n\n\n\n\ncbg_chongxin &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongxin Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongxin &lt;- ggplot() +\n  geom_line(data = cbg_chongxin, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongxin)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongxin Village exhibit fluctuations across the epidemiological weeks. There’s an initial round of increase and decrease in local Gi* values between Epidemiology Week 30 to 37, but the values relatively stable between 3 to 5. Starting from Week 37, the trend heads downward gradually, reaching the dip at Week 48 with local Gi* value of -0.68. This could suggest that the village has effectively managed to control the outbreak, leading to a gradual decline in case count and ultimately transforming into a cold spot.\n\n\n\n\ncbg_wuwang &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Wuwang Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_wuwang &lt;- ggplot() +\n  geom_line(data = cbg_wuwang, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_wuwang)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Wuwang Village starts at a whopping 10.32 at Epidemiology Week 31, signifying an extremely significant hotspot. However, the local Gi* values drastically drop over the subsequent weeks and reach at 1.64 by Week 35. From this point forward, the values oscillate between 0 and 2 without any further increase. This could suggest that the village has effectively managed to control the outbreak and prevent any potential second outbreak.\n\n\n\n\ncbg_chongde &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongde Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongde &lt;- ggplot() +\n  geom_line(data = cbg_chongde, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongde)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongde Village exhibit the peak at 6.55 at Epidemiology Week 32, signifying an significant hotspot. After week 32, there is a decline in the local Gi* values, followed by subsequent smaller peaks and troughs, indicating fluctuations in the values over the weeks. It eventually goes below 0 at Week 46. The village experience a sudden surge at Week 49 to 1.5, potentially signaling the onset of a second outbreak. Despite this, the value ultimately falls back to -1 by Week 50. This could suggest that the village was successful in averting the second outbreak.\n\n\n\n\ncbg_chongxue &lt;- gi_stars_epiweek%&gt;%\n  ungroup() %&gt;%\n  filter(VILLENG == \"Chongxue Vil.\") |&gt;\n  select(VILLENG, Epidemiol_Week, gi_star)\n\n\np_chongxue &lt;- ggplot() +\n  geom_line(data = cbg_chongxue, mapping = aes(x = Epidemiol_Week, y = gi_star))\n\nplotly::ggplotly(p_chongxue)\n\n\n\n\n\n\nAnalysis & Discussion\nThe local Gi* values in Chongxue Village exhibit very similar pattern as Chongde Village. It peaks at 6.37 at Epidemiology Week 32, signifying an significant hotspot. After week 32, there is a decline in the local Gi* values, followed by subsequent smaller peaks and troughs, indicating fluctuations in the values over the weeks. The value eventually goes below 0 at Week 46. The village experience a sudden surge at Week 49 to 1.8, potentially signaling the onset of a second outbreak. Despite this, the value ultimately falls back to -1.1 by Week 50. This could suggest that the village was successful in averting the second outbreak."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hot-spot-analysis-ehsa",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "8.4 Performing Emerging Hot Spot Analysis (EHSA)",
    "text": "8.4 Performing Emerging Hot Spot Analysis (EHSA)\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object dengue_tainan_spt, and the name of the variable of interest CASE_COUNT for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = dengue_tainan_spt, \n  .var = \"CASE_COUNT\", \n  k = 1, \n  nsim = 99\n)\n\ntainan_ehsa &lt;- study_area_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(VILLCODE == location)) %&gt;%\n  mutate(label = paste(VILLENG,TOWNENG))\n\n\n8.4.1 Visualizing Distribution of EHSA Classes\nBefore creating EHSA maps, we can start by plotting a bar chart to reveal different EHSA classes that have been identified and number of villages in each classification. To do so, we will plot a histogram using ggplot2 functions.\n\nggplot(data = ehsa,\n       aes(y = classification,fill = classification)) +\n  geom_bar(show.legend = FALSE)\n\n\n\n\nBased on the figure above, the EHSA analysis has identified a total of 8 distinct hotspot and coldspot classes. The identified hotspot classes include consecutive hotspots, new hotspots, oscillating hotspots, and sporadic hotspots. The identified coldspot classes include consecutive coldspots, oscillating coldspots, and sporadic coldspots. It is interesting to see that oscillating hotspots and oscillating coldspots constitute the highest and second highest count of villages respectively, which seems to suggest that a significant number of villages undergo periodic fluctuations in disease incidence.\nHowever, it is imperative to evaluate the statistical significance of these findings. To accomplish this, we will isolate only those EHSA classes that possess a p-value less than 0.05.\n\ntainan_ehsa_sig &lt;- tainan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\nNext, we will try to plot a new bar chart with tainan_ehsa_sig to observe any changes in distribution after the exclusion of statistically insignificant classes.\n\nggplot(data = tainan_ehsa_sig,\n       aes(y = classification, fill = classification))+\n  geom_bar(show.legend = FALSE)\n\n\n\n\nIt seems that the count of oscillating coldspots has dropped significantly while oscillating hotspots remain consistent.\n\n\n8.4.2 Visualizing EHSA Maps\nIn this section, we will create EHSA maps to identify the geographic distribution of different hotspots and coldspots in our study area. Prior to this, we will formulate two new sf objects, tainan_ehsa_sig_cold and tainan_ehsa_sig_hot which will respectively represent the statistically significant coldspots and hotspots within our study area.\n\ntainan_ehsa_sig_cold &lt;- tainan_ehsa_sig %&gt;% filter(classification %in% c(\"consecutive coldspot\",\"oscilating coldspot\",\"sporadic coldspot\"))\n  \ntainan_ehsa_sig_hot &lt;- tainan_ehsa_sig %&gt;% filter(classification %in% c(\"consecutive hotspot\",\"new hotspot\",\"oscilating hotspot\",\"sporadic hotspot\"))\n\n\n\n8.4.3 Emerging Hotspots of Dengue Cases in Study Area\nIn this section, we will employ relevant tmap functions to visualize the identified emerging hotspots of dengue cases across the study area.\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_hot)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f67774\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_hot)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f67774\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n8.4.4 Emerging Coldspots of Dengue Cases in Study Area\nIn this section, we will employ relevant tmap functions to visualize the identified emerging coldspots of dengue cases across the study area.\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_cold)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Coldspots of Dengue Cases in Study Area (Tainan City)\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig_cold)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\n8.4.5 Emerging Hospots and Coldspots of Dengue Cases in Tainan City\n\ntmap_mode(\"plot\")  \ntm_shape(tainan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#de573e\",\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Tainan City\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            legend.hist.width = 0.5,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(tainan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(tainan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#de573e\",\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\n8.5.4 Visualising EHSA Map for Each Township\nIn our previous section, we have prepared and conducted analysis of emerging hotspots and coldspots across the entire study area. In this section, we will further refine our analysis by segmenting the results according to each district/township. This will allow us to delve into a more detailed examination of the hotspots and coldspots within each individual district.\nFirstly, we will filter tainan_ehsa and tainan_ehsa_sig objects to each districts within our study area. Following this, we will generate EHSA for each district. These maps will serve as the basis for our subsequent discussions and analyses of the findings.\n\n\nShow the code\nannan_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Annan District\")\nrende_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"South District\")\neast_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"East District\")\nnorth_ehsa &lt;- tainan_ehsa %&gt;% filter(TOWNENG == \"North District\")\n\n\nannan_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Annan District\")\nrende_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Rende District\")\nyongkang_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Yongkang District\")\nanping_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"Anping District\")\nwestcentral_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"West Central District\")\nsouth_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"South District\")\neast_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"East District\")\nnorth_ehsa_sig &lt;- tainan_ehsa_sig %&gt;% filter(TOWNENG == \"North District\")\n\n\n\nAnnan DistrictRende DistrictYongkang DistrictAnping DistrictWest Central DistrictSouth DistrictEast DistrictNorth District\n\n\n\n\nShow the code\ntm_shape(annan_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Annan District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"RIGHT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Annan District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(annan_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(annan_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nAnnan District is predominantly characterized by hotspots rather than coldspots. A significant number of oscillating hotspots are observed in this district, suggesting that these villages, despite being coldspots at one point, have experienced a rise in case counts. Two new hotspots have been identified - Chengnan Village and Chengbei Village. Additionally, Chengxi Village has been identified as a sporadic hotspot, which may indicate intermittent outbreaks of dengue cases in the village. In close proximity to these hotspots are oscillating coldspots, suggesting that these villages have effectively controlled the spread of cases, reduced the case counts, and eventually transitioned into coldspots.\n\n\n\n\n\nShow the code\ntm_shape(rende_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Rende District\",\n           main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"LEFT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Rende District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(rende_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(rende_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\nAnalysis & Discussion\nRende district observes a mixture of coldspots, hotspots and villages with no pattern detected. Interestingly, two villages - Chenggong Village and Wenxian Village - are identified as oscillating hotspots, while two others - Dajia Village and Renyi Village - are identified as oscillating coldspots. This contrast may suggest a non-uniform spatial distribution of dengue cases, influenced by localized factors that either increase or decrease case counts. Additionally, one village, Bao’an Village, is identified as a sporadic coldspot. This could indicate intermittent periods of lower case counts in this area.\n\n\n\n\n\nShow the code\ntm_shape(yongkang_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Yongkang District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Yongkang District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(yongkang_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(yongkang_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#f2ffff\",\"#b977cb\",\"#f8b675\",\"#7977f3\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nYongkand district is predominantly characterised by hotspots. Three villages - Ankang Village, Xishi Village, and Wangxing Village - stand out as consecutive hotspots. This could suggest that these villages have maintained statistically significant dengue clusters over the observed period. Upon closer inspection, it appears that many villages neighboring these consecutive hotspots are identified as oscillating hotspots. This indicates that dengue cases have been spreading from the consecutive hotspots to their neighbors, which were previously coldspots. It is critically important for the local governments to implement more proactive and effective vector control and intervention measures to prevent the outbreak from spreading further.\nIt is also interesting to see three oscilating coldspots - Erwang Village, Zhongxing Village, Zhengqiang Village - and one sporadic coldspot - Guangfu Village, despite being surrounded by hotspots. These villages may have implemented localized measures that have effectively prevented the spread of cases from their neighboring hotspots.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(anping_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(anping_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in Anping District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for Anping District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(anping_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(anping_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#7977f3\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nAnping District observes a mixture of coldspots and hotspots. Three villages - Yizai Village, Pingtong Village, and Ping’an Village - are identified as oscillating hotspots. In contrast, two villages - Huaping Village and Jianping Village - are identified as oscillating coldspots. Yiping Village is singled out as a sporadic coldspot. Overall, it appears that Anping District has generally low case counts, resulting in a predominance of coldspots over the observed period. However, the presence of three oscillating hotspots signals potential for the spread of outbreaks, necessitating effective containment measures.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(westcentral_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(westcentral_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in West Central District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for West Central District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(westcentral_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(westcentral_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#f67774\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nWest Central District is predominantly chracterised by hotspots, evident by one sporatic hotspot - Xihu Village, three oscilating hotspot - Xihe Village, Duiyue Village, Wutiaogang Village and one new hotspot - Chuhkkan Village. It appears that the cases have been spreading eastward from Xihu Village to its neighboring villages. Interestingly, Xixan Village, which is located adjacent to Xihu Village, is classified as an oscillating coldspot. This suggests that Xixan Village has managed to contain the spread from Xihu Village and gradually reduce the case count.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")  \ntm_shape(south_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in South District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            legend.position = c(\"LEFT\", \"TOP\"),\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"RIGHT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for South District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(south_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(south_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nSouth District is predominantly characterized by hotspots, either oscilating hotspots or sporadic hotspots. Interestingly, two villages - Zhangnan Village and Wennan Village - located in proximity to these hotspots, are identified as oscillating coldspots. This suggests that these two villages have implemented effective localized measures that have successfully curtailed the spread of cases from their neighboring hotspots, thereby reducing the case count over the observed period.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(east_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in East District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for East District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(east_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(east_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#de573e\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nEast District is predominantly characterised by hotspots, majority of which are either sporadic or oscilating hotspots. Datong Village is classified as consecutive hotspots. There are two oscilating coldspots observed - Quannan Village and Dongzhi Village. Particularly intriguing is the fact that Quannan Village, despite being in immediate proximity to Datong Village, has managed to contain the spread of cases from Datong Village and transition into a coldspot.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(north_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Emerging Hotspots & Coldspots \\nof Dengue Cases in North District\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\n\n\nEHSA Interactive Map for North District\n\n\nShow the code\ntmap_mode(\"view\")\ntm_shape(north_ehsa)+\n  tm_polygons(id = \"label\")+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(north_ehsa_sig)+\n  tm_fill(\"classification\", \n          palette = c(\"#57bfc0\",\"#b977cb\",\"#f8b675\",\"#f8d673\"),\n          title = \"classification\",\n          midpoint = 0,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6)\n\n\n\n\n\n\n\n\n\nAnalysis & Discussion\nNorth District exhibits a blend of oscillating hotspots, sporadic hotspots, and oscillating coldspots. Intriguingly, North District is the only district in our study area that records a consecutive coldspot - Zhonglou Village. Despite its two neighboring villages, Zhangsheng Village and Ren’ai Village, being classified as an oscillating hotspot and a sporadic hotspot respectively, it is remarkable to observe that Zhonglou Village has successfully prevented the spread of cases from these hotspots throughout the entire observed period, maintaining its status as a stable coldspot. This village will be of interest for further investigation to understand the localized factors and measures that might have contributed to this successful containment."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "title": "Take-Home Exercise 02: Spatio-Temporal Analysis of Dengue Fever Outbreak in Tainan City, Taiwan",
    "section": "References",
    "text": "References\nGriffith, D. (2017). Spatial Autocorrelation. The Geographic Information Science & Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed). DOI: 10.22224/gistbok/2017.4.13\nKhambhammettu, P. (2005). Mann-Kendall Analysis for the Fort Ord Site (Report No. OU-1 2004 Annual Groundwater Monitoring Report-Former Fort Ord, California). HydroGeoLogic, Inc.\nGilbert, R.O. (1987). Statistical methods for environmental pollution monitoring. Van Nostrand Reinhold.\nMergenthaler, C., Gurp, M., Rood, E., & Bakker, M. (2022). The study of spatial autocorrelation for infectious disease epidemiology decision-making: A systematized literature review. CABI Reviews. https://doi.org/10.1079/cabireviews202217018\nMoraga, P. (2024). Spatial neighbourhood matrices. In Spatial statistics for Data Science: Theory and practice with R (pp. 83–94). CRC Press.\nHaining, R. P. (2001). Spatial autocorrelation. International Encyclopaedia of the Social & Behavioural Sciences, 14763–14768. https://doi.org/10.1016/b0-08-043076-7/02511-0\nSalima, B. A., & Bellefon, M.-P., (2018). Spatial autocorrelation indices. In Handbook of Spatial Analysis: Theory and Application with R (pp. . INSEE.\nAnselin, L. (2020). Contiguity-Based Spatial Weights. GeoDa: An Introduction to Spatial Data Science. https://geodacenter.github.io/workbook/4a_contig_weights/lab4a.html\nAnselin, L. (2018). Applications of Spatial Weights. GeoDa: An Introduction to Spatial Data Science. https://geodacenter.github.io/workbook/4d_weights_applications/lab4d.html\nGetis, A., & Ord, J. K. (1992). The analysis of Spatial Association by use of Distance Statistics. Geographical Analysis, 24(3), 189–206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#overview",
    "href": "In-class_Ex/In-class_Ex02.html#overview",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "title": "In-Class Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow for reading and writing Apache Parquet files\nlubridate for tackling with temporal data (dates and times)\ntidyverse for manipulating and wrangling data, as well as, implementing data science functions\ntmap for creating and visualizing thematic maps\nsf for handling geospatial data.\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "href": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "title": "In-Class Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia.\n\nApart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\n\n\n3.1 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format.\n\nFirstly, we will use read_parquet function from arrow package\n\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet')\ndf_1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet')\n\n\nNext, we will use head() function to quickly scan through the data columns and values.\n\n\nhead(df)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)\n\n\n\n3.2 Extracting Trip Starting Locations and Temporal Data Values\nAfter loading the Grab-Posisi dataset, we will extract features that we want to use for analysis. Firstly, we will extract trip starting locations for all trajectories in the dataset and save it into a new df called origin_df.\nAlso, we are interested to derive useful temporal data such as day of the week, hour, and yy-mm-dd. To do so, we will use the following functions from lubridate package, and add the newly derived values as new columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n3.3 Extracting Trip Ending Locations and Temporal Data Values\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, for destination_df, we use arrange(desc()) argument to sort the timestamps in descending order\n\n\n\n\n3.4 Saving R Objects in RDS Format\nRDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow us to preserve the state of an object between R sessions. Saving R object as an RDS file in R can be useful for sharing our work with others, replicating our analysis, or simply storing our work for later use.\n\nwrite_rds(origin_df, \"../data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"../data/rds/destination_df.rds\")\n\n\n\n3.4 Importing RDS Objects\n\norigin_df &lt;- read_rds(\"../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../data/rds/destination_df.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04.html",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates. Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights. Particularly, we will explore using spdep and GWmodel.\n\n\n\nIn this hands-on exercise, we will use the following R package:\n\nsf\nsp\nspdep,\ntmap\ntidyverse\nknitr\nGWmodel\n\n\npacman::p_load(sf, sp, spdep, tmap, tidyverse, knitr, GWmodel)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;\n\n\n\n\n\n\n\n\nIn previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nhunan_sp &lt;- hunan %&gt;% as_Spatial()\n\n\n\n\n\nThis function calculates basic and robust GWSS, including geographically weighted means, standard deviations and skew. Robust alternatives include geographically weighted medians, inter-quartile ranges and quantile imbalances. This function also calculates basic geographically weighted covariances together with basic and robust geographically weighted correlations.\n\ngwstat &lt;- gwss(data= hunan_sp,\n               vars = \"GDPPC\",\n               bw = 6,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\nWhat can we learn from this code chunk?\n\nThe data argument is set to hunan_sp, which means the data being used is stored in the hunan_sp variable.\nThe vars argument is set to \"GDPPC\". This indicates that the variable of interest in the hunan_sp data is \"GDPPC\".\nThe bw argument is set to 6. This is the bandwidth parameter for the geographical weighting, which controls the degree of smoothing.\nThe kernel argument is set to \"bisquare\". This means the bisquare kernel function is used for weighting.\nThe adaptive argument is set to TRUE, which means the bandwidth is adaptive. In other words, the bandwidth adjusts depending on the density of the data points.\nThe longlat argument is set to T (short for TRUE). This indicates that the data’s coordinates are in longitude and latitude.\n\n\ngwstat\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n\n   ***********************Calibration information*************************\n\n   Local summary statistics calculated for variables:\n    GDPPC\n   Number of summary points: 88\n   Kernel function: bisquare \n   Summary points: the same locations as observations are used.\n   Adaptive bandwidth: 6 (number of nearest neighbours)\n   Distance metric: Great Circle distance metric is used.\n\n   ************************Local Summary Statistics:**********************\n   Summary information for Local means:\nGDPPC_LM \n    Min.  1st Qu.   Median  3rd Qu.     Max. \n10313.11 17616.21 21981.39 28547.05 73545.30 \n   Summary information for local standard deviation :\nGDPPC_LSD \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n  926.3095  3319.0782  5041.7997  7602.3637 24377.4421 \n   Summary information for local variance :\nGDPPC_LVar \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n   858049.3  11030994.6  25419746.0  57798649.1 594259684.0 \n   Summary information for Local skewness:\nGDPPC_LSKe \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-3.74328455 -0.06903772  0.74202796  1.20539540  5.98374890 \n   Summary information for localized coefficient of variation:\nGDPPC_LCV \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n0.04955751 0.15100369 0.22079470 0.33316203 0.95567593 \n\n   ************************************************************************\n\n\nThe output of the gwss() function is a SpatialPointsDataFrame (SDF). We can view the values inside this dataframe.\n\ngwstat_df &lt;- gwstat[[\"SDF\"]]@data\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127\n\n\nGDPPC_LM refers to local means\nGDPPC_LSD refers to local standard deviation\nGDPPC_LVar refers to local variance\nGDPPC_LSKe refers to local skew\nGDPPC_LCV refers to local coefficients of variation\n\n\n\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#overview",
    "href": "In-class_Ex/In-class_Ex04.html#overview",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "Spatial weights are a key component in any cross-sectional analysis of spatial dependence. They are an essential element in the construction of spatial autocorrelation statistics, and provide the means to create spatially explicit variables, such as spatially lagged variables and spatially smoothed rates. Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this exercise, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights. Particularly, we will explore using spdep and GWmodel."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex04.html#importing-packages",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this hands-on exercise, we will use the following R package:\n\nsf\nsp\nspdep,\ntmap\ntidyverse\nknitr\nGWmodel\n\n\npacman::p_load(sf, sp, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex04.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nIn this section, read_csv() of sf package will be used to import the csv file into R environment. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "In previous section, we have imported a shapefile hunan representing the geographical boundaries of Hunan and a dataframe hunan2012 which contains the attribute fields corresponding to counties in Hunan.\nThe next step in our analysis involves updating the attribute table of the hunan shapefile with the values from hunan2012.\nHence, we will need to update the attribute table of Hunan by using left_join() of dplyr package. This function effectively merges the two datasets, ensuring that each county’s geographical data is accurately linked with its corresponding attribute data from the hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012, join_by(County))%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nhunan_sp &lt;- hunan %&gt;% as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#geographically-weighted-summary-statistics-gwss",
    "href": "In-class_Ex/In-class_Ex04.html#geographically-weighted-summary-statistics-gwss",
    "title": "In-Class Exercise 04",
    "section": "",
    "text": "This function calculates basic and robust GWSS, including geographically weighted means, standard deviations and skew. Robust alternatives include geographically weighted medians, inter-quartile ranges and quantile imbalances. This function also calculates basic geographically weighted covariances together with basic and robust geographically weighted correlations.\n\ngwstat &lt;- gwss(data= hunan_sp,\n               vars = \"GDPPC\",\n               bw = 6,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\nWhat can we learn from this code chunk?\n\nThe data argument is set to hunan_sp, which means the data being used is stored in the hunan_sp variable.\nThe vars argument is set to \"GDPPC\". This indicates that the variable of interest in the hunan_sp data is \"GDPPC\".\nThe bw argument is set to 6. This is the bandwidth parameter for the geographical weighting, which controls the degree of smoothing.\nThe kernel argument is set to \"bisquare\". This means the bisquare kernel function is used for weighting.\nThe adaptive argument is set to TRUE, which means the bandwidth is adaptive. In other words, the bandwidth adjusts depending on the density of the data points.\nThe longlat argument is set to T (short for TRUE). This indicates that the data’s coordinates are in longitude and latitude.\n\n\ngwstat\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n\n   ***********************Calibration information*************************\n\n   Local summary statistics calculated for variables:\n    GDPPC\n   Number of summary points: 88\n   Kernel function: bisquare \n   Summary points: the same locations as observations are used.\n   Adaptive bandwidth: 6 (number of nearest neighbours)\n   Distance metric: Great Circle distance metric is used.\n\n   ************************Local Summary Statistics:**********************\n   Summary information for Local means:\nGDPPC_LM \n    Min.  1st Qu.   Median  3rd Qu.     Max. \n10313.11 17616.21 21981.39 28547.05 73545.30 \n   Summary information for local standard deviation :\nGDPPC_LSD \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n  926.3095  3319.0782  5041.7997  7602.3637 24377.4421 \n   Summary information for local variance :\nGDPPC_LVar \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n   858049.3  11030994.6  25419746.0  57798649.1 594259684.0 \n   Summary information for Local skewness:\nGDPPC_LSKe \n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-3.74328455 -0.06903772  0.74202796  1.20539540  5.98374890 \n   Summary information for localized coefficient of variation:\nGDPPC_LCV \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n0.04955751 0.15100369 0.22079470 0.33316203 0.95567593 \n\n   ************************************************************************\n\n\nThe output of the gwss() function is a SpatialPointsDataFrame (SDF). We can view the values inside this dataframe.\n\ngwstat_df &lt;- gwstat[[\"SDF\"]]@data\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127\n\n\nGDPPC_LM refers to local means\nGDPPC_LSD refers to local standard deviation\nGDPPC_LVar refers to local variance\nGDPPC_LSKe refers to local skew\nGDPPC_LCV refers to local coefficients of variation\n\n\n\ngwstat_df\n\n   GDPPC_LM  GDPPC_LSD  GDPPC_LVar  GDPPC_LSKe  GDPPC_LCV\n1  26797.76  5368.4595  28820357.0  0.66633204 0.20033243\n2  21194.35  1698.0390   2883336.3  2.09614479 0.08011754\n3  27755.09  4825.3960  23284446.3  0.64707878 0.17385627\n4  27142.60  4146.6643  17194824.5  1.16119871 0.15277327\n5  27681.99  4332.3440  18769204.5  0.92967739 0.15650406\n6  24249.16  3670.7229  13474206.9 -0.85466668 0.15137523\n7  59246.71 22419.9966 502656247.4 -0.62939642 0.37841757\n8  50685.94 16702.7409 278981554.2 -0.86488843 0.32953402\n9  69729.10 15306.7753 234297370.2 -0.84589675 0.21951775\n10 19740.12  8635.5046  74571939.8  1.07796143 0.43745947\n11 24375.01 22290.1025 496848668.9  1.18483157 0.91446544\n12 29322.23  5962.4104  35550337.5 -1.88571194 0.20334096\n13 23969.10  5043.2717  25434589.3  0.56523820 0.21040723\n14 25383.12 24258.0380 588452406.9  1.03964154 0.95567593\n15 21816.34  5040.3276  25404902.6  1.04816353 0.23103455\n16 36359.36 16102.9533 259305105.8  0.56214571 0.44288329\n17 42389.06 24307.7075 590864643.0 -0.32129825 0.57344291\n18 23672.87  2642.5387   6983010.8 -2.57545765 0.11162733\n19 27607.43  3389.1128  11486085.5  1.50576400 0.12276090\n20 22223.48  1889.3881   3569787.4  0.97182679 0.08501764\n21 24679.04  2679.7697   7181165.8 -1.15519704 0.10858485\n22 26558.82  7204.3387  51902495.6  0.01029577 0.27125975\n23 19778.52  2879.7720   8293087.0  1.11653676 0.14560101\n24 18458.67  5484.3159  30077721.1  1.62822457 0.29711324\n25 23423.43  7021.5422  49302054.4  0.03116617 0.29976582\n26 16541.26  2397.1846   5746493.9  0.60063497 0.14492156\n27 16545.42  3392.6269  11509917.5  0.06849616 0.20504933\n28 15066.82  4122.0062  16990935.1  3.08938061 0.27358169\n29 14821.56  3662.2795  13412290.8  0.44253465 0.24709131\n30 16915.67  2471.5922   6108768.0  0.72194074 0.14611257\n31 15183.78  3588.4893  12877255.6  2.94946079 0.23633700\n32 21541.65  5069.4678  25699503.7 -1.43147322 0.23533329\n33 19601.11  3108.9744   9665722.0  0.50522382 0.15861214\n34 32412.12 24242.1346 587681090.0  0.53522173 0.74793418\n35 20599.49  4617.7969  21324048.0  1.19120123 0.22417046\n36 28859.13 23348.8623 545169370.5  0.85575588 0.80906328\n37 11984.64  2493.0666   6215381.2  0.86286370 0.20802177\n38 17861.63  4667.0959  21781784.0 -0.94299674 0.26129176\n39 12911.26  2058.7063   4238271.6  0.57312479 0.15945051\n40 10313.11  1358.4328   1845339.8  1.46878192 0.13171898\n41 22146.45  4918.0988  24187695.5 -1.25740463 0.22207165\n42 14975.46  2055.6929   4225873.1 -1.70732382 0.13727074\n43 12164.44  1245.7923   1551998.4  1.22251626 0.10241262\n44 13193.61  3927.1815  15422754.5  1.39569677 0.29765777\n45 28443.02 24377.4421 594259684.0  0.77640738 0.85706245\n46 47125.62 15312.7089 234479054.1 -0.27861152 0.32493383\n47 38611.99 14830.2228 219935507.2  0.36610524 0.38408332\n48 17657.75  5844.9396  34163319.0  1.30857144 0.33101270\n49 17491.60  6505.9155  42326936.2  1.45654920 0.37194513\n50 15515.46  7517.4264  56511699.0  1.54877539 0.48451204\n51 20510.72  6286.3751  39518511.4  0.76211517 0.30649211\n52 26867.31  6659.8455  44353542.2 -0.81626138 0.24787916\n53 10537.76  1827.5034   3339768.6  1.73988753 0.17342427\n54 21458.65  6410.2129  41090829.2  0.92850266 0.29872402\n55 11158.95  2038.4420   4155245.7  0.87994007 0.18267325\n56 17755.28 12355.3608 152654940.8  3.44850457 0.69586971\n57 24456.63  3410.7395  11633143.6  0.67835216 0.13946070\n58 26336.34  4277.4598  18296661.9  0.62670078 0.16241662\n59 16996.88  1726.1703   2979664.0  0.32191512 0.10155806\n60 21694.07  5586.4342  31208247.3  0.41714732 0.25750971\n61 13001.25  1283.9048   1648411.5  5.98374890 0.09875244\n62 18465.25  3787.5902  14345839.6 -0.53021537 0.20511986\n63 18303.20  7692.6257  59176489.7  1.19968844 0.42028858\n64 26403.02  3950.9600  15610084.7 -0.16628639 0.14964046\n65 29483.86  5240.6913  27464845.8 -0.03662150 0.17774779\n66 39627.60  5939.7432  35280549.4  2.79485609 0.14988906\n67 31222.95 20177.8114 407144072.6  1.17435721 0.64624938\n68 38729.57 13251.8288 175610966.4  1.87097064 0.34216308\n69 21023.94  4320.0698  18663002.8  0.33292211 0.20548333\n70 21779.33  7897.1802  62365454.8  0.84949897 0.36259973\n71 36145.76  7572.2763  57339368.9  0.60666792 0.20949281\n72 18320.55  7250.8272  52574495.5  2.82295265 0.39577568\n73 32111.38  6803.2988  46284874.5 -0.78168385 0.21186567\n74 31241.75  6281.7162  39459957.9  1.01170224 0.20106800\n75 14149.53  3479.5527  12107287.0  0.61165691 0.24591292\n76 30094.68  5357.3059  28700726.8  1.43635637 0.17801508\n77 20612.58  1021.5081   1043478.9 -1.22827158 0.04955751\n78 28030.22 16659.8549 277550765.8  1.55786400 0.59435324\n79 11568.42  4599.1330  21152024.5  2.42445803 0.39755941\n80 30072.20 20122.3972 404910869.5  1.07612199 0.66913621\n81 20798.53  6099.8177  37207775.7  0.91816728 0.29328120\n82 20711.07  2402.3836   5771447.0  1.04546797 0.11599516\n83 31736.24  3995.0473  15960402.8 -3.74328455 0.12588280\n84 73545.30 17307.2896 299542274.1 -0.94731806 0.23532827\n85 22871.95  1329.5897   1767808.8  3.12696176 0.05813188\n86 32486.94 11032.8903 121724668.1  1.59450531 0.33961003\n87 17429.45  2571.4590   6612401.6 -0.50838795 0.14753533\n88 17096.49   926.3095    858049.3 -0.68882538 0.05418127"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on07.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "We can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"#ff7d04\")\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nNext, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n\n\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"../data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\n\n#shan_sf &lt;- read_rds(\"../data/rds/shan_sf.rds\")\n\n\n\n\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\ntm_shape(shan_sf) +\n  tm_fill(\"RADIO_PR\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          palette=\"plasma\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                palette=\"plasma\",\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#correlation-analysis",
    "href": "Hands-on_Ex/hands_on07.html#correlation-analysis",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex/hands_on07.html#hierarchy-cluster-analysis",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this section, we will explore how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#visually-driven-hierarchical-clustering-analysis",
    "href": "Hands-on_Ex/hands_on07.html#visually-driven-hierarchical-clustering-analysis",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = plasma,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n::: {#fig-width : 10 .cell-output-display}\n\n\n\n:::"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#mapping-the-clusters-formed",
    "href": "Hands-on_Ex/hands_on07.html#mapping-the-clusters-formed",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "With closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\ntm_shape(shan_sf_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex/hands_on07.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(shan_sp, \n     border=grey(.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(shan_sp, \n     border=grey(.5))\n\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\n#| fig-width: 10\n\ntm_shape(shan_sf_spatialcluster) +\n  tm_fill(\"SP_CLUSTER\",\n          palette=\"-plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex/hands_on07.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "In this section, we will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n\n\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\ntm_shape(shan_sf_ngeo_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#spatially-constrained-hierarchical-clustering",
    "href": "Hands-on_Ex/hands_on07.html#spatially-constrained-hierarchical-clustering",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "Before we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\ntm_shape(shan_sf_Gcluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on07.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex/hands_on07.html#visual-interpretation-of-clusters",
    "title": "Hands-On Exercise 07",
    "section": "",
    "text": "Past studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06.html",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.\n\n\n\nIn this hands-on exercise, we are interested in delineating homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\n\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, sp and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(sp,spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\nIn this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\n\n\n\nshan_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf we look at the data, there are many polygons that belong to other provinces than the study area we are interested in. Hence, it is important to use filter() to filter the data to our study area only. We filtered the data to include only the rows where the ST column matches any of the values in the vector c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"). We will select only the columns 2 through 7 of the data. The remaining columns are discarded as they are irrelevant to our study.\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\n\n\n\n\nTip\n\n\n\nNoticed that the CRS for shan_sf is in WGS 84. In this study particular, CRS is not relevant, so we are not required to change it to local projection.\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\nThe csv file will be import using read_csv function of readr package.\n\nict &lt;- read_csv (\"../data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\n\n\n\nTip\n\n\n\nRanges are different for each indicator variable. To do a proper hierarchical clustering, we need to standardise the range.\n\n\n\n\n\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\n\n\n\n\n\nTip\n\n\n\nTo make the analysis more intuitive, we rename the column names. It is a good practice in analytics for tidying up the data.\n\n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.\n\n\n\n\n\n\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"#ff7d04\")\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nNext, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n\n\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"../data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\n\n\n\n\n\n\nTip\n\n\n\nWhen we use left_join(), if we want the output to be geometric type, the first data.frame should be a sf object so that the second data.frame is joint to the first data.frame. by is an argument for specifying the unique identifier that is common in both data.frames.\nAnother function that can be used is st_join() but both data.frame should be in sf object if we use it.\n\n\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data\npframe are now updated into the data frame of shan_sf.\n\nshan_sf &lt;- read_rds(\"../data/rds/shan_sf.rds\")\n\n\n\n\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\ntm_shape(shan_sf) +\n  tm_fill(\"RADIO_PR\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                palette=\"viridis\",\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\n\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.\n\n\n\nIn this section, we will explore how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nTip\n\n\n\nst_set_geometry(NULL) is used to drop the geometry column from shan_sf object, to avoid any potential error when using some analytical packages and functions.\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\n\n\n\n\n\n\nTip\n\n\n\nThe code line row.names(cluster_vars) &lt;- cluster_vars$\"TS.x\" is used to set the row names of the cluster_vars data frame to be the same as the values in the TS.x column of the cluster_vars data frame. This can be useful for identifying rows later on.\n\n\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nwrite_rds(shan_ict,\"../data/rds/shan_ict.rds\")\nshan_ict &lt;- read_rds(\"../data/rds/shan_ict.rds\")\n\n\n\n\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\n\n\n\n\n\n\nTip\n\n\n\nmethod argument specify the distance measure to be used. This must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\" or \"minkowski\".\n\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\n\n\n\n\n\n\nTip\n\n\n\nmethod argument specify the agglomeration method to be used. This should be one of \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"average\" (= UPGMA), \"mcquitty\" (= WPGMA), \"median\" (= WPGMC) or \"centroid\" (= UPGMC).\n\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLocal optimization has high chance of producing more disaggregated results with multiple clusters than manageable.\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = plasma,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n::: {#fig-width : 10 .cell-output-display}\n\n\n\n:::\n\n\n\n\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\n\n\n\n\n\n\nTip\n\n\n\nas.factor() is used to convert numerical to factor variable type.\n\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\n\n\n\n\n\nTip\n\n\n\nleft_join() require unique identifier to map between two objects to be merged. In this case, we can use cbind() to append the new cluser data to shan.sfobejct.\n\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\ntm_shape(shan_sf_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.\n\n\n\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\n\n\n\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan_sp &lt;- as_Spatial(shan_sf)\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\npts &lt;- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb, \n     pts, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\nTip\n\n\n\nwhen using sf, we have to convert from poygon to point using st_coordinates() and st_centroid() functions. The newly created points are saved in a variable called pts before using it ot plot the map.\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\n\nplot.mst(shan.mst, \n         pts, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\n\nplot(clust6, \n     pts, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\n#| fig-width: 10\n\ntm_shape(shan_sf_spatialcluster) +\n  tm_fill(\"SP_CLUSTER\",\n          palette=\"-plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nIn this section, we will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n\n\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\ntm_shape(shan_sf_ngeo_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\ntm_shape(shan_sf_Gcluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nClustering analysis did not end at mapping the clusters. We also need to interpret clusters visually through plots as well.\n\n\n\n\n\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#overview",
    "href": "In-class_Ex/In-class_Ex06.html#overview",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThe advantage of spatially constrained methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. This provides a lot of upside in cases where there is a real-life application that requires separating geographies into discrete regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. There are many applications and many situations where the optimal clustering, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#analytical-objective",
    "href": "In-class_Ex/In-class_Ex06.html#analytical-objective",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this hands-on exercise, we are interested in delineating homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex06.html#importing-packages",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "The R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, sp and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(sp,spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex06.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this exercise, we will use the following datasets:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\n\n\n\nshan_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf we look at the data, there are many polygons that belong to other provinces than the study area we are interested in. Hence, it is important to use filter() to filter the data to our study area only. We filtered the data to include only the rows where the ST column matches any of the values in the vector c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"). We will select only the columns 2 through 7 of the data. The remaining columns are discarded as they are irrelevant to our study.\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\n\n\n\n\nTip\n\n\n\nNoticed that the CRS for shan_sf is in WGS 84. In this study particular, CRS is not relevant, so we are not required to change it to local projection.\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n\nThe csv file will be import using read_csv function of readr package.\n\nict &lt;- read_csv (\"../data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\n\n\n\nTip\n\n\n\nRanges are different for each indicator variable. To do a proper hierarchical clustering, we need to standardise the range.\n\n\n\n\n\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\n\n\n\n\n\nTip\n\n\n\nTo make the analysis more intuitive, we rename the column names. It is a good practice in analytics for tidying up the data.\n\n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#exploratory-data-analysis-eda",
    "href": "In-class_Ex/In-class_Ex06.html#exploratory-data-analysis-eda",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "We can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"#ff7d04\")\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\n\n\n\nNext, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n\n\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"../data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\n\n\n\n\n\n\nTip\n\n\n\nWhen we use left_join(), if we want the output to be geometric type, the first data.frame should be a sf object so that the second data.frame is joint to the first data.frame. by is an argument for specifying the unique identifier that is common in both data.frames.\nAnother function that can be used is st_join() but both data.frame should be in sf object if we use it.\n\n\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data\npframe are now updated into the data frame of shan_sf.\n\nshan_sf &lt;- read_rds(\"../data/rds/shan_sf.rds\")\n\n\n\n\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\ntm_shape(shan_sf) +\n  tm_fill(\"RADIO_PR\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          palette=\"viridis\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                palette=\"viridis\",\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#correlation-analysis",
    "href": "In-class_Ex/In-class_Ex06.html#correlation-analysis",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#hierarchy-cluster-analysis",
    "href": "In-class_Ex/In-class_Ex06.html#hierarchy-cluster-analysis",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this section, we will explore how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\n\n\n\nTip\n\n\n\nst_set_geometry(NULL) is used to drop the geometry column from shan_sf object, to avoid any potential error when using some analytical packages and functions.\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\n\n\n\n\n\n\nTip\n\n\n\nThe code line row.names(cluster_vars) &lt;- cluster_vars$\"TS.x\" is used to set the row names of the cluster_vars data frame to be the same as the values in the TS.x column of the cluster_vars data frame. This can be useful for identifying rows later on.\n\n\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nwrite_rds(shan_ict,\"../data/rds/shan_ict.rds\")\nshan_ict &lt;- read_rds(\"../data/rds/shan_ict.rds\")\n\n\n\n\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"#ff7d04\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\n\n\n\n\n\n\nTip\n\n\n\nmethod argument specify the distance measure to be used. This must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\" or \"minkowski\".\n\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\n\n\n\n\n\n\nTip\n\n\n\nmethod argument specify the agglomeration method to be used. This should be one of \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"average\" (= UPGMA), \"mcquitty\" (= WPGMA), \"median\" (= WPGMC) or \"centroid\" (= UPGMC).\n\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLocal optimization has high chance of producing more disaggregated results with multiple clusters than manageable.\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#visually-driven-hierarchical-clustering-analysis",
    "href": "In-class_Ex/In-class_Ex06.html#visually-driven-hierarchical-clustering-analysis",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = plasma,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n::: {#fig-width : 10 .cell-output-display}\n\n\n\n:::"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#mapping-the-clusters-formed",
    "href": "In-class_Ex/In-class_Ex06.html#mapping-the-clusters-formed",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "With closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\n\n\n\n\n\n\nTip\n\n\n\nas.factor() is used to convert numerical to factor variable type.\n\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\n\n\n\n\n\nTip\n\n\n\nleft_join() require unique identifier to map between two objects to be merged. In this case, we can use cbind() to append the new cluser data to shan.sfobejct.\n\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\ntm_shape(shan_sf_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#spatially-constrained-clustering-skater-approach",
    "href": "In-class_Ex/In-class_Ex06.html#spatially-constrained-clustering-skater-approach",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\n\n\n\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan_sp &lt;- as_Spatial(shan_sf)\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\npts &lt;- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb, \n     pts, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\nTip\n\n\n\nwhen using sf, we have to convert from poygon to point using st_coordinates() and st_centroid() functions. The newly created points are saved in a variable called pts before using it ot plot the map.\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\n\n\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\n\nplot.mst(shan.mst, \n         pts, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\n\nplot(clust6, \n     pts, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\n#| fig-width: 10\n\ntm_shape(shan_sf_spatialcluster) +\n  tm_fill(\"SP_CLUSTER\",\n          palette=\"-plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#spatially-constrained-clustering-clustgeo-method",
    "href": "In-class_Ex/In-class_Ex06.html#spatially-constrained-clustering-clustgeo-method",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "In this section, we will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n\n\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\ntm_shape(shan_sf_ngeo_cluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#spatially-constrained-hierarchical-clustering",
    "href": "In-class_Ex/In-class_Ex06.html#spatially-constrained-hierarchical-clustering",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "Before we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\ntm_shape(shan_sf_Gcluster) +\n  tm_fill(\"CLUSTER\",\n          palette=\"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nClustering analysis did not end at mapping the clusters. We also need to interpret clusters visually through plots as well."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#visual-interpretation-of-clusters",
    "href": "In-class_Ex/In-class_Ex06.html#visual-interpretation-of-clusters",
    "title": "In-Class Exercise 06",
    "section": "",
    "text": "Past studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "spflow.html",
    "href": "spflow.html",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "This exercise uses spflow package for modeling spatial interactions using the example of home-to-work commuting flows. We will use information on the 71 municipalities that are located closest to the center of Paris. This data is contained in the package and was originally diffused by the French National Institutes of Statistics and Economic Studies (INSEE), and of Geographic and Forest Information (IGN).\n\n\n\n\npacman::p_load(Matrix,sf,spdep,tmap)\n#devtools::install_github(\"LukeCe/spflow\")\nlibrary(spflow)\n\ndata(\"paris10km_municipalities\")\ndata(\"paris10km_commuteflows\")\n\nEach municipality is identified by a unique id. Additionally, we have information on the population, the median income and the number of companies.\n\nparis10km_municipalities\n\nSimple feature collection with 71 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.199785 ymin: 48.76454 xmax: 2.499719 ymax: 48.95192\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ID_MUN POPULATION MED_INCOME NB_COMPANY AREA                       geometry\n1   75101      17100   31842.56      14333  182 MULTIPOLYGON (((2.344559 48...\n2   75102      22390   30024.50      14478   99 MULTIPOLYGON (((2.347832 48...\n3   75103      35991   30988.00      10696  117 MULTIPOLYGON (((2.350091 48...\n4   75104      27769   30514.67       7412  160 MULTIPOLYGON (((2.344559 48...\n5   75105      60179   32950.00      10290  252 MULTIPOLYGON (((2.344559 48...\n6   75106      43224   38447.69      10620  215 MULTIPOLYGON (((2.344559 48...\n7   75107      57092   41949.00      12602  412 MULTIPOLYGON (((2.320777 48...\n8   75108      38749   39774.00      52237  386 MULTIPOLYGON (((2.327121 48...\n9   75109      59474   32771.00      23687  218 MULTIPOLYGON (((2.325762 48...\n10  75110      94474   25154.00      23996  288 MULTIPOLYGON (((2.364678 48...\n\n\n\ntm_shape(paris10km_municipalities)+\n  tm_fill(\"ID_MUN\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\")) +\n  tm_borders(col = \"white\")+\n  tm_layout(main.title = \"Paris Municipalities\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\nThere are three different neighborhood matrices that can be used to describe the connectivity between the municipalities.\n\nold_par &lt;- par(mfrow = c(1, 3), mar = c(0,0,1,0))\n\nmid_points &lt;- suppressWarnings({\n    st_point_on_surface(st_geometry(paris10km_municipalities))})\n\nparis10km_nb &lt;- list(\n  \"by_contiguity\" = spdep::poly2nb(paris10km_municipalities),\n  \"by_distance\" = spdep::dnearneigh(mid_points,d1 = 0, d2 = 5),\n  \"by_knn\" = spdep::knn2nb(knearneigh(mid_points,3))\n)\n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_contiguity, mid_points, add = T, col = rgb(0,0,0,alpha=0.5))\ntitle(\"Contiguity\") \n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_distance,mid_points, add = T, col = rgb(0,0,0,alpha=0.5)) \ntitle(\"Distance\") \n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_knn, mid_points, add = T, col = rgb(0,0,0,alpha=0.5))\ntitle(\"3 Nearest Neighbors\") \n\n\n\npar(old_par)\n\nFinally, there is data on the size of the commuting flows and the distance between all pairs of municipalities\n\nhead(paris10km_commuteflows)\n\n  ID_ORIG ID_DEST DISTANCE COMMUTE_FLOW\n1   75101   75101    0.000   3771.23556\n2   75101   75102  786.743    294.76899\n3   75101   75103 1729.063     71.25116\n4   75101   75104 1807.294     99.38468\n5   75101   75105 2266.598     98.88915\n6   75101   75106 1512.870     65.15406\n\n\n\n\n\nThe spflow package builds on the idea that flows correspond to pairwise interactions between the nodes of an origin network with the nodes of a destination network.\nIn our example, the origin and destination networks are the same because every municipality is both an origin and destination of a flow.\nTo estimate the model efficiently, the spflow package uses moment-based estimation methods, that exploit the relational structure of flow data. This avoids duplication arising from the fact that each municipality is at the origin and destination of many flows. For more details on the model and the estimation methods see LeSage (2008), Dargel (2021) and Dargel (2022).\n\n\nTo describe the nodes of a network the package provides spflow_network-class that combines attributes of the nodes with the chosen network structure. For our model we choose the contiguity based neighborhood structure.\nFirst, we will create an object paris10km_net using sp_network_nodes() function. This object represents the nodes in our network. Each municipality will serve as a node.\n\nparis10km_net &lt;-  spflow_network(\n  id_net = \"paris\",\n  node_neighborhood = nb2mat(paris10km_nb$by_contiguity),\n  node_data = paris10km_municipalities,\n  node_key_column = \"ID_MUN\")\n\nparis10km_net\n\nSpatial network nodes with id: paris\n--------------------------------------------------\nNumber of nodes: 71\nAverage number of links per node: 5.239\nDensity of the neighborhood matrix: 7.38% (non-zero connections)\n\nData on nodes:\n    ID_MUN POPULATION MED_INCOME NB_COMPANY AREA COORD_X COORD_Y\n1    75101      17100   31842.56      14333  182    2.34   48.86\n2    75102      22390    30024.5      14478   99    2.34   48.87\n3    75103      35991      30988      10696  117    2.36   48.86\n4    75104      27769   30514.67       7412  160    2.36   48.85\n5    75105      60179      32950      10290  252    2.35   48.85\n6    75106      43224   38447.69      10620  215    2.33   48.85\n---    ---        ---        ---        ---  ---     ---     ---\n66   94046      54186      24329       3385  537    2.44    48.8\n67   94067      21846   31559.38       1763   90    2.42   48.84\n68   94069      14870   25790.65        957  144    2.43   48.82\n69   94076      56504      19447       2690  529    2.36   48.79\n70   94080      49831      30798       4655  191    2.43   48.85\n71   94081      88102    17860.5       4467 1166     2.4   48.79\n\n\nNext, we will create an object paris10km_pairs using sp_network_pair() function. This object represents the edges in our network. Each origin-destination record from paris10km_commuteflows will serve as an edge.\n\nparis10km_net_pairs &lt;-  spflow_network_pair(\n  id_orig_net = \"paris\",\n  id_dest_net = \"paris\",\n  pair_data = paris10km_commuteflows,\n  orig_key_column = \"ID_ORIG\",\n  dest_key_column = \"ID_DEST\")\nparis10km_net_pairs\n\nSpatial network pair with id: paris_paris\n--------------------------------------------------\nOrigin network id: paris (with 71 nodes)\nDestination network id: paris (with 71 nodes)\nNumber of pairs: 5041\nCompleteness of pairs: 100.00% (5041/5041)\n\nData on node-pairs:\n     ID_DEST ID_ORIG DISTANCE COMMUTE_FLOW\n1      75101   75101        0      3771.24\n2      75102   75101   786.74       294.77\n3      75103   75101  1729.06        71.25\n4      75104   75101  1807.29        99.38\n5      75105   75101   2266.6        98.89\n6      75106   75101  1512.87        65.15\n---      ---     ---      ---          ---\n5036   94046   94081  3742.08       218.66\n5037   94067   94081  6105.73        60.28\n5038   94069   94081  4535.03       102.04\n5039   94076   94081  2567.25      1067.62\n5040   94080   94081  7277.43       120.11\n5041   94081   94081        0      9257.91\n\n\nThe function spflow_network_multi() combines information on the nodes and the node-pairs and also ensures that both data sources are consistent.\n\nparis10km_multinet &lt;- spflow_network_multi(paris10km_net,paris10km_net_pairs)\nparis10km_multinet\n\nCollection of spatial network nodes and pairs\n--------------------------------------------------\nContains 1 spatial network nodes  \n    With id :  paris\nContains 1 spatial network pairs  \n    With id :  paris_paris\n\nAvailability of origin-destination pair information:\n\n ID_ORIG_NET ID_DEST_NET ID_NET_PAIR COMPLETENESS   C_PAIRS C_ORIG C_DEST\n       paris       paris paris_paris      100.00% 5041/5041  71/71  71/71\n\n\nGiven the information on origins, destinations and OD pairs we can use the spflow_map() method for a simple geographic representation of the largest flows.\n\nplot(paris10km_municipalities$geometry) \nspflow_map(\n  paris10km_multinet,\n  flow_var = \"COMMUTE_FLOW\",\n  add = TRUE,         \n  legend_position = \"bottomleft\",\n  filter_lowest = .95,\n  remove_intra = TRUE,\n  cex = 1)\n\n\n\n\nBefore estimating a model we should investigate the correlation structure of the input data. The pair_cor() method creates a correlation matrix, which we can represent using the cor_image(). The formula is used clarify which variables should be included in the correlation matrix.\n\ncor_formula &lt;- log(1 + COMMUTE_FLOW) ~ . + P_(log( 1 + DISTANCE))\ncor_mat &lt;- pair_cor(paris10km_multinet, spflow_formula = cor_formula, add_lags_x = FALSE)\n\ncolnames(cor_mat) &lt;- paste0(substr(colnames(cor_mat),1,3),\"...\")\ncor_image(cor_mat)\n\n\n\n\n\n\n\n\nThe core function of the package is spflow(), which provides an interface to four different estimators of the spatial econometric interaction model.\n\n\nEstimation with default settings requires two arguments: a spflow_network_multi-class and a spflow_formula. The spflow_formula specifies the model we want to estimate. In this example, the dependent variable is a transformation of commuting flows and we use the do- shortcut to indicate that all available variables should be included in the model. Using the defaults leads to the most comprehensive spatial interaction model, which includes spatial lags of the dependent variable, the exogenous variables and additional attributes for intra-regional observations.\n\nresults_default &lt;- spflow(\n  spflow_formula = log(1 + COMMUTE_FLOW) ~ . + P_(log( 1 + DISTANCE)),\n  spflow_networks = paris10km_multinet)\n\nresults_default\n\n--------------------------------------------------\nSpatial interaction model estimated by: MLE  \nSpatial correlation structure: SDM (model_9)\nDependent variable: log(1 + COMMUTE_FLOW)\n\n--------------------------------------------------\nCoefficients:\n                        est     sd   t.stat  p.val\nrho_d                 0.439  0.016   27.834  0.000\nrho_o                 0.796  0.010   82.408  0.000\nrho_w                -0.372  0.020  -18.226  0.000\n(Intercept)          -0.158  0.073   -2.168  0.030\n(Intra)               6.179  0.296   20.908  0.000\nD_POPULATION          0.000  0.000    4.254  0.000\nD_POPULATION.lag1     0.000  0.000   -0.170  0.865\nD_MED_INCOME          0.000  0.000   -2.422  0.015\nD_MED_INCOME.lag1     0.000  0.000    6.300  0.000\nD_NB_COMPANY          0.000  0.000    2.437  0.015\nD_NB_COMPANY.lag1     0.000  0.000    2.542  0.011\nD_AREA                0.000  0.000    6.368  0.000\nD_AREA.lag1           0.000  0.000   -4.319  0.000\nO_POPULATION          0.000  0.000   22.902  0.000\nO_POPULATION.lag1     0.000  0.000   -5.740  0.000\nO_MED_INCOME          0.000  0.000    0.609  0.542\nO_MED_INCOME.lag1     0.000  0.000    0.192  0.848\nO_NB_COMPANY          0.000  0.000   -5.610  0.000\nO_NB_COMPANY.lag1     0.000  0.000    1.050  0.294\nO_AREA                0.000  0.000    6.278  0.000\nO_AREA.lag1           0.000  0.000   -4.951  0.000\nI_POPULATION          0.000  0.000   -3.538  0.000\nI_MED_INCOME          0.000  0.000   -8.722  0.000\nI_NB_COMPANY          0.000  0.000    4.787  0.000\nI_AREA               -0.001  0.000   -5.401  0.000\nP_log(1 + DISTANCE)      NA     NA       NA     NA\n\n--------------------------------------------------\nR2_corr: 0.9110008  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nWe can adjust how the exogenous variables are to be used by wrapping them into the D_(), O_(), I_() and P_() functions. The variables in P_() are used as OD pair features and those in D_(), O_() and I_() are used as destination, origin and intra-regional features. We can take advantage of the formula interface to specify transformations and expand factor variables to dummies.\n\nclog &lt;- function(x) {\n  log_x &lt;- log(x)\n  log_x - mean(log_x)\n}\n\nspflow_formula  &lt;- \n  log(COMMUTE_FLOW + 1) ~\n  D_(log(NB_COMPANY) + clog(MED_INCOME)) +\n  O_(log(POPULATION) + clog(MED_INCOME)) +\n  I_(log(POPULATION)) +\n  P_(log(DISTANCE + 1))\n\nresults_mle  &lt;- spflow(\n  spflow_formula,\n  paris10km_multinet)\nresults_mle\n\n--------------------------------------------------\nSpatial interaction model estimated by: MLE  \nSpatial correlation structure: SDM (model_9)\nDependent variable: log(COMMUTE_FLOW + 1)\n\n--------------------------------------------------\nCoefficients:\n                            est     sd   t.stat  p.val\nrho_d                     0.213  0.019   11.150  0.000\nrho_o                     0.726  0.012   59.127  0.000\nrho_w                    -0.022  0.024   -0.904  0.366\n(Intercept)              -0.809  0.289   -2.796  0.005\n(Intra)                   6.829  0.893    7.646  0.000\nD_log(NB_COMPANY)         0.285  0.016   18.075  0.000\nD_log(NB_COMPANY).lag1   -0.220  0.022  -10.099  0.000\nD_clog(MED_INCOME)       -0.343  0.051   -6.663  0.000\nD_clog(MED_INCOME).lag1   0.509  0.073    6.959  0.000\nO_log(POPULATION)         0.763  0.021   36.230  0.000\nO_log(POPULATION).lag1   -0.649  0.031  -21.085  0.000\nO_clog(MED_INCOME)       -0.081  0.050   -1.631  0.103\nO_clog(MED_INCOME).lag1  -0.006  0.066   -0.094  0.925\nI_log(POPULATION)        -0.422  0.082   -5.178  0.000\nP_log(DISTANCE + 1)      -0.073  0.022   -3.257  0.001\n\n--------------------------------------------------\nR2_corr: 0.9207571  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nMore fine-grained adjustments are possible via the spflow_control argument. Here we change the estimation method and the way we want to model the spatial autoregression in the flows. To use spatial lags only for certain variables, we need to specify them as a second formula.\n\nsdm_formula &lt;- ~\n  O_(log(POPULATION) + clog(MED_INCOME)) +\n  D_(log(NB_COMPANY) + clog(MED_INCOME))\n\ncntrl &lt;- spflow_control(\n  estimation_method = \"mcmc\",\n  sdm_variables = sdm_formula,\n  model = \"model_7\")\n\nresults_mcmc  &lt;- spflow(\n  spflow_formula,\n  paris10km_multinet,\n  estimation_control = cntrl)\n\nresults_mcmc\n\n--------------------------------------------------\nSpatial interaction model estimated by: MCMC  \nSpatial correlation structure: SDM (model_7)\nDependent variable: log(COMMUTE_FLOW + 1)\n\n--------------------------------------------------\nCoefficients:\n                            est  quant_025  quant_975     sd\nrho_d                     0.201      0.171      0.229  0.014\nrho_o                     0.721      0.698      0.745  0.012\n(Intercept)              -0.772     -1.349     -0.226  0.284\n(Intra)                   6.816      5.045      8.553  0.903\nD_log(NB_COMPANY)         0.291      0.261      0.320  0.016\nD_log(NB_COMPANY).lag1   -0.231     -0.267     -0.194  0.019\nD_clog(MED_INCOME)       -0.348     -0.448     -0.249  0.051\nD_clog(MED_INCOME).lag1   0.511      0.366      0.649  0.071\nO_log(POPULATION)         0.774      0.741      0.806  0.017\nO_log(POPULATION).lag1   -0.664     -0.710     -0.617  0.024\nO_clog(MED_INCOME)       -0.083     -0.185      0.017  0.051\nO_clog(MED_INCOME).lag1  -0.004     -0.133      0.125  0.066\nI_log(POPULATION)        -0.420     -0.581     -0.259  0.083\nP_log(DISTANCE + 1)      -0.070     -0.112     -0.026  0.022\n\n--------------------------------------------------\nR2_corr: 0.9205607  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nCalling plot(results_mcmc) would create a whole sequence of graphics that allow to diagnose the fit. Here we concentrate on a selection of these graphics. The pairwise correlations of the model data show, for example, that the residuals and their spatial lags are not correlated with the explanatory variables.\n\nres_corr &lt;- pair_cor(results_mcmc)\ncolnames(res_corr) &lt;- substr(colnames(res_corr),1,3)\ncor_image(res_corr)\n\n\n\n\nWe can also create Moran scatter plots to check whether the residuals still exhibit spatial autocorrelation with respect to the three potential neighborhood matrices \\(W_d\\), \\(W_o\\), & \\(W_w\\).\n\nold_par &lt;- par(mfrow = c(1, 3), mar = c(2,2,2,2))\nspflow_moran_plots(results_mcmc)\n\n\n\npar(old_par)\n\nA quick investigation of the 2% residuals with largest magnitude reveals that long distances seem to be predicted with lower precision.\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  flow_type = \"fitted\",\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  flow_type = \"actual\",\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\nLooking at the relation between the distances and the error confirms this impression. A more complex model could account for the increasing variance by weighting the observations during the estimation. This could be achieved using the weight_variable option in spflow_control(), but is left out in this introductory vignette.\n\nplot(log(dat(paris10km_multinet, \"paris_paris\")[[\"DISTANCE\"]] + 1), resid(results_mcmc))\n\n\n\n\n\n\n\n\nFinally we can evaluate the impact certain characteristics have on the outcome. Here we look at a scenario where the population in the central municipality is increased by 10%. As this has diverse effects on all flows we will first look at an image of the effect matrix.\n\ncenter_mun &lt;- \"75101\"\nchange_paris &lt;- dat(paris10km_multinet, \"paris\")\nchange_paris &lt;- change_paris[change_paris$ID_MUN == center_mun,]\nchange_paris[,\"POPULATION\"] &lt;- change_paris[,\"POPULATION\"]*1.1\nchange_paris &lt;- change_paris[,1:2] # keep the ID and the variable that changed\n\neffect_matrix &lt;- predict_effect(\n  results_mcmc,                           # the model\n  new_dat = list(\"paris\" = change_paris), # changes in network \"paris\"\n  return_type = \"M\")                      # return in matrix form\n  \n# in the first row are those flows that go to the center\n# in the first column are those flows that start from the center\nimage(effect_matrix)\n\n\n\n\nHere we see that flows starting from the center increase and flows that start from neighbors of the center to the center decrease. All other effects are very small. We can then have a closer look at the flows that start from the center or go to it. Additionally we look at all the internal flows, which decrease for all municipalities except for the center.\n\nplot(cbind(\"FLOWS_FROM_CENTER\" = effect_matrix[,1], paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"FLOWS_TO_CENTER\" = effect_matrix[1,], paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"INTRA_FLOWS\" = diag(effect_matrix), paris10km_municipalities[\"geometry\"]))\n\n\n\n\nWe can then look at the indirect effects on all flows that do not have the central municipality as origin or destination. To summarize these by total inflow and outflow we additionally set the internal flows to zero.\n\neffect_matrix2 &lt;- effect_matrix\ndiag(effect_matrix2) &lt;- effect_matrix2[1,] &lt;- effect_matrix2[,1] &lt;- 0\nplot(cbind(\"TOTAL_OUTFLOWS\" = rowSums(effect_matrix2), paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"TOTAL_INFLOWS\" = colSums(effect_matrix2), paris10km_municipalities[\"geometry\"]))"
  },
  {
    "objectID": "spflow.html#overview",
    "href": "spflow.html#overview",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "This exercise uses spflow package for modeling spatial interactions using the example of home-to-work commuting flows. We will use information on the 71 municipalities that are located closest to the center of Paris. This data is contained in the package and was originally diffused by the French National Institutes of Statistics and Economic Studies (INSEE), and of Geographic and Forest Information (IGN)."
  },
  {
    "objectID": "spflow.html#importing-packages-and-datasets",
    "href": "spflow.html#importing-packages-and-datasets",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "pacman::p_load(Matrix,sf,spdep,tmap)\n#devtools::install_github(\"LukeCe/spflow\")\nlibrary(spflow)\n\ndata(\"paris10km_municipalities\")\ndata(\"paris10km_commuteflows\")\n\nEach municipality is identified by a unique id. Additionally, we have information on the population, the median income and the number of companies.\n\nparis10km_municipalities\n\nSimple feature collection with 71 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.199785 ymin: 48.76454 xmax: 2.499719 ymax: 48.95192\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ID_MUN POPULATION MED_INCOME NB_COMPANY AREA                       geometry\n1   75101      17100   31842.56      14333  182 MULTIPOLYGON (((2.344559 48...\n2   75102      22390   30024.50      14478   99 MULTIPOLYGON (((2.347832 48...\n3   75103      35991   30988.00      10696  117 MULTIPOLYGON (((2.350091 48...\n4   75104      27769   30514.67       7412  160 MULTIPOLYGON (((2.344559 48...\n5   75105      60179   32950.00      10290  252 MULTIPOLYGON (((2.344559 48...\n6   75106      43224   38447.69      10620  215 MULTIPOLYGON (((2.344559 48...\n7   75107      57092   41949.00      12602  412 MULTIPOLYGON (((2.320777 48...\n8   75108      38749   39774.00      52237  386 MULTIPOLYGON (((2.327121 48...\n9   75109      59474   32771.00      23687  218 MULTIPOLYGON (((2.325762 48...\n10  75110      94474   25154.00      23996  288 MULTIPOLYGON (((2.364678 48...\n\n\n\ntm_shape(paris10km_municipalities)+\n  tm_fill(\"ID_MUN\", \n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\")) +\n  tm_borders(col = \"white\")+\n  tm_layout(main.title = \"Paris Municipalities\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2) +\n  tm_grid(labels.size = 1,alpha =0.2)\n\n\n\n\nThere are three different neighborhood matrices that can be used to describe the connectivity between the municipalities.\n\nold_par &lt;- par(mfrow = c(1, 3), mar = c(0,0,1,0))\n\nmid_points &lt;- suppressWarnings({\n    st_point_on_surface(st_geometry(paris10km_municipalities))})\n\nparis10km_nb &lt;- list(\n  \"by_contiguity\" = spdep::poly2nb(paris10km_municipalities),\n  \"by_distance\" = spdep::dnearneigh(mid_points,d1 = 0, d2 = 5),\n  \"by_knn\" = spdep::knn2nb(knearneigh(mid_points,3))\n)\n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_contiguity, mid_points, add = T, col = rgb(0,0,0,alpha=0.5))\ntitle(\"Contiguity\") \n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_distance,mid_points, add = T, col = rgb(0,0,0,alpha=0.5)) \ntitle(\"Distance\") \n\nplot(st_geometry(paris10km_municipalities))\nplot(paris10km_nb$by_knn, mid_points, add = T, col = rgb(0,0,0,alpha=0.5))\ntitle(\"3 Nearest Neighbors\") \n\n\n\npar(old_par)\n\nFinally, there is data on the size of the commuting flows and the distance between all pairs of municipalities\n\nhead(paris10km_commuteflows)\n\n  ID_ORIG ID_DEST DISTANCE COMMUTE_FLOW\n1   75101   75101    0.000   3771.23556\n2   75101   75102  786.743    294.76899\n3   75101   75103 1729.063     71.25116\n4   75101   75104 1807.294     99.38468\n5   75101   75105 2266.598     98.88915\n6   75101   75106 1512.870     65.15406"
  },
  {
    "objectID": "spflow.html#modeling-spatial-interactions-with-spflow",
    "href": "spflow.html#modeling-spatial-interactions-with-spflow",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "The spflow package builds on the idea that flows correspond to pairwise interactions between the nodes of an origin network with the nodes of a destination network.\nIn our example, the origin and destination networks are the same because every municipality is both an origin and destination of a flow.\nTo estimate the model efficiently, the spflow package uses moment-based estimation methods, that exploit the relational structure of flow data. This avoids duplication arising from the fact that each municipality is at the origin and destination of many flows. For more details on the model and the estimation methods see LeSage (2008), Dargel (2021) and Dargel (2022).\n\n\nTo describe the nodes of a network the package provides spflow_network-class that combines attributes of the nodes with the chosen network structure. For our model we choose the contiguity based neighborhood structure.\nFirst, we will create an object paris10km_net using sp_network_nodes() function. This object represents the nodes in our network. Each municipality will serve as a node.\n\nparis10km_net &lt;-  spflow_network(\n  id_net = \"paris\",\n  node_neighborhood = nb2mat(paris10km_nb$by_contiguity),\n  node_data = paris10km_municipalities,\n  node_key_column = \"ID_MUN\")\n\nparis10km_net\n\nSpatial network nodes with id: paris\n--------------------------------------------------\nNumber of nodes: 71\nAverage number of links per node: 5.239\nDensity of the neighborhood matrix: 7.38% (non-zero connections)\n\nData on nodes:\n    ID_MUN POPULATION MED_INCOME NB_COMPANY AREA COORD_X COORD_Y\n1    75101      17100   31842.56      14333  182    2.34   48.86\n2    75102      22390    30024.5      14478   99    2.34   48.87\n3    75103      35991      30988      10696  117    2.36   48.86\n4    75104      27769   30514.67       7412  160    2.36   48.85\n5    75105      60179      32950      10290  252    2.35   48.85\n6    75106      43224   38447.69      10620  215    2.33   48.85\n---    ---        ---        ---        ---  ---     ---     ---\n66   94046      54186      24329       3385  537    2.44    48.8\n67   94067      21846   31559.38       1763   90    2.42   48.84\n68   94069      14870   25790.65        957  144    2.43   48.82\n69   94076      56504      19447       2690  529    2.36   48.79\n70   94080      49831      30798       4655  191    2.43   48.85\n71   94081      88102    17860.5       4467 1166     2.4   48.79\n\n\nNext, we will create an object paris10km_pairs using sp_network_pair() function. This object represents the edges in our network. Each origin-destination record from paris10km_commuteflows will serve as an edge.\n\nparis10km_net_pairs &lt;-  spflow_network_pair(\n  id_orig_net = \"paris\",\n  id_dest_net = \"paris\",\n  pair_data = paris10km_commuteflows,\n  orig_key_column = \"ID_ORIG\",\n  dest_key_column = \"ID_DEST\")\nparis10km_net_pairs\n\nSpatial network pair with id: paris_paris\n--------------------------------------------------\nOrigin network id: paris (with 71 nodes)\nDestination network id: paris (with 71 nodes)\nNumber of pairs: 5041\nCompleteness of pairs: 100.00% (5041/5041)\n\nData on node-pairs:\n     ID_DEST ID_ORIG DISTANCE COMMUTE_FLOW\n1      75101   75101        0      3771.24\n2      75102   75101   786.74       294.77\n3      75103   75101  1729.06        71.25\n4      75104   75101  1807.29        99.38\n5      75105   75101   2266.6        98.89\n6      75106   75101  1512.87        65.15\n---      ---     ---      ---          ---\n5036   94046   94081  3742.08       218.66\n5037   94067   94081  6105.73        60.28\n5038   94069   94081  4535.03       102.04\n5039   94076   94081  2567.25      1067.62\n5040   94080   94081  7277.43       120.11\n5041   94081   94081        0      9257.91\n\n\nThe function spflow_network_multi() combines information on the nodes and the node-pairs and also ensures that both data sources are consistent.\n\nparis10km_multinet &lt;- spflow_network_multi(paris10km_net,paris10km_net_pairs)\nparis10km_multinet\n\nCollection of spatial network nodes and pairs\n--------------------------------------------------\nContains 1 spatial network nodes  \n    With id :  paris\nContains 1 spatial network pairs  \n    With id :  paris_paris\n\nAvailability of origin-destination pair information:\n\n ID_ORIG_NET ID_DEST_NET ID_NET_PAIR COMPLETENESS   C_PAIRS C_ORIG C_DEST\n       paris       paris paris_paris      100.00% 5041/5041  71/71  71/71\n\n\nGiven the information on origins, destinations and OD pairs we can use the spflow_map() method for a simple geographic representation of the largest flows.\n\nplot(paris10km_municipalities$geometry) \nspflow_map(\n  paris10km_multinet,\n  flow_var = \"COMMUTE_FLOW\",\n  add = TRUE,         \n  legend_position = \"bottomleft\",\n  filter_lowest = .95,\n  remove_intra = TRUE,\n  cex = 1)\n\n\n\n\nBefore estimating a model we should investigate the correlation structure of the input data. The pair_cor() method creates a correlation matrix, which we can represent using the cor_image(). The formula is used clarify which variables should be included in the correlation matrix.\n\ncor_formula &lt;- log(1 + COMMUTE_FLOW) ~ . + P_(log( 1 + DISTANCE))\ncor_mat &lt;- pair_cor(paris10km_multinet, spflow_formula = cor_formula, add_lags_x = FALSE)\n\ncolnames(cor_mat) &lt;- paste0(substr(colnames(cor_mat),1,3),\"...\")\ncor_image(cor_mat)"
  },
  {
    "objectID": "spflow.html#estimation",
    "href": "spflow.html#estimation",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "The core function of the package is spflow(), which provides an interface to four different estimators of the spatial econometric interaction model.\n\n\nEstimation with default settings requires two arguments: a spflow_network_multi-class and a spflow_formula. The spflow_formula specifies the model we want to estimate. In this example, the dependent variable is a transformation of commuting flows and we use the do- shortcut to indicate that all available variables should be included in the model. Using the defaults leads to the most comprehensive spatial interaction model, which includes spatial lags of the dependent variable, the exogenous variables and additional attributes for intra-regional observations.\n\nresults_default &lt;- spflow(\n  spflow_formula = log(1 + COMMUTE_FLOW) ~ . + P_(log( 1 + DISTANCE)),\n  spflow_networks = paris10km_multinet)\n\nresults_default\n\n--------------------------------------------------\nSpatial interaction model estimated by: MLE  \nSpatial correlation structure: SDM (model_9)\nDependent variable: log(1 + COMMUTE_FLOW)\n\n--------------------------------------------------\nCoefficients:\n                        est     sd   t.stat  p.val\nrho_d                 0.439  0.016   27.834  0.000\nrho_o                 0.796  0.010   82.408  0.000\nrho_w                -0.372  0.020  -18.226  0.000\n(Intercept)          -0.158  0.073   -2.168  0.030\n(Intra)               6.179  0.296   20.908  0.000\nD_POPULATION          0.000  0.000    4.254  0.000\nD_POPULATION.lag1     0.000  0.000   -0.170  0.865\nD_MED_INCOME          0.000  0.000   -2.422  0.015\nD_MED_INCOME.lag1     0.000  0.000    6.300  0.000\nD_NB_COMPANY          0.000  0.000    2.437  0.015\nD_NB_COMPANY.lag1     0.000  0.000    2.542  0.011\nD_AREA                0.000  0.000    6.368  0.000\nD_AREA.lag1           0.000  0.000   -4.319  0.000\nO_POPULATION          0.000  0.000   22.902  0.000\nO_POPULATION.lag1     0.000  0.000   -5.740  0.000\nO_MED_INCOME          0.000  0.000    0.609  0.542\nO_MED_INCOME.lag1     0.000  0.000    0.192  0.848\nO_NB_COMPANY          0.000  0.000   -5.610  0.000\nO_NB_COMPANY.lag1     0.000  0.000    1.050  0.294\nO_AREA                0.000  0.000    6.278  0.000\nO_AREA.lag1           0.000  0.000   -4.951  0.000\nI_POPULATION          0.000  0.000   -3.538  0.000\nI_MED_INCOME          0.000  0.000   -8.722  0.000\nI_NB_COMPANY          0.000  0.000    4.787  0.000\nI_AREA               -0.001  0.000   -5.401  0.000\nP_log(1 + DISTANCE)      NA     NA       NA     NA\n\n--------------------------------------------------\nR2_corr: 0.9110008  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nWe can adjust how the exogenous variables are to be used by wrapping them into the D_(), O_(), I_() and P_() functions. The variables in P_() are used as OD pair features and those in D_(), O_() and I_() are used as destination, origin and intra-regional features. We can take advantage of the formula interface to specify transformations and expand factor variables to dummies.\n\nclog &lt;- function(x) {\n  log_x &lt;- log(x)\n  log_x - mean(log_x)\n}\n\nspflow_formula  &lt;- \n  log(COMMUTE_FLOW + 1) ~\n  D_(log(NB_COMPANY) + clog(MED_INCOME)) +\n  O_(log(POPULATION) + clog(MED_INCOME)) +\n  I_(log(POPULATION)) +\n  P_(log(DISTANCE + 1))\n\nresults_mle  &lt;- spflow(\n  spflow_formula,\n  paris10km_multinet)\nresults_mle\n\n--------------------------------------------------\nSpatial interaction model estimated by: MLE  \nSpatial correlation structure: SDM (model_9)\nDependent variable: log(COMMUTE_FLOW + 1)\n\n--------------------------------------------------\nCoefficients:\n                            est     sd   t.stat  p.val\nrho_d                     0.213  0.019   11.150  0.000\nrho_o                     0.726  0.012   59.127  0.000\nrho_w                    -0.022  0.024   -0.904  0.366\n(Intercept)              -0.809  0.289   -2.796  0.005\n(Intra)                   6.829  0.893    7.646  0.000\nD_log(NB_COMPANY)         0.285  0.016   18.075  0.000\nD_log(NB_COMPANY).lag1   -0.220  0.022  -10.099  0.000\nD_clog(MED_INCOME)       -0.343  0.051   -6.663  0.000\nD_clog(MED_INCOME).lag1   0.509  0.073    6.959  0.000\nO_log(POPULATION)         0.763  0.021   36.230  0.000\nO_log(POPULATION).lag1   -0.649  0.031  -21.085  0.000\nO_clog(MED_INCOME)       -0.081  0.050   -1.631  0.103\nO_clog(MED_INCOME).lag1  -0.006  0.066   -0.094  0.925\nI_log(POPULATION)        -0.422  0.082   -5.178  0.000\nP_log(DISTANCE + 1)      -0.073  0.022   -3.257  0.001\n\n--------------------------------------------------\nR2_corr: 0.9207571  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nMore fine-grained adjustments are possible via the spflow_control argument. Here we change the estimation method and the way we want to model the spatial autoregression in the flows. To use spatial lags only for certain variables, we need to specify them as a second formula.\n\nsdm_formula &lt;- ~\n  O_(log(POPULATION) + clog(MED_INCOME)) +\n  D_(log(NB_COMPANY) + clog(MED_INCOME))\n\ncntrl &lt;- spflow_control(\n  estimation_method = \"mcmc\",\n  sdm_variables = sdm_formula,\n  model = \"model_7\")\n\nresults_mcmc  &lt;- spflow(\n  spflow_formula,\n  paris10km_multinet,\n  estimation_control = cntrl)\n\nresults_mcmc\n\n--------------------------------------------------\nSpatial interaction model estimated by: MCMC  \nSpatial correlation structure: SDM (model_7)\nDependent variable: log(COMMUTE_FLOW + 1)\n\n--------------------------------------------------\nCoefficients:\n                            est  quant_025  quant_975     sd\nrho_d                     0.201      0.171      0.229  0.014\nrho_o                     0.721      0.698      0.745  0.012\n(Intercept)              -0.772     -1.349     -0.226  0.284\n(Intra)                   6.816      5.045      8.553  0.903\nD_log(NB_COMPANY)         0.291      0.261      0.320  0.016\nD_log(NB_COMPANY).lag1   -0.231     -0.267     -0.194  0.019\nD_clog(MED_INCOME)       -0.348     -0.448     -0.249  0.051\nD_clog(MED_INCOME).lag1   0.511      0.366      0.649  0.071\nO_log(POPULATION)         0.774      0.741      0.806  0.017\nO_log(POPULATION).lag1   -0.664     -0.710     -0.617  0.024\nO_clog(MED_INCOME)       -0.083     -0.185      0.017  0.051\nO_clog(MED_INCOME).lag1  -0.004     -0.133      0.125  0.066\nI_log(POPULATION)        -0.420     -0.581     -0.259  0.083\nP_log(DISTANCE + 1)      -0.070     -0.112     -0.026  0.022\n\n--------------------------------------------------\nR2_corr: 0.9205607  \nObservations: 5041  \nModel coherence: Validated\n\n\n\n\n\nCalling plot(results_mcmc) would create a whole sequence of graphics that allow to diagnose the fit. Here we concentrate on a selection of these graphics. The pairwise correlations of the model data show, for example, that the residuals and their spatial lags are not correlated with the explanatory variables.\n\nres_corr &lt;- pair_cor(results_mcmc)\ncolnames(res_corr) &lt;- substr(colnames(res_corr),1,3)\ncor_image(res_corr)\n\n\n\n\nWe can also create Moran scatter plots to check whether the residuals still exhibit spatial autocorrelation with respect to the three potential neighborhood matrices \\(W_d\\), \\(W_o\\), & \\(W_w\\).\n\nold_par &lt;- par(mfrow = c(1, 3), mar = c(2,2,2,2))\nspflow_moran_plots(results_mcmc)\n\n\n\npar(old_par)\n\nA quick investigation of the 2% residuals with largest magnitude reveals that long distances seem to be predicted with lower precision.\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  flow_type = \"fitted\",\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\n\nplot(paris10km_municipalities$geometry)\nspflow_map(\n  results_mcmc,\n  flow_type = \"actual\",\n  add = TRUE,\n  legend_position = \"bottomleft\",\n  filter_lowest = .98, # concentrate on the 2% largest (in magnitude)\n  cex = 1) \n\n\n\n\nLooking at the relation between the distances and the error confirms this impression. A more complex model could account for the increasing variance by weighting the observations during the estimation. This could be achieved using the weight_variable option in spflow_control(), but is left out in this introductory vignette.\n\nplot(log(dat(paris10km_multinet, \"paris_paris\")[[\"DISTANCE\"]] + 1), resid(results_mcmc))"
  },
  {
    "objectID": "spflow.html#evaluating-the-impact-of-changing-the-input-data",
    "href": "spflow.html#evaluating-the-impact-of-changing-the-input-data",
    "title": "Spatial Econometric Interaction Modelling",
    "section": "",
    "text": "Finally we can evaluate the impact certain characteristics have on the outcome. Here we look at a scenario where the population in the central municipality is increased by 10%. As this has diverse effects on all flows we will first look at an image of the effect matrix.\n\ncenter_mun &lt;- \"75101\"\nchange_paris &lt;- dat(paris10km_multinet, \"paris\")\nchange_paris &lt;- change_paris[change_paris$ID_MUN == center_mun,]\nchange_paris[,\"POPULATION\"] &lt;- change_paris[,\"POPULATION\"]*1.1\nchange_paris &lt;- change_paris[,1:2] # keep the ID and the variable that changed\n\neffect_matrix &lt;- predict_effect(\n  results_mcmc,                           # the model\n  new_dat = list(\"paris\" = change_paris), # changes in network \"paris\"\n  return_type = \"M\")                      # return in matrix form\n  \n# in the first row are those flows that go to the center\n# in the first column are those flows that start from the center\nimage(effect_matrix)\n\n\n\n\nHere we see that flows starting from the center increase and flows that start from neighbors of the center to the center decrease. All other effects are very small. We can then have a closer look at the flows that start from the center or go to it. Additionally we look at all the internal flows, which decrease for all municipalities except for the center.\n\nplot(cbind(\"FLOWS_FROM_CENTER\" = effect_matrix[,1], paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"FLOWS_TO_CENTER\" = effect_matrix[1,], paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"INTRA_FLOWS\" = diag(effect_matrix), paris10km_municipalities[\"geometry\"]))\n\n\n\n\nWe can then look at the indirect effects on all flows that do not have the central municipality as origin or destination. To summarize these by total inflow and outflow we additionally set the internal flows to zero.\n\neffect_matrix2 &lt;- effect_matrix\ndiag(effect_matrix2) &lt;- effect_matrix2[1,] &lt;- effect_matrix2[,1] &lt;- 0\nplot(cbind(\"TOTAL_OUTFLOWS\" = rowSums(effect_matrix2), paris10km_municipalities[\"geometry\"]))\n\n\n\n\n\nplot(cbind(\"TOTAL_INFLOWS\" = colSums(effect_matrix2), paris10km_municipalities[\"geometry\"]))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07.html",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\nFirstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\n\n\n\n\n\n\n\nReflection\n\n\n\noslrr is a good package that compiles multiple important functions for model diagnostics. However, the package is specifically prepared for OLS models, so it cannot be applied to other regression models such as simple linear regression, multiple linear regression or binary logistic regression.\n\n\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot, ggstatplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nCreating publication ready HTML tables\n\nvtable, tableHTML\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, ggstatsplot, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")\n\n\n\n\n\n\nWe use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nWe can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\n#tmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\nIn this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\nAnother way to visualise more sophisticated correlation matrix is to use ggstatsplot package. Particularly, ggcormat() function can be used to create the correlation matrix with details from statistical tests included in the plots themselves. The resulting plot provides a visual and statistical summary of the relationships between the variables in the selected columns of the data frame. It's a powerful tool for quickly understanding complex multivariate data.\n\nset.seed(123)\nggcorrmat(\n  data = condo_resale[, 5:23],  \n          matrix.type = \"upper\",\n  type = \"parametric\",\n  tr = 0.2,\n  partial = FALSE,\n  k = 2L,\n  sig.level = 0.05,\n  conf.level = 0.95,\n  bf.prior = 0.707,\n  ggcorrplot.args = list(\n     tl.cex = 10,\n     pch.cex = 5,\n     lab_size = 3\n  )) + \n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(\n      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above !\n\ndata = condo_resale[, 5:23]: This specifies the data to be used for the correlation matrix. It selects columns 5 to 23 from the condo_resale data frame.\nmatrix.type = \"upper\": This argument specifies that only the upper triangle of the correlation matrix should be displayed.\ntype = \"parametric\": This specifies the type of correlation coefficient to be computed. In this case, it's a parametric correlation.\ntr = 0.2: This is the transparency level for the correlation matrix.\npartial = FALSE: This indicates that partial correlations should not be computed.\nk = 2L: This specifies the number of decimal places to be used when displaying the correlation coefficients.\nsig.level = 0.05: This sets the significance level for the correlation coefficients.\nconf.level = 0.95: This sets the confidence level for the correlation coefficients.\nbf.prior = 0.707: This sets the prior for the Bayes factor computation.\nggcorrplot.args = list(tl.cex = 10, pch.cex = 5, lab_size = 3): These are additional arguments passed to the ggcorrplot function, which controls the size of the text labels, points, and label size.\nggplot2::theme(axis.text.x = ggplot2::element_text(margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\"))): This is a theme setting from the ggplot2 package that adjusts the margin around the x-axis text.\n\n\n\n\n\n\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\ntbl_regression(condo.mlr1,\n               intercept = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nThe function ggcoefstats() generates dot-and-whisker plots for regression models saved in a tidy data frame. The tidy data frames are prepared using parameters::model_parameters(). Additionally, if available, the model summary indices are also extracted from performance::model_performance().\n\nmlr.p &lt;- ggcoefstats(condo.mlr1)\nmlr.p\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can also use sort = argument to specify whether we want to sort the coefficient estimates in \"ascending\" or \"descending\" order. By default, sort = NULL is specified and no ordering is implemented.\n\n\n\n\n\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n\n\n\n\nAfter exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 15:17:04.894653 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 15:17:06.376575 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 15:17:06.377451 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 15:17:08.020667 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\nNext, we will visualise the coefficient estimates\n\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n            palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n            size = 0.15,\n            border.col = \"gray20\",\n            border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#overview",
    "href": "In-class_Ex/In-class_Ex07.html#overview",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-datasets-and-packages",
    "href": "In-class_Ex/In-class_Ex07.html#importing-datasets-and-packages",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "Firstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\n\n\n\n\n\n\n\nReflection\n\n\n\noslrr is a good package that compiles multiple important functions for model diagnostics. However, the package is specifically prepared for OLS models, so it cannot be applied to other regression models such as simple linear regression, multiple linear regression or binary logistic regression.\n\n\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot, ggstatplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nCreating publication ready HTML tables\n\nvtable, tableHTML\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, ggstatsplot, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex07.html#data-wrangling",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "We use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#exploratory-data-analysis-eda",
    "href": "In-class_Ex/In-class_Ex07.html#exploratory-data-analysis-eda",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "We can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\n#tmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#hedonic-pricing-modelling-in-r",
    "href": "In-class_Ex/In-class_Ex07.html#hedonic-pricing-modelling-in-r",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "In this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\nAnother way to visualise more sophisticated correlation matrix is to use ggstatsplot package. Particularly, ggcormat() function can be used to create the correlation matrix with details from statistical tests included in the plots themselves. The resulting plot provides a visual and statistical summary of the relationships between the variables in the selected columns of the data frame. It's a powerful tool for quickly understanding complex multivariate data.\n\nset.seed(123)\nggcorrmat(\n  data = condo_resale[, 5:23],  \n          matrix.type = \"upper\",\n  type = \"parametric\",\n  tr = 0.2,\n  partial = FALSE,\n  k = 2L,\n  sig.level = 0.05,\n  conf.level = 0.95,\n  bf.prior = 0.707,\n  ggcorrplot.args = list(\n     tl.cex = 10,\n     pch.cex = 5,\n     lab_size = 3\n  )) + \n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(\n      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLet’s breakdown the code chunk above !\n\ndata = condo_resale[, 5:23]: This specifies the data to be used for the correlation matrix. It selects columns 5 to 23 from the condo_resale data frame.\nmatrix.type = \"upper\": This argument specifies that only the upper triangle of the correlation matrix should be displayed.\ntype = \"parametric\": This specifies the type of correlation coefficient to be computed. In this case, it's a parametric correlation.\ntr = 0.2: This is the transparency level for the correlation matrix.\npartial = FALSE: This indicates that partial correlations should not be computed.\nk = 2L: This specifies the number of decimal places to be used when displaying the correlation coefficients.\nsig.level = 0.05: This sets the significance level for the correlation coefficients.\nconf.level = 0.95: This sets the confidence level for the correlation coefficients.\nbf.prior = 0.707: This sets the prior for the Bayes factor computation.\nggcorrplot.args = list(tl.cex = 10, pch.cex = 5, lab_size = 3): These are additional arguments passed to the ggcorrplot function, which controls the size of the text labels, points, and label size.\nggplot2::theme(axis.text.x = ggplot2::element_text(margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\"))): This is a theme setting from the ggplot2 package that adjusts the margin around the x-axis text.\n\n\n\n\n\n\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\ntbl_regression(condo.mlr1,\n               intercept = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nThe function ggcoefstats() generates dot-and-whisker plots for regression models saved in a tidy data frame. The tidy data frames are prepared using parameters::model_parameters(). Additionally, if available, the model summary indices are also extracted from performance::model_performance().\n\nmlr.p &lt;- ggcoefstats(condo.mlr1)\nmlr.p\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWe can also use sort = argument to specify whether we want to sort the coefficient estimates in \"ascending\" or \"descending\" order. By default, sort = NULL is specified and no ordering is implemented.\n\n\n\n\n\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#building-hedonic-pricing-model-using-gwmodel",
    "href": "In-class_Ex/In-class_Ex07.html#building-hedonic-pricing-model-using-gwmodel",
    "title": "In-Class Exercise 07",
    "section": "",
    "text": "After exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 15:17:04.894653 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 15:17:06.376575 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-11 15:17:06.377451 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-03-11 15:17:08.020667 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          palette = c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\nNext, we will visualise the coefficient estimates\n\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n          border.col = \"gray20\",\n          border.lwd = 1)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n            palette=c(\"#57bfc0\",\"#7977f3\", \"#ce77b4\",\"#f67774\",\"#f89974\", \"#f8d673\",\"#f9f777\"),\n            size = 0.15,\n            border.col = \"gray20\",\n            border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#learning-outcomes",
    "href": "Hands-on_Ex/hands_on09.html#learning-outcomes",
    "title": "Hands-On Exercise 09",
    "section": "1.1 Learning Outcomes",
    "text": "1.1 Learning Outcomes\nIn this in-class exercise, we will explore how to build predictive model by using geographical random forest method. By the end of this hands-on exercise, we will acquire the skills of:\n\npreparing training and test data sets by using appropriate data sampling methods,\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated."
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#reading-data-file-to-rds",
    "href": "Hands-on_Ex/hands_on09.html#reading-data-file-to-rds",
    "title": "Hands-On Exercise 09",
    "section": "4.1 Reading data file to rds",
    "text": "4.1 Reading data file to rds\nFirst, we will input the dataset to R environment. The dataset is stored as an RDS (R Data Structure) file, a format native to R that preserves the metadata of the original data, making it ideal for storing R objects. We use the read_rds() function from the readr package to read this file. This function is specifically designed to read RDS files and load them into R.\n\nmdata &lt;- read_rds(\"../data/rds/mdata.rds\")\n\n\nmdata\n\nSimple feature collection with 15901 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 11597.31 ymin: 28217.39 xmax: 42623.63 ymax: 48741.06\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 15,901 × 18\n   resale_price floor_area_sqm storey_order remaining_lease_mths PROX_CBD\n          &lt;dbl&gt;          &lt;dbl&gt;        &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1       330000             92            1                  684     8.82\n 2       360000             91            3                  738     9.84\n 3       370000             92            1                  733     9.56\n 4       375000             99            2                  700     9.61\n 5       380000             92            2                  715     8.35\n 6       380000             92            4                  732     9.49\n 7       385000             92            3                  706     8.96\n 8       395000             92            2                  745     9.81\n 9       395000             93            4                  731    10.3 \n10       395000             91            3                  725    10.4 \n# ℹ 15,891 more rows\n# ℹ 13 more variables: PROX_ELDERLYCARE &lt;dbl&gt;, PROX_HAWKER &lt;dbl&gt;,\n#   PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;, PROX_GOOD_PRISCH &lt;dbl&gt;, PROX_MALL &lt;dbl&gt;,\n#   PROX_CHAS &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, WITHIN_350M_KINDERGARTEN &lt;int&gt;,\n#   WITHIN_350M_CHILDCARE &lt;int&gt;, WITHIN_350M_BUS &lt;int&gt;,\n#   WITHIN_1KM_PRISCH &lt;int&gt;, geometry &lt;POINT [m]&gt;\n\n\nOur dataset consists of 15,901 observations (rows) across 18 variables (columns). Each variable represents a different attribute of the data, including floor area in square meters, order of the storey, remaining lease in months, and proximity measures to local amenities."
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#data-sampling",
    "href": "Hands-on_Ex/hands_on09.html#data-sampling",
    "title": "Hands-On Exercise 09",
    "section": "4.2 Data Sampling",
    "text": "4.2 Data Sampling\nIn this section, we will be dividing our dataset into two parts: a training set and a test set. The training set will be used to build our model, while the test set will be used to evaluate its performance.\nWe will be using the initial_split() function from the rsample package to perform this split. The rsample package is part of the tidymodels framework, which provides a cohesive set of packages for modeling and machine learning using tidyverse principles.\nWe set a seed for reproducibility, and then use the initial_split() function to split our data into a training set (65% of the data) and a test set (35% of the data).\n\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\nAfter creating the training and test sets, we save them as RDS files using the write_rds() function. This will allow us to easily load the data in future R sessions.\n\nwrite_rds(train_data, \"../data/rds/train_data.rds\")\nwrite_rds(test_data, \"../data/rds/test_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "href": "Hands-on_Ex/hands_on09.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "title": "Hands-On Exercise 09",
    "section": "7.1 Converting the sf data.frame to SpatialPointDataFrame",
    "text": "7.1 Converting the sf data.frame to SpatialPointDataFrame\nFirst, we need to convert our sf data frame to a SpatialPointDataFrame. This is because the functions in the GWmodel package require data in this format. We use the as_Spatial() function from the sf package to perform this conversion.\n\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ..."
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#computing-adaptive-bandwidth",
    "href": "Hands-on_Ex/hands_on09.html#computing-adaptive-bandwidth",
    "title": "Hands-On Exercise 09",
    "section": "7.2 Computing Adaptive Bandwidth",
    "text": "7.2 Computing Adaptive Bandwidth\nNext, we use the bw.gwr() function from the GWmodel package to determine the optimal bandwidth for our GWR model. The bandwidth is a parameter of the GWR model that determines the extent of the geographical area that influences a given location’s estimate.\nWe set the approach argument to “CV” to use cross-validation to select the optimal bandwidth, the kernel argument to “gaussian” to use a Gaussian kernel, and the adaptive argument to TRUE to use an adaptive bandwidth. The longlat argument is set to FALSE because our coordinates are not in longitude and latitude.\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\n\n\n\n\n\n“Reflection\n\n\n\nIn the adaptive approach, the bandwidth is not a fixed distance but is determined based on the number of nearest neighbor points. This approach is particularly useful in areas where the density of data points varies.\nIn regions where data points are densely clustered, the adaptive bandwidth will be smaller, allowing the model to capture local variations more accurately. Conversely, in regions where data points are sparse, the adaptive bandwidth will be larger, ensuring that the model has enough data points to make reliable predictions.\nThe result from the bw.gwr() function indicates that the optimal bandwidth for this dataset is 40 neighbor points. This means that when estimating the parameters for a given location, the model will consider the 40 nearest neighbors.\n\n\nAfter constructing the model, we save it as an RDS file using the write_rds() function. This allows us to easily load the model in future R sessions.\n\nwrite_rds(bw_adaptive, \"../data/rds/bw_adaptive.rds\")\n\n\nbw_adaptive &lt;- read_rds(\"../data/rds/bw_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#constructing-the-adaptive-bandwidth-gwr-model",
    "href": "Hands-on_Ex/hands_on09.html#constructing-the-adaptive-bandwidth-gwr-model",
    "title": "Hands-On Exercise 09",
    "section": "7.3 Constructing the Adaptive Bandwidth GWR Model",
    "text": "7.3 Constructing the Adaptive Bandwidth GWR Model\nWith the optimal bandwidth determined, we can now calibrate the Geographically Weighted Regression (GWR) model. We will use the gwr.basic() function from the GWmodel package, specifying our formula, data, bandwidth, kernel type, and setting adaptive=TRUE and longlat=FALSE.\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nAfter constructing the model, we save it as an RDS file using the write_rds() function. This allows us to easily load the model in future R sessions.\n\nwrite_rds(gwr_adaptive, \"../data/rds/gwr_adaptive.rds\")\n\n\ngwr_adaptive &lt;- read_rds(\"../data/rds/gwr_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#retrieving-gwr-output-object",
    "href": "Hands-on_Ex/hands_on09.html#retrieving-gwr-output-object",
    "title": "Hands-On Exercise 09",
    "section": "7.4 Retrieving GWR Output Object",
    "text": "7.4 Retrieving GWR Output Object\nFinally, we can retrieve the GWR output object by simply calling its name.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-03-13 16:54:25.608497 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + \n    remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data_sp, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2594e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2291e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1660e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1881e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2489e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5224e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0262e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.8 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209 \n   Residual sum of squares: 4.829177e+12 \n   R-square value:  0.9676571 \n   Adjusted R-square value:  0.9611535 \n\n   ***********************************************************************\n   Program stops at: 2024-03-13 16:56:53.239337"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#converting-the-test-data-from-sf-data.frame-to-spatialpointdataframe",
    "href": "Hands-on_Ex/hands_on09.html#converting-the-test-data-from-sf-data.frame-to-spatialpointdataframe",
    "title": "Hands-On Exercise 09",
    "section": "7.5 Converting the Test Data from sf Data.Frame to SpatialPointDataFrame",
    "text": "7.5 Converting the Test Data from sf Data.Frame to SpatialPointDataFrame\nJust like we did with the training data, we need to convert our test data from an sf data frame to a SpatialPointDataFrame. This is because the functions in the GWmodel package require data in this format. We use the as_Spatial() function from the sf package to perform this conversion.\n\ntest_data_sp &lt;- test_data %&gt;%\n  as_Spatial()\ntest_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ..."
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#computing-adaptive-bandwidth-for-the-test-data",
    "href": "Hands-on_Ex/hands_on09.html#computing-adaptive-bandwidth-for-the-test-data",
    "title": "Hands-On Exercise 09",
    "section": "7.6 Computing Adaptive Bandwidth for the Test Data",
    "text": "7.6 Computing Adaptive Bandwidth for the Test Data\nNext, we use the bw.gwr() function from the GWmodel package to determine the optimal bandwidth for our GWR model on the test data. The process is the same as we did for the training data.\n\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nAfter constructing the model, we save it as an RDS file using the write_rds() function. This allows us to easily load the model in future R sessions.\n\nwrite_rds(gwr_bw_test_adaptive, \"../data/rds/gwr_bw_test_adaptive.rds\")\n\n\ngwr_bw_test_adaptive &lt;- read_rds(\"../data/rds/gwr_bw_test_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#computing-predicted-values-of-the-test-data",
    "href": "Hands-on_Ex/hands_on09.html#computing-predicted-values-of-the-test-data",
    "title": "Hands-On Exercise 09",
    "section": "7.7 Computing Predicted Values of the Test Data",
    "text": "7.7 Computing Predicted Values of the Test Data\nFinally, we use the gwr.predict() function from the GWmodel package to compute the predicted values of the test data based on our GWR model. We specify our formula, training data, test data, bandwidth, kernel type, and set adaptive=TRUE and longlat=FALSE.\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#calibrating-using-training-data",
    "href": "Hands-on_Ex/hands_on09.html#calibrating-using-training-data",
    "title": "Hands-On Exercise 09",
    "section": "10.1 Calibrating using Training Data",
    "text": "10.1 Calibrating using Training Data\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nwrite_rds(gwRF_adaptive, \"../data/rds/gwRF_adaptive.rds\")\n\n\ngwRF_adaptive &lt;- read_rds(\"../data/rds/gwRF_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#predicting-by-using-test-data",
    "href": "Hands-on_Ex/hands_on09.html#predicting-by-using-test-data",
    "title": "Hands-On Exercise 09",
    "section": "10.2 Predicting by using test data",
    "text": "10.2 Predicting by using test data\n\n10.2.1 Preparing the test data\nFirst, we combine the test data with its corresponding coordinates data. We use the cbind() function to combine the data and the st_drop_geometry() function to remove the geometry column.\n\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n10.2.2 Predicting with test data\nNext, we use the predict.grf() function from the SpatialML package to predict the resale value using the test data and the gwRF_adaptive model that we calibrated earlier.\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nBefore moving on, let us save the output into rds file for future use.\n\nwrite_rds(gwRF_pred, \"../data/rds/GRF_pred.rds\")\n\n\n\n10.2.3 Converting the predicting output into a data frame\nThe output of the predict.grf() function is a vector of predicted values. For further visualization and analysis, it’s useful to convert it into a data frame.\n\nGRF_pred &lt;- read_rds(\"../data/rds/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\n\nWe then use the cbind() function to append the predicted values onto the test_data.\n\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\nwrite_rds(test_data_p, \"../data/rds/test_data_p.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#calculating-root-mean-square-error",
    "href": "Hands-on_Ex/hands_on09.html#calculating-root-mean-square-error",
    "title": "Hands-On Exercise 09",
    "section": "10.3 Calculating Root Mean Square Error",
    "text": "10.3 Calculating Root Mean Square Error\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. We use the rmse() function from the Metrics package to compute the RMSE.\n\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n[1] 27302.9"
  },
  {
    "objectID": "Hands-on_Ex/hands_on09.html#visualising-the-predicted-values",
    "href": "Hands-on_Ex/hands_on09.html#visualising-the-predicted-values",
    "title": "Hands-On Exercise 09",
    "section": "10.4 Visualising the predicted values",
    "text": "10.4 Visualising the predicted values\nFinally, we can visualize the actual resale price and the predicted resale price using a scatterplot. This can help us understand how well our model is performing.\n\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09.html",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "pacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, tidymodels, tidyverse,\n               gtsummary, rpart, rpart.plot, ggstatsplot,\n               performance)\n\n\n\n\n\n\n\nReflection\n\n\n\nSpaialML package focuses only on random forest models and not other ML algorithms.\ntidymodels framework is a collection of packages for modeling and machine learning such as recipes for data pre-processing, tune for tuning models, yardstick for evaluating model performance, among others.\nrpart and rpart.plot packages will be used to demonstrate recursive partitioning, a fundamental concept behind random forest modelling.\n\n\n\n\n\nWe will read the datasets into R environment.\n\nrs_sf &lt;- read_rds(\"../data/rds/HDB_resale.rds\")\n\nNext, the code chunk below is used tor reveal the properties of rs_sf object.\n\nrs_sf\n\nSimple feature collection with 15901 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 11597.31 ymin: 28217.39 xmax: 42623.63 ymax: 48741.06\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 15,901 × 18\n   RESALE_PRICE FLOOR_AREA_SQM STOREY_ORDER REMAINING_LEASE_MTHS PROX_CBD\n          &lt;dbl&gt;          &lt;dbl&gt;        &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1       330000             92            1                  684     8.82\n 2       360000             91            3                  738     9.84\n 3       370000             92            1                  733     9.56\n 4       375000             99            2                  700     9.61\n 5       380000             92            2                  715     8.35\n 6       380000             92            4                  732     9.49\n 7       385000             92            3                  706     8.96\n 8       395000             92            2                  745     9.81\n 9       395000             93            4                  731    10.3 \n10       395000             91            3                  725    10.4 \n# ℹ 15,891 more rows\n# ℹ 13 more variables: PROX_ELDERLYCARE &lt;dbl&gt;, PROX_HAWKER &lt;dbl&gt;,\n#   PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;, PROX_GOOD_PRISCH &lt;dbl&gt;, PROX_MALL &lt;dbl&gt;,\n#   PROX_CHAS &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, WITHIN_350M_KINDERGARTEN &lt;int&gt;,\n#   WITHIN_350M_CHILDCARE &lt;int&gt;, WITHIN_350M_BUS &lt;int&gt;,\n#   WITHIN_1KM_PRISCH &lt;int&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nrs_sf is in simple feature data frame format and has a total of 15901 observations with 18 variables.\n\n\n\n\n\n\nset.seed(1234)\nresale_split &lt;- initial_split(\n  rs_sf,\n  prop = 5/10,)\ntrain_sf &lt;- training(resale_split)\ntest_sf &lt;- testing(resale_split)\n\n\n\n\n\n\n\nReflection\n\n\n\nset.seed(1234) sets the seed of R’s random number generator, which is useful for creating simulations or random objects that can be reproduced.\ninitial_split function from the rsample package is used to split the rs_sf dataset into two parts. The prop = 5/10 argument indicates that the data is being split into two equal halves, with 50% of the data going into each set.\ninitial_split function use random sampling approach. rsample package can be used for other sampling patterns such as stratified sampling, group sampling and time-based sampling. More details can be read here: https://rsample.tidymodels.org/articles/Common_Patterns.html\n\n\nWe will save the train_sf and test_sf into rds for later use.\n\nwrite_rds(train_sf, \"../data/rds/train_sf.rds\")\nwrite_rds(test_sf, \"../data/rds/test_sf.rds\")\ntrain_sf &lt;- read_rds(\"../data/rds/train_sf.rds\")\ntest_sf &lt;- read_rds(\"../data/rds/test_sf.rds\")\n\n\ntrain_df &lt;- train_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  as.data.frame()\n\ntest_df &lt;- test_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  as.data.frame()\n\n\n\n\n\n\n\nReflection\n\n\n\nIt is important to check which data class is compatible for the chosen analysis. Not all machine learning algorithms can handle all types of data. For example, many algorithms cannot process spatial data directly, which is why we dropped the geometry column. Also some algorithms cannot handle tibble data type and hence we convert it to data.frame.\n\n\n\n\n\nTo check for multicollinearity, we compute a correlation matrix using the ggcorrmat() function from the ggstatsplot package. This function creates a correlation matrix plot, which is a graphical representation of the correlation matrix. This correlation matrix will give us a visual overview of how the predictors in our dataset are related to each other. If we see high correlation coefficients (close to 1 or -1), we may need to address multicollinearity before proceeding with our analysis.\n\nggstatsplot::ggcorrmat(\n  data = train_df[, 2:17],\n  matrix.type = \"upper\",\n  type = \"parametric\",\n  tr = 0.2,\n  partial = FALSE,\n  k = 2L,\n  sig.level = 0.05,\n  conf.level = 0.95,\n  bf.prior = 0.707,\n  ggcorrplot.args = list(\n     tl.cex = 10,\n     pch.cex = 5,\n     lab_size = 3\n  )) + \n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(\n      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe correlation matrix above shows that all the correlation values are below 0.65. Hence, there is no sign of multicollinearity.\n\n\n\n\n\nIn this section, we will be building a non-spatial multiple linear regression model. This type of model is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal is to model the relationship between the explanatory and response variables.\n\nrs_mlr &lt;- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + PROX_CHAS +\n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n         data=train_df)\nsummary(rs_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_CHAS + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179676  -39020   -1719   36755  327324 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              109622.960  11993.611   9.140  &lt; 2e-16 ***\nFLOOR_AREA_SQM             2733.136    103.116  26.505  &lt; 2e-16 ***\nSTOREY_ORDER              14198.168    384.182  36.957  &lt; 2e-16 ***\nREMAINING_LEASE_MTHS        346.624      5.208  66.557  &lt; 2e-16 ***\nPROX_CBD                 -16943.794    227.064 -74.621  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -13891.413   1124.964 -12.348  &lt; 2e-16 ***\nPROX_HAWKER              -17758.037   1461.269 -12.152  &lt; 2e-16 ***\nPROX_MRT                 -32357.534   1965.095 -16.466  &lt; 2e-16 ***\nPROX_PARK                 -6714.626   1672.160  -4.016 5.99e-05 ***\nPROX_MALL                -14080.474   2268.191  -6.208 5.64e-10 ***\nPROX_CHAS                 -5819.260   7208.182  -0.807 0.419510    \nPROX_SUPERMARKET         -24077.152   5068.317  -4.751 2.06e-06 ***\nWITHIN_350M_KINDERGARTEN   8730.822    721.593  12.099  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4629.126    399.231 -11.595  &lt; 2e-16 ***\nWITHIN_350M_BUS             979.339    252.851   3.873 0.000108 ***\nWITHIN_1KM_PRISCH         -8434.367    553.862 -15.228  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61050 on 7934 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1510 on 15 and 7934 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(rs_mlr,\n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n109,623\n86,112, 133,134\n&lt;0.001\n    FLOOR_AREA_SQM\n2,733\n2,531, 2,935\n&lt;0.001\n    STOREY_ORDER\n14,198\n13,445, 14,951\n&lt;0.001\n    REMAINING_LEASE_MTHS\n347\n336, 357\n&lt;0.001\n    PROX_CBD\n-16,944\n-17,389, -16,499\n&lt;0.001\n    PROX_ELDERLYCARE\n-13,891\n-16,097, -11,686\n&lt;0.001\n    PROX_HAWKER\n-17,758\n-20,623, -14,894\n&lt;0.001\n    PROX_MRT\n-32,358\n-36,210, -28,505\n&lt;0.001\n    PROX_PARK\n-6,715\n-9,992, -3,437\n&lt;0.001\n    PROX_MALL\n-14,080\n-18,527, -9,634\n&lt;0.001\n    PROX_CHAS\n-5,819\n-19,949, 8,311\n0.4\n    PROX_SUPERMARKET\n-24,077\n-34,012, -14,142\n&lt;0.001\n    WITHIN_350M_KINDERGARTEN\n8,731\n7,316, 10,145\n&lt;0.001\n    WITHIN_350M_CHILDCARE\n-4,629\n-5,412, -3,847\n&lt;0.001\n    WITHIN_350M_BUS\n979\n484, 1,475\n&lt;0.001\n    WITHIN_1KM_PRISCH\n-8,434\n-9,520, -7,349\n&lt;0.001\n  \n  \n    \n      R² = 0.741; Adjusted R² = 0.740; AIC = 197,787; Statistic = 1,510; p-value = &lt;0.001; σ = 61,046\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nFrom the results in previous section, PROX_CHAS is not statistically significant. Hence, we will remove the PROX_CHAS column from the training and testing sets.\n\ntrain_df &lt;- train_df %&gt;%\n  select(-c(PROX_CHAS))\ntrain_sf &lt;- train_sf %&gt;%\n  select(-c(PROX_CHAS))\ntest_df &lt;- test_df %&gt;%\n  select(-c(PROX_CHAS))\ntest_sf &lt;- test_sf %&gt;%\n  select(-c(PROX_CHAS))\n\nAfter removal, we will re-run the MLR model again.\n\nrs_mlr &lt;- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + \n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n         data=train_df)\nsummary(rs_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179178  -39031   -1868   36751  327631 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              109413.550  11990.543   9.125  &lt; 2e-16 ***\nFLOOR_AREA_SQM             2725.663    102.698  26.541  &lt; 2e-16 ***\nSTOREY_ORDER              14192.913    384.118  36.949  &lt; 2e-16 ***\nREMAINING_LEASE_MTHS        346.996      5.187  66.893  &lt; 2e-16 ***\nPROX_CBD                 -16943.081    227.058 -74.620  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -13972.191   1120.481 -12.470  &lt; 2e-16 ***\nPROX_HAWKER              -17968.486   1437.798 -12.497  &lt; 2e-16 ***\nPROX_MRT                 -32448.233   1961.837 -16.540  &lt; 2e-16 ***\nPROX_PARK                 -6753.096   1671.444  -4.040 5.39e-05 ***\nPROX_MALL                -14003.731   2266.148  -6.180 6.75e-10 ***\nPROX_SUPERMARKET         -25566.285   4720.643  -5.416 6.28e-08 ***\nWITHIN_350M_KINDERGARTEN   8740.242    721.483  12.114  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4614.476    398.810 -11.571  &lt; 2e-16 ***\nWITHIN_350M_BUS             990.698    252.454   3.924 8.77e-05 ***\nWITHIN_1KM_PRISCH         -8438.093    553.831 -15.236  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61040 on 7935 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1618 on 14 and 7935 DF,  p-value: &lt; 2.2e-16\n\n\n\nclass(rs_mlr)\n\n[1] \"lm\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe MLR model we fitted returns an object of class lm. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the lm class object.\n\n\n\n\n\nIn this step, we extract the X and Y coordinates from the full, training, and test datasets. The st_coordinates() function from the sf package is used to perform this extraction.\n\ncoords &lt;- st_coordinates(rs_sf)\ncoords_train &lt;- st_coordinates(train_sf)\ncoords_test &lt;- st_coordinates(test_sf)\n\n\n\n\n\n\n\nReflection\n\n\n\nIf we inspect the documentation (https://search.r-project.org/CRAN/refmans/SpatialML/html/grf.bw.html) for bandwidth calculation of geographical random forest using grf.bw(), it expects a separate input called coords, which represent a numeric matrix or data frame of two columns giving the X,Y coordinates of the observations. These coordinates values are used to calculate spatial weight matrix.\nIn GWmodel package for GWR modelling, we just have to convert it to sp object and the algorithms automatically extract the coordinates, eliminating the need for manual input.\n\n\n\n\n\n\nset.seed(1234)\nrs_rp &lt;- rpart(\n  formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + \n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n  data = train_df)\nrs_rp\n\nn= 7950 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7950 1.139546e+14 433705.6  \n   2) PROX_CBD&gt;=7.974483 6665 4.472144e+13 403736.0  \n     4) REMAINING_LEASE_MTHS&lt; 1020.5 4228 1.573100e+13 370187.4  \n       8) PROX_CBD&gt;=14.48068 1820 2.748388e+12 337963.6 *\n       9) PROX_CBD&lt; 14.48068 2408 9.664405e+12 394542.6 *\n     5) REMAINING_LEASE_MTHS&gt;=1020.5 2437 1.597594e+13 461940.1  \n      10) PROX_CBD&gt;=10.40657 2331 9.762718e+12 451754.4  \n        20) PROX_CBD&gt;=14.20377 1088 3.345588e+12 426109.1 *\n        21) PROX_CBD&lt; 14.20377 1243 5.075243e+12 474201.8 *\n      11) PROX_CBD&lt; 10.40657 106 6.532500e+11 685929.1 *\n   3) PROX_CBD&lt; 7.974483 1285 3.219685e+13 589151.4  \n     6) REMAINING_LEASE_MTHS&lt; 930.5 745 6.613365e+12 486637.6  \n      12) FLOOR_AREA_SQM&lt; 98.5 451 2.446537e+12 442460.5 *\n      13) FLOOR_AREA_SQM&gt;=98.5 294 1.936449e+12 554405.7 *\n     7) REMAINING_LEASE_MTHS&gt;=930.5 540 6.952722e+12 730582.5  \n      14) REMAINING_LEASE_MTHS&lt; 1071.5 314 2.461969e+12 676641.3 *\n      15) REMAINING_LEASE_MTHS&gt;=1071.5 226 2.307737e+12 805527.4 *\n\n\nTo visualise how recursive partitioning works, we will pass the model to rpart.plot() function.\n\nrpart.plot(rs_rp)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nRecursive partitioning can use the same variables more than once in different parts of the tree. This capability can uncover complex interdependencies between sets of variables. That is why, we see variables such as PROX_CBD and REMAINING_LEASE_MTHS in different levels of the trees.\n\n\n\n\n\n\nset.seed(1234)\nrs_rf &lt;- ranger(\n  RESALE_PRICE  ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + \n               REMAINING_LEASE_MTHS +\n                  PROX_CBD + \n               PROX_ELDERLYCARE + \n               PROX_HAWKER +\n                  PROX_MRT + \n               PROX_PARK + \n               PROX_MALL + \n                  PROX_SUPERMARKET + \n               WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + \n               WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df,\n  importance = \"impurity\")\n\nwrite_rds(rs_rf, \"../data/models/rs_rf.rds\")\n\n\n\n\n\n\n\nReflection\n\n\n\nThe ranger package in R is a fast implementation of Random Forests, particularly suited to high dimensional data. By using ranger as a base, we can take advantage of its speed and functionality when calibrating your models.\nWhen it comes to spatial analysis, packages like SpatialML use ranger as a dependency. This means that the functions in SpatialML are built upon the functions in ranger. Using ranger as a base model allows for seamless integration and readily calibration of base model to a spatial model.\n\n\n\nThe “impurity” measure is the Gini index for classification, the variance of the responses for regression and the sum of statistics for survival.\n\n\nrs_rf &lt;- read_rds(\"../data/models/rs_rf.rds\")\nrs_rf\n\nRanger result\n\nCall:\n ranger(RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_df, importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      7950 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       782127271 \nR squared (OOB):                  0.9454421 \n\n\n\n\n\nVariable importance or feature importance scores are indicative of how “important” the variable is to our model.\nBy using impurity importance argument in our ranger function earlier, rs_rf has contains the generated variable.importance. Now we will extract variable.importance and save it into vi.\n\nvi &lt;- as.data.frame(rs_rf$variable.importance)\nvi$variables &lt;- rownames(vi)\nvi &lt;- vi%&gt;%\n  rename(vi = \"rs_rf$variable.importance\")\n\nPlot the graph.\n\nggplot(data = vi,\n       aes(x = vi,\n           y = reorder(variables, vi),\n       fill = variables)) +\n  geom_bar(stat=\"identity\", show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nreorder(variables, vi) function is used to reorder the levels of variables based on the values of vi, so that the bars will be sorted in the plot.\nSince there is no sign of quasi-complete or complete separation, we can proceed to calibrate our bandwidth.\n\n\n\n\n\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + \n                       STOREY_ORDER +\n                       REMAINING_LEASE_MTHS + \n                       PROX_CBD + \n                       PROX_ELDERLYCARE +\n                       PROX_HAWKER + \n                       PROX_MRT + \n                       PROX_PARK + \n                       PROX_MALL +\n                       PROX_SUPERMARKET + \n                       WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + \n                       WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_df, \n                     bw = 55,\n                     step = 1,\n                     nthreads = 16,\n                     forest = FALSE,\n                     weighted = TRUE,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nrs_grf &lt;- read_rds(\"../data/models/rs_grf.rds\")\n\n\ntest_df &lt;- cbind(test_sf, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\nNext, predict.grf() of spatialML will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\ngrf_pred &lt;- read_rds(\"../data/models/grf_pred.rds\")\ngrf_pred_df &lt;- as.data.frame(grf_pred)\n\n\ntest_pred &lt;- test_df %&gt;%\n  select(RESALE_PRICE) %&gt;%\n  cbind(grf_pred_df)\n\n\nrf_pred &lt;- predict(rs_rf, test_df)\nrf_pred_df &lt;- as.data.frame(rf_pred$predictions) %&gt;% rename(rf_pred = \"rf_pred$predictions\")\n\n\nmlr_pred &lt;- predict(rs_mlr, test_df)\nmlr_pred_df &lt;- as.data.frame(mlr_pred) %&gt;% rename(mlr_pred = \"mlr_pred\")\n\n\ntest_pred &lt;- cbind(test_pred,rf_pred_df)\ntest_pred &lt;- cbind(test_pred,mlr_pred_df)\n\n\n\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                grf_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      28745.\n\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                rf_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      29242.\n\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                mlr_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      61821.\n\n\n\nmc &lt;- test_pred %&gt;%\n  pivot_longer(cols = c(2:4),\n               names_to = \"models\",\n               values_to = \"predicted\")\n\nFinally, we can visualize the actual resale price and the predicted resale price using a scatterplot. This can help us understand how well our model is performing.\n\nggplot(data = mc,\n       aes(x = predicted,\n           y = RESALE_PRICE)) +\n  geom_point()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex09.html#importing-packages",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "pacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, tidymodels, tidyverse,\n               gtsummary, rpart, rpart.plot, ggstatsplot,\n               performance)\n\n\n\n\n\n\n\nReflection\n\n\n\nSpaialML package focuses only on random forest models and not other ML algorithms.\ntidymodels framework is a collection of packages for modeling and machine learning such as recipes for data pre-processing, tune for tuning models, yardstick for evaluating model performance, among others.\nrpart and rpart.plot packages will be used to demonstrate recursive partitioning, a fundamental concept behind random forest modelling."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#importing-datasets",
    "href": "In-class_Ex/In-class_Ex09.html#importing-datasets",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "We will read the datasets into R environment.\n\nrs_sf &lt;- read_rds(\"../data/rds/HDB_resale.rds\")\n\nNext, the code chunk below is used tor reveal the properties of rs_sf object.\n\nrs_sf\n\nSimple feature collection with 15901 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 11597.31 ymin: 28217.39 xmax: 42623.63 ymax: 48741.06\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 15,901 × 18\n   RESALE_PRICE FLOOR_AREA_SQM STOREY_ORDER REMAINING_LEASE_MTHS PROX_CBD\n          &lt;dbl&gt;          &lt;dbl&gt;        &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1       330000             92            1                  684     8.82\n 2       360000             91            3                  738     9.84\n 3       370000             92            1                  733     9.56\n 4       375000             99            2                  700     9.61\n 5       380000             92            2                  715     8.35\n 6       380000             92            4                  732     9.49\n 7       385000             92            3                  706     8.96\n 8       395000             92            2                  745     9.81\n 9       395000             93            4                  731    10.3 \n10       395000             91            3                  725    10.4 \n# ℹ 15,891 more rows\n# ℹ 13 more variables: PROX_ELDERLYCARE &lt;dbl&gt;, PROX_HAWKER &lt;dbl&gt;,\n#   PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;, PROX_GOOD_PRISCH &lt;dbl&gt;, PROX_MALL &lt;dbl&gt;,\n#   PROX_CHAS &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, WITHIN_350M_KINDERGARTEN &lt;int&gt;,\n#   WITHIN_350M_CHILDCARE &lt;int&gt;, WITHIN_350M_BUS &lt;int&gt;,\n#   WITHIN_1KM_PRISCH &lt;int&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nrs_sf is in simple feature data frame format and has a total of 15901 observations with 18 variables."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#data-sampling",
    "href": "In-class_Ex/In-class_Ex09.html#data-sampling",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "set.seed(1234)\nresale_split &lt;- initial_split(\n  rs_sf,\n  prop = 5/10,)\ntrain_sf &lt;- training(resale_split)\ntest_sf &lt;- testing(resale_split)\n\n\n\n\n\n\n\nReflection\n\n\n\nset.seed(1234) sets the seed of R’s random number generator, which is useful for creating simulations or random objects that can be reproduced.\ninitial_split function from the rsample package is used to split the rs_sf dataset into two parts. The prop = 5/10 argument indicates that the data is being split into two equal halves, with 50% of the data going into each set.\ninitial_split function use random sampling approach. rsample package can be used for other sampling patterns such as stratified sampling, group sampling and time-based sampling. More details can be read here: https://rsample.tidymodels.org/articles/Common_Patterns.html\n\n\nWe will save the train_sf and test_sf into rds for later use.\n\nwrite_rds(train_sf, \"../data/rds/train_sf.rds\")\nwrite_rds(test_sf, \"../data/rds/test_sf.rds\")\ntrain_sf &lt;- read_rds(\"../data/rds/train_sf.rds\")\ntest_sf &lt;- read_rds(\"../data/rds/test_sf.rds\")\n\n\ntrain_df &lt;- train_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  as.data.frame()\n\ntest_df &lt;- test_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  as.data.frame()\n\n\n\n\n\n\n\nReflection\n\n\n\nIt is important to check which data class is compatible for the chosen analysis. Not all machine learning algorithms can handle all types of data. For example, many algorithms cannot process spatial data directly, which is why we dropped the geometry column. Also some algorithms cannot handle tibble data type and hence we convert it to data.frame."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#computing-correlation-matrix",
    "href": "In-class_Ex/In-class_Ex09.html#computing-correlation-matrix",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "To check for multicollinearity, we compute a correlation matrix using the ggcorrmat() function from the ggstatsplot package. This function creates a correlation matrix plot, which is a graphical representation of the correlation matrix. This correlation matrix will give us a visual overview of how the predictors in our dataset are related to each other. If we see high correlation coefficients (close to 1 or -1), we may need to address multicollinearity before proceeding with our analysis.\n\nggstatsplot::ggcorrmat(\n  data = train_df[, 2:17],\n  matrix.type = \"upper\",\n  type = \"parametric\",\n  tr = 0.2,\n  partial = FALSE,\n  k = 2L,\n  sig.level = 0.05,\n  conf.level = 0.95,\n  bf.prior = 0.707,\n  ggcorrplot.args = list(\n     tl.cex = 10,\n     pch.cex = 5,\n     lab_size = 3\n  )) + \n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(\n      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = \"cm\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe correlation matrix above shows that all the correlation values are below 0.65. Hence, there is no sign of multicollinearity."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#building-and-fitting-mlr-model",
    "href": "In-class_Ex/In-class_Ex09.html#building-and-fitting-mlr-model",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "In this section, we will be building a non-spatial multiple linear regression model. This type of model is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal is to model the relationship between the explanatory and response variables.\n\nrs_mlr &lt;- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + PROX_CHAS +\n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n         data=train_df)\nsummary(rs_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_CHAS + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179676  -39020   -1719   36755  327324 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              109622.960  11993.611   9.140  &lt; 2e-16 ***\nFLOOR_AREA_SQM             2733.136    103.116  26.505  &lt; 2e-16 ***\nSTOREY_ORDER              14198.168    384.182  36.957  &lt; 2e-16 ***\nREMAINING_LEASE_MTHS        346.624      5.208  66.557  &lt; 2e-16 ***\nPROX_CBD                 -16943.794    227.064 -74.621  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -13891.413   1124.964 -12.348  &lt; 2e-16 ***\nPROX_HAWKER              -17758.037   1461.269 -12.152  &lt; 2e-16 ***\nPROX_MRT                 -32357.534   1965.095 -16.466  &lt; 2e-16 ***\nPROX_PARK                 -6714.626   1672.160  -4.016 5.99e-05 ***\nPROX_MALL                -14080.474   2268.191  -6.208 5.64e-10 ***\nPROX_CHAS                 -5819.260   7208.182  -0.807 0.419510    \nPROX_SUPERMARKET         -24077.152   5068.317  -4.751 2.06e-06 ***\nWITHIN_350M_KINDERGARTEN   8730.822    721.593  12.099  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4629.126    399.231 -11.595  &lt; 2e-16 ***\nWITHIN_350M_BUS             979.339    252.851   3.873 0.000108 ***\nWITHIN_1KM_PRISCH         -8434.367    553.862 -15.228  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61050 on 7934 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1510 on 15 and 7934 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(rs_mlr,\n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n109,623\n86,112, 133,134\n&lt;0.001\n    FLOOR_AREA_SQM\n2,733\n2,531, 2,935\n&lt;0.001\n    STOREY_ORDER\n14,198\n13,445, 14,951\n&lt;0.001\n    REMAINING_LEASE_MTHS\n347\n336, 357\n&lt;0.001\n    PROX_CBD\n-16,944\n-17,389, -16,499\n&lt;0.001\n    PROX_ELDERLYCARE\n-13,891\n-16,097, -11,686\n&lt;0.001\n    PROX_HAWKER\n-17,758\n-20,623, -14,894\n&lt;0.001\n    PROX_MRT\n-32,358\n-36,210, -28,505\n&lt;0.001\n    PROX_PARK\n-6,715\n-9,992, -3,437\n&lt;0.001\n    PROX_MALL\n-14,080\n-18,527, -9,634\n&lt;0.001\n    PROX_CHAS\n-5,819\n-19,949, 8,311\n0.4\n    PROX_SUPERMARKET\n-24,077\n-34,012, -14,142\n&lt;0.001\n    WITHIN_350M_KINDERGARTEN\n8,731\n7,316, 10,145\n&lt;0.001\n    WITHIN_350M_CHILDCARE\n-4,629\n-5,412, -3,847\n&lt;0.001\n    WITHIN_350M_BUS\n979\n484, 1,475\n&lt;0.001\n    WITHIN_1KM_PRISCH\n-8,434\n-9,520, -7,349\n&lt;0.001\n  \n  \n    \n      R² = 0.741; Adjusted R² = 0.740; AIC = 197,787; Statistic = 1,510; p-value = &lt;0.001; σ = 61,046\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#revising-mlr-model",
    "href": "In-class_Ex/In-class_Ex09.html#revising-mlr-model",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "From the results in previous section, PROX_CHAS is not statistically significant. Hence, we will remove the PROX_CHAS column from the training and testing sets.\n\ntrain_df &lt;- train_df %&gt;%\n  select(-c(PROX_CHAS))\ntrain_sf &lt;- train_sf %&gt;%\n  select(-c(PROX_CHAS))\ntest_df &lt;- test_df %&gt;%\n  select(-c(PROX_CHAS))\ntest_sf &lt;- test_sf %&gt;%\n  select(-c(PROX_CHAS))\n\nAfter removal, we will re-run the MLR model again.\n\nrs_mlr &lt;- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + \n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n         data=train_df)\nsummary(rs_mlr)\n\n\nCall:\nlm(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179178  -39031   -1868   36751  327631 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              109413.550  11990.543   9.125  &lt; 2e-16 ***\nFLOOR_AREA_SQM             2725.663    102.698  26.541  &lt; 2e-16 ***\nSTOREY_ORDER              14192.913    384.118  36.949  &lt; 2e-16 ***\nREMAINING_LEASE_MTHS        346.996      5.187  66.893  &lt; 2e-16 ***\nPROX_CBD                 -16943.081    227.058 -74.620  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -13972.191   1120.481 -12.470  &lt; 2e-16 ***\nPROX_HAWKER              -17968.486   1437.798 -12.497  &lt; 2e-16 ***\nPROX_MRT                 -32448.233   1961.837 -16.540  &lt; 2e-16 ***\nPROX_PARK                 -6753.096   1671.444  -4.040 5.39e-05 ***\nPROX_MALL                -14003.731   2266.148  -6.180 6.75e-10 ***\nPROX_SUPERMARKET         -25566.285   4720.643  -5.416 6.28e-08 ***\nWITHIN_350M_KINDERGARTEN   8740.242    721.483  12.114  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4614.476    398.810 -11.571  &lt; 2e-16 ***\nWITHIN_350M_BUS             990.698    252.454   3.924 8.77e-05 ***\nWITHIN_1KM_PRISCH         -8438.093    553.831 -15.236  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61040 on 7935 degrees of freedom\nMultiple R-squared:  0.7405,    Adjusted R-squared:  0.7401 \nF-statistic:  1618 on 14 and 7935 DF,  p-value: &lt; 2.2e-16\n\n\n\nclass(rs_mlr)\n\n[1] \"lm\"\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe MLR model we fitted returns an object of class lm. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the lm class object."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#extracting-coordinates-data",
    "href": "In-class_Ex/In-class_Ex09.html#extracting-coordinates-data",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "In this step, we extract the X and Y coordinates from the full, training, and test datasets. The st_coordinates() function from the sf package is used to perform this extraction.\n\ncoords &lt;- st_coordinates(rs_sf)\ncoords_train &lt;- st_coordinates(train_sf)\ncoords_test &lt;- st_coordinates(test_sf)\n\n\n\n\n\n\n\nReflection\n\n\n\nIf we inspect the documentation (https://search.r-project.org/CRAN/refmans/SpatialML/html/grf.bw.html) for bandwidth calculation of geographical random forest using grf.bw(), it expects a separate input called coords, which represent a numeric matrix or data frame of two columns giving the X,Y coordinates of the observations. These coordinates values are used to calculate spatial weight matrix.\nIn GWmodel package for GWR modelling, we just have to convert it to sp object and the algorithms automatically extract the coordinates, eliminating the need for manual input."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#recursive-partitioning",
    "href": "In-class_Ex/In-class_Ex09.html#recursive-partitioning",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "set.seed(1234)\nrs_rp &lt;- rpart(\n  formula = RESALE_PRICE ~ FLOOR_AREA_SQM +\n         STOREY_ORDER + REMAINING_LEASE_MTHS +\n         PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n         PROX_MRT + PROX_PARK + PROX_MALL + \n         PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n         WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n         WITHIN_1KM_PRISCH,\n  data = train_df)\nrs_rp\n\nn= 7950 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7950 1.139546e+14 433705.6  \n   2) PROX_CBD&gt;=7.974483 6665 4.472144e+13 403736.0  \n     4) REMAINING_LEASE_MTHS&lt; 1020.5 4228 1.573100e+13 370187.4  \n       8) PROX_CBD&gt;=14.48068 1820 2.748388e+12 337963.6 *\n       9) PROX_CBD&lt; 14.48068 2408 9.664405e+12 394542.6 *\n     5) REMAINING_LEASE_MTHS&gt;=1020.5 2437 1.597594e+13 461940.1  \n      10) PROX_CBD&gt;=10.40657 2331 9.762718e+12 451754.4  \n        20) PROX_CBD&gt;=14.20377 1088 3.345588e+12 426109.1 *\n        21) PROX_CBD&lt; 14.20377 1243 5.075243e+12 474201.8 *\n      11) PROX_CBD&lt; 10.40657 106 6.532500e+11 685929.1 *\n   3) PROX_CBD&lt; 7.974483 1285 3.219685e+13 589151.4  \n     6) REMAINING_LEASE_MTHS&lt; 930.5 745 6.613365e+12 486637.6  \n      12) FLOOR_AREA_SQM&lt; 98.5 451 2.446537e+12 442460.5 *\n      13) FLOOR_AREA_SQM&gt;=98.5 294 1.936449e+12 554405.7 *\n     7) REMAINING_LEASE_MTHS&gt;=930.5 540 6.952722e+12 730582.5  \n      14) REMAINING_LEASE_MTHS&lt; 1071.5 314 2.461969e+12 676641.3 *\n      15) REMAINING_LEASE_MTHS&gt;=1071.5 226 2.307737e+12 805527.4 *\n\n\nTo visualise how recursive partitioning works, we will pass the model to rpart.plot() function.\n\nrpart.plot(rs_rp)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nRecursive partitioning can use the same variables more than once in different parts of the tree. This capability can uncover complex interdependencies between sets of variables. That is why, we see variables such as PROX_CBD and REMAINING_LEASE_MTHS in different levels of the trees."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#non-spatial-random-forest-modelling",
    "href": "In-class_Ex/In-class_Ex09.html#non-spatial-random-forest-modelling",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "set.seed(1234)\nrs_rf &lt;- ranger(\n  RESALE_PRICE  ~ FLOOR_AREA_SQM +\n                  STOREY_ORDER + \n               REMAINING_LEASE_MTHS +\n                  PROX_CBD + \n               PROX_ELDERLYCARE + \n               PROX_HAWKER +\n                  PROX_MRT + \n               PROX_PARK + \n               PROX_MALL + \n                  PROX_SUPERMARKET + \n               WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + \n               WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_df,\n  importance = \"impurity\")\n\nwrite_rds(rs_rf, \"../data/models/rs_rf.rds\")\n\n\n\n\n\n\n\nReflection\n\n\n\nThe ranger package in R is a fast implementation of Random Forests, particularly suited to high dimensional data. By using ranger as a base, we can take advantage of its speed and functionality when calibrating your models.\nWhen it comes to spatial analysis, packages like SpatialML use ranger as a dependency. This means that the functions in SpatialML are built upon the functions in ranger. Using ranger as a base model allows for seamless integration and readily calibration of base model to a spatial model.\n\n\n\nThe “impurity” measure is the Gini index for classification, the variance of the responses for regression and the sum of statistics for survival.\n\n\nrs_rf &lt;- read_rds(\"../data/models/rs_rf.rds\")\nrs_rf\n\nRanger result\n\nCall:\n ranger(RESALE_PRICE ~ FLOOR_AREA_SQM + STOREY_ORDER + REMAINING_LEASE_MTHS +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_df, importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      7950 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       782127271 \nR squared (OOB):                  0.9454421"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#variable-importance",
    "href": "In-class_Ex/In-class_Ex09.html#variable-importance",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "Variable importance or feature importance scores are indicative of how “important” the variable is to our model.\nBy using impurity importance argument in our ranger function earlier, rs_rf has contains the generated variable.importance. Now we will extract variable.importance and save it into vi.\n\nvi &lt;- as.data.frame(rs_rf$variable.importance)\nvi$variables &lt;- rownames(vi)\nvi &lt;- vi%&gt;%\n  rename(vi = \"rs_rf$variable.importance\")\n\nPlot the graph.\n\nggplot(data = vi,\n       aes(x = vi,\n           y = reorder(variables, vi),\n       fill = variables)) +\n  geom_bar(stat=\"identity\", show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nreorder(variables, vi) function is used to reorder the levels of variables based on the values of vi, so that the bars will be sorted in the plot.\nSince there is no sign of quasi-complete or complete separation, we can proceed to calibrate our bandwidth."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#fitting-geographical-random-forest",
    "href": "In-class_Ex/In-class_Ex09.html#fitting-geographical-random-forest",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "set.seed(1234)\ngwRF_adaptive &lt;- grf(formula = RESALE_PRICE ~ FLOOR_AREA_SQM + \n                       STOREY_ORDER +\n                       REMAINING_LEASE_MTHS + \n                       PROX_CBD + \n                       PROX_ELDERLYCARE +\n                       PROX_HAWKER + \n                       PROX_MRT + \n                       PROX_PARK + \n                       PROX_MALL +\n                       PROX_SUPERMARKET + \n                       WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + \n                       WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_df, \n                     bw = 55,\n                     step = 1,\n                     nthreads = 16,\n                     forest = FALSE,\n                     weighted = TRUE,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nrs_grf &lt;- read_rds(\"../data/models/rs_grf.rds\")\n\n\ntest_df &lt;- cbind(test_sf, coords_test) %&gt;%\n  st_drop_geometry()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#predicting-with-test-data",
    "href": "In-class_Ex/In-class_Ex09.html#predicting-with-test-data",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "Next, predict.grf() of spatialML will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\ngrf_pred &lt;- read_rds(\"../data/models/grf_pred.rds\")\ngrf_pred_df &lt;- as.data.frame(grf_pred)\n\n\ntest_pred &lt;- test_df %&gt;%\n  select(RESALE_PRICE) %&gt;%\n  cbind(grf_pred_df)\n\n\nrf_pred &lt;- predict(rs_rf, test_df)\nrf_pred_df &lt;- as.data.frame(rf_pred$predictions) %&gt;% rename(rf_pred = \"rf_pred$predictions\")\n\n\nmlr_pred &lt;- predict(rs_mlr, test_df)\nmlr_pred_df &lt;- as.data.frame(mlr_pred) %&gt;% rename(mlr_pred = \"mlr_pred\")\n\n\ntest_pred &lt;- cbind(test_pred,rf_pred_df)\ntest_pred &lt;- cbind(test_pred,mlr_pred_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#performance-evaluation-with-rmse",
    "href": "In-class_Ex/In-class_Ex09.html#performance-evaluation-with-rmse",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "yardstick::rmse(test_pred,\n                RESALE_PRICE,\n                grf_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      28745.\n\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                rf_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      29242.\n\n\n\nyardstick::rmse(test_pred,\n                RESALE_PRICE,\n                mlr_pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      61821.\n\n\n\nmc &lt;- test_pred %&gt;%\n  pivot_longer(cols = c(2:4),\n               names_to = \"models\",\n               values_to = \"predicted\")\n\nFinally, we can visualize the actual resale price and the predicted resale price using a scatterplot. This can help us understand how well our model is performing.\n\nggplot(data = mc,\n       aes(x = predicted,\n           y = RESALE_PRICE)) +\n  geom_point()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#basic-building-blocks-of-shiny",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#basic-building-blocks-of-shiny",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "2.1 Basic Building Blocks of Shiny",
    "text": "2.1 Basic Building Blocks of Shiny\nWhen we’re working with Shiny, we basically have a folder that contains an R script named app.R. This script is the heart of our Shiny application. It’s made up of two main parts: a user interface ui object and a server function.\nOne of the cool things about Shiny is that it lets us keep our ui object and server function separate. This means we can clearly split up the code that creates our user interface (that’s the front end, what our users see and interact with) from the code that decides how our application behaves (that’s the back end, the behind-the-scenes stuff). It’s like having a clean, organized workspace, which makes our job a whole lot easier!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#ui",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#ui",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "2.2 UI",
    "text": "2.2 UI\nWhen we’re building a Shiny application, we work with three main components headerPanel, sidebarPanel, and mainPanel. These are the building blocks for our app’s user-interface.\n\nSidebar Panel (sidebarPanel): This is a vertical panel on the side of the application. It is displayed with a distinct background color and typically contains input controls.\nMain Panel (mainPanel): This is the primary area of the application and it typically contains outputs. The main panel displays the output (like maps, plots, tables, etc.) based on the input given in the sidebar panel.\nHeader Panel (headerPanel): This is the topmost part of the UI where you can place the title of our application. It is not always necessary but can be used to provide a title or brief description of our application.\n\n\n\n\n\n\n\nGrid Layout System: FluidRow and Column\nWhen we’re designing the layout of our Shiny app, we get to play around with two functions fluidRow() and column(). Rows are created using the fluidRow() function. Within these rows, we can add columns - but a thing to note here is that columns can only be included within a row. We can define columns using the column() function. The width of these columns is based on the Bootstrap 12-wide grid system. This means we can have up to 12 columns in a single row, giving us a lot of flexibility in how we want to arrange things.\nBy playing around with fluidRow() and column(), we can create a wide variety of layouts. A few examples of different layout grids can be seen below:\n\n\n\n\n\n\n\nHeader Panel: Navbar Pages\nTo create a Shiny application that consists of multiple distinct sub-components (each with their own sidebar, tabsets, or other layout constructs), Shiny provides navbarPage() function to create subsection pages within a single Shiny application. So, no matter how complex our app gets, navbarPage() helps us keep things organized and user-friendly.\n\n\n\n\n\n\n\nSidebar Panel: User Inputs and Controls\nTo insert the user input specifications and controls, Shiny provides functions like sliderInput(), selectInput(), textInput(), numericInput(), checkboxInput(), and checkboxGroupInput(), among others. Each of these functions helps us create a different user interface widget for input specifications and controls. Below is a summary of how different functions create user interface widgets for input specifications and controls.\n\n\n\n\n\nHere are a few functions that are commonly used!\n\nsliderInput(): This function lets us add a slider bar. It’s perfect when we want users to select a value within a range.\nselectInput(): This function creates a dropdown list. It’s great for when we have a list of options and we want users to select one.\ntextInput(): This function gives us a text box. It’s ideal for when we want users to type in some text.\nnumericInput(): This function creates a numeric input field. It’s useful when we need users to enter a number.\ncheckboxInput(): This function lets us add a checkbox. It’s handy when we want users to make a binary choice.\ncheckboxGroupInput(): This function creates a group of checkboxes. It’s perfect for when we want users to select multiple options from a list.\n\nsubmitButton() can be used to compile the specified inputs and controls and send to the server to refresh and update the main panel contents.\n\n\nMain Panel: Outputs\nIn our Shiny app, we can create placeholders in the main panel for outputs. These are later filled in by the server function, which translates our inputs into outputs. There are three main types of output, which correspond to the three things we usually include in a report: text, tables, and plots. Shiny provides functions like textOutput(), tableOutput(), and plotOutput() to define output elements and renderText(), renderTable() and renderPlot() to render and create output elements on user interface. Using fluidRow() and column(), configurations of the output elements can be customised as well.\n\n\n\n\n\n\n\nMain Panel: Tabsets\nShiny provides tabsetPanel() function that allows us to subdivide the main panel into multiple sections. Each section can then show different outputs. It’s like having different tabs in a browser, each showing a different webpage. This way, we can keep our outputs neat and organized, and our users can easily find what they’re looking for."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#server",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#server",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "2.3 Server",
    "text": "2.3 Server\nIn a Shiny application, the server component is responsible for the server-side logic of our application. The server object can include one or more functions that take the inputs from our ui object and turn them into outputs. A server function usually take in an input and an output parameter. The input parameter allows the server to access the UI inputs, and the output parameter is used to define how to display outputs on the UI. An optional parameter session can also be inputted to define the session-related logic to the application.\nSo, how do the u and server interact to run the app? Well, imagine it like a two-way street. The ui sends the inputs to the server, and the server sends back the outputs to be displayed on the ui. It’s a continuous loop of interaction that keeps our app running smoothly. A generi diagram of how they interact to run the application has been formulated below:\n\n\n\n\n\nIn actual fact, there are multiple customisation and extensions available in Shiny to improve the look and feel of our application. However, I only covered basic components and certain customisations from my research that I intend to use in my proposed design."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#prototyping-for-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#prototyping-for-shiny-application",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "1.1 Prototyping for Shiny Application",
    "text": "1.1 Prototyping for Shiny Application\nIn the context of developing a Shiny application with R, prototyping takes on a more specific role. It involves evaluating and determining the necessary R packages that are supported in R CRAN, which forms the backbone of the application. This step ensures that the application is built on a solid and reliable foundation.\nFurthermore, prototyping involves preparing and testing specific R codes to ensure they run correctly and return the expected output. This step is akin to a rehearsal before the actual performance, ensuring that the final product runs smoothly and meets the desired objectives.\nAnother critical aspect of prototyping in Shiny application development is determining the parameters and outputs that will be exposed on the Shiny applications. This process is like setting the stage for user interaction, deciding what the users see and how they interact with the application.\nLastly, prototyping involves selecting the appropriate Shiny UI components for exposing the parameters determined above. This step is where the application starts to take shape, and the user interface begins to reflect the application’s functionality and purpose.\nIn essence, prototyping is a journey of transformation, from abstract ideas to a functional application, ensuring that the final product not only meets design specifications but also provides an engaging and satisfactory user experience. It is the lighthouse that guides the application development process, ensuring that the final product is not just a mere application, but a solution that meets the needs of its users.\nThis exercise is designed as a comprehensive walk-through of the prototyping process for a Shiny application. It will guide you through each step, starting from the basics of Shiny, moving on to the ideation of a generic design, followed by the analysis of R-packages and testing of R codes, which all will contribute to the proposed storyboard of our Shiny application."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#creating-outflow-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#creating-outflow-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.1 Creating Outflow Map",
    "text": "4.1 Creating Outflow Map\nIn this section, we will create outflow map for each hexagon. This map will serve as a visual representation of the total count of outflow movement of people from each hexagon to others.\nOur original dataset is quite large, encompassing a vast amount of data. However, for the purpose of prototyping, we will be utilizing only a fragment of this data. This approach allows us to test the functionality of various R packages and assess the types of visualizations and analytical outcomes that can be produced without exhausting computational resources.\nFor this purpose, I filter the data where DAY_TYPE is “WEEKDAY” and TIME_PER_HOUR is 8, which represents 8 AM in the morning. the resultant data stored as flow_data_weekday_morn. This filtered data, stored as flow_data_weekday_morn, will be used for subsequent map creations.\n\nflow_data_weekday_morn &lt;- flow_data_merged %&gt;% filter(DAY_TYPE == \"WEEKDAY\", TIME_PER_HOUR %in% c(8))\n\nIn our dataset, each origin-destination trajectory is represented as individual row. For map creation, we will aggregate the total trips by origin hexagon. This gives us a summary of the total trips that originated from each hexagon - hence providing outflow volume.\n\noutflow_data_weekday_morn &lt;- aggregate(flow_data_weekday_morn$TOTAL_TRIPS, by=list(Category=flow_data_weekday_morn$ORIGIN_hex), FUN=sum)\n\ncolnames(outflow_data_weekday_morn) &lt;- c(\"index\", \"TOTAL_TRIPS\")\n\nThe newly aggregated dataset that we have is of the data.frame type. While this format is useful for many types of data analysis, it is not directly compatible with mapping functions. Therefore, to facilitate the creation of our outflow maps, we need to join this dataset with the spatial features object hex_grid_pa_sz which is of the sf (simple features) type.\nTo accomplish this, we will use the left_join() function from the dplyr package in R. The left_join() function merges two datasets together based on a common column. In our case, this common column is the index column. This will result in a new sf object called outflow_data_weekday_morn_hex that contains both the outflow data and the corresponding spatial data for each hexagon.\n\noutflow_data_weekday_morn_hex &lt;- left_join(hex_grid_pa_sz, outflow_data_weekday_morn, by = 'index')\n\nIt’s crucial to understand that not every hexagon may have an aggregated value, especially considering that we are working with a subset of the original dataset. To address it, any NA values in the TOTAL_TRIPS column of the outflow_data_weekday_morn_hex data frame will be replaced with 0.\n\noutflow_data_weekday_morn_hex$TOTAL_TRIPS &lt;- ifelse(is.na(outflow_data_weekday_morn_hex$TOTAL_TRIPS), 0, outflow_data_weekday_morn_hex$TOTAL_TRIPS)\n\nFinally we can prepare the outflow map using the appropriate tmap functions.\n\ntmap_mode(\"view\")\ntm_shape(outflow_data_weekday_morn_hex) +\n  tm_fill(col = \"TOTAL_TRIPS\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Outflow Trip Count\",\n          id = \"TOTAL_TRIPS\") +\n  tm_borders(col = \"grey\")\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nNotably, style argument is set to “fixed”. This means that the color breaks are manually specified and breaks argument specifies the boundaries (flow volume count) for the color breaks.\nI choose to manually specify these color breaks because it helps the users to have a consistent interpretation of the colors across different maps. By using the same color breaks, I ensure that the same range of data values corresponds to the same color on all my maps. This makes it easier for the users to compare maps and understand trends and patterns. For instance, if one color represents a range of 0-10 on one map and 0-20 on another, it would be difficult to make easy and accurate comparisons between the two maps. By using fixed color breaks, I eliminate this issue and make my maps more intuitive and user-friendly."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#making-inflow-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#making-inflow-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.2 Making Inflow Map",
    "text": "4.2 Making Inflow Map\nSimilar to what we did for outflow map, we will follow the same process for inflow map as well. Instead of aggregating the data by original hexagon ORIGIN_hex, we will use DESTIN_hex this time, so that we aggregate the total of inflow movement into each hexagon.\n\ninflow_data_weekday_morn &lt;- aggregate(flow_data_weekday_morn$TOTAL_TRIPS, by=list(Category=flow_data_weekday_morn$DESTIN_hex), FUN=sum)\n\ncolnames(inflow_data_weekday_morn) &lt;- c(\"index\", \"TOTAL_TRIPS\")\n\nAfter aggregation, we implement left_join() with hex_grid_pa_sz and then replace the NULL values with 0.\n\ninflow_data_weekday_morn_hex &lt;- left_join(hex_grid_pa_sz, inflow_data_weekday_morn, by = 'index')\n\ninflow_data_weekday_morn_hex$TOTAL_TRIPS &lt;- ifelse(is.na(inflow_data_weekday_morn_hex$TOTAL_TRIPS), 0, inflow_data_weekday_morn_hex$TOTAL_TRIPS)\n\nFinally we can prepare the inflow map using the appropriate tmap functions.\n\ntmap_mode(\"view\")\ntm_shape(inflow_data_weekday_morn_hex) +\n  tm_fill(col = \"TOTAL_TRIPS\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Inflow Trip Count\",\n          id = \"TOTAL_TRIPS\")+\n  tm_borders(col = \"grey\")+\n  tm_layout(legend.title.size = 1,\n            legend.text.size = 0.6,\n            frame = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#making-distribution-graphs",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#making-distribution-graphs",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.3 Making Distribution Graphs",
    "text": "4.3 Making Distribution Graphs\nAnother visualisation that can be produced to support the maps created above is a distribution graph. Although the map provides the specifica spatial patterns of the flow volumes, it is not striaghtforward to discent the distribution. In this regards, we will create distribution graphs to supplement the maps.\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nWhen I create distribution graphs to accompany and supplement my maps, I believe it’s of utmost importance to maintain a consistent representation. To achieve this, I’ll create the histograms using the breaks I specified in the maps as bins and use the same color palette. Essentially, I want users to look at the maps and histograms together and receive a consistent message. This way, they can easily interpret the information and understand the patterns I’m trying to highlight.\n\n\nHere, we will define the breaks, labels and color_map for our distribution graphs. We use the consistent language with the breaks and labels and consistent color palette from the previous overall SIngapore map.\n\nbreaks &lt;- c(0,0.9,100,1000,10000,100000,500000,1000000,5000000)\nlabels &lt;- c(\"0\", \"1 to 100\", \"100 to 1,000\", \"1,000 to 10,000\", \"10,000 to 100,000\", \"100,000 to 500,00\", \"500,000 to 1,000,000\", \"1,000,000 to 5,000,000\")\ncolors = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\")\ncolor_map = setNames(colors, labels)\n\n\nOutflow Distribution Graph\nUsing the breaks and labels, we will create a new data column called TRIPS_BIN which categorizes the TOTAL_TRIPS into different bins based on the breaks we defined earlier.\n\noutflow_data_weekday_morn_hex$TRIPS_BIN &lt;- cut(outflow_data_weekday_morn_hex$TOTAL_TRIPS, breaks = breaks, labels=labels, include.lowest = TRUE, right = FALSE)\nhead(outflow_data_weekday_morn_hex)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2292.538 ymin: 21664.98 xmax: 3417.538 ymax: 28376.68\nProjected CRS: SVY21 / Singapore TM\n  PLN_AREA_N           SUBZONE_N index TOTAL_TRIPS\n1       TUAS TUAS VIEW EXTENSION     1           0\n2       TUAS TUAS VIEW EXTENSION     2           0\n3       TUAS TUAS VIEW EXTENSION     3           0\n4       TUAS TUAS VIEW EXTENSION     4           0\n5       TUAS TUAS VIEW EXTENSION     5           0\n6       TUAS TUAS VIEW EXTENSION     6           0\n                        geometry TRIPS_BIN\n1 POLYGON ((2667.538 22314.5,...         0\n2 POLYGON ((2667.538 23613.54...         0\n3 POLYGON ((2667.538 24912.58...         0\n4 POLYGON ((2667.538 26211.61...         0\n5 POLYGON ((2667.538 27510.65...         0\n6 POLYGON ((3042.538 21664.98...         0\n\n\nNext, we will create distribution graph for outflow map using relevant ggplot2 functions.\n\nggplot(data = outflow_data_weekday_morn_hex,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nLooking at the graphs above, I noticed that the number of hexagons with zero flow volume significantly outweighs the overall distribution. This makes the differences in other categories appear less significant. To address this, I decided to create a new graph where I exclude the hexagons with zero outflow trips. This will allow me to focus on the hexagons with non-zero outflow trips and gain a better understanding of their distribution.\n\n\n\noutflow_data_weekday_morn_nozero &lt;- outflow_data_weekday_morn_hex %&gt;% filter(TOTAL_TRIPS != 0)\n\nNext, we will create distribution graph for outflow map using relevant ggplot2 functions.\n\nggplot(data = outflow_data_weekday_morn_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero outflow trip)\")\n\n\n\n\n\n\nInflow Distribution Graph\nWe will repeat the same procedures for inflow distribution graphs as well.\n\ninflow_data_weekday_morn_hex$TRIPS_BIN &lt;- cut(inflow_data_weekday_morn_hex$TOTAL_TRIPS, breaks = breaks, labels=labels, include.lowest = TRUE, right = FALSE)\nhead(inflow_data_weekday_morn_hex)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2292.538 ymin: 21664.98 xmax: 3417.538 ymax: 28376.68\nProjected CRS: SVY21 / Singapore TM\n  PLN_AREA_N           SUBZONE_N index TOTAL_TRIPS\n1       TUAS TUAS VIEW EXTENSION     1           0\n2       TUAS TUAS VIEW EXTENSION     2           0\n3       TUAS TUAS VIEW EXTENSION     3           0\n4       TUAS TUAS VIEW EXTENSION     4           0\n5       TUAS TUAS VIEW EXTENSION     5           0\n6       TUAS TUAS VIEW EXTENSION     6           0\n                        geometry TRIPS_BIN\n1 POLYGON ((2667.538 22314.5,...         0\n2 POLYGON ((2667.538 23613.54...         0\n3 POLYGON ((2667.538 24912.58...         0\n4 POLYGON ((2667.538 26211.61...         0\n5 POLYGON ((2667.538 27510.65...         0\n6 POLYGON ((3042.538 21664.98...         0\n\n\n\nggplot(data = inflow_data_weekday_morn_hex,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\")\n\n\n\n\n\ninflow_data_weekday_morn_nozero &lt;- inflow_data_weekday_morn_hex %&gt;% filter(TOTAL_TRIPS != 0)\n\n\nggplot(data = inflow_data_weekday_morn_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero inflow trip)\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-into-planning-area",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-into-planning-area",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.4 Zooming into Planning Area",
    "text": "4.4 Zooming into Planning Area\nIn previous section, we only explore the inflow and outflow maps for the whole of Singapore island. However, it may not be intuitive enough to identify patterns at local subzone levels. One possible user calibration in this regard may be to allow the users to specify subzone they want to look into detail.\n\nhead(hex_inflow_morn)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2292.538 ymin: 21810.9 xmax: 3417.538 ymax: 27873.08\nProjected CRS: SVY21 / Singapore TM\n  index TOTAL_TRIPS                       geometry TRIPS_BIN\n1    33           0 POLYGON ((2667.538 21810.9,...         0\n2    34           0 POLYGON ((2667.538 23109.94...         0\n3    35           0 POLYGON ((2667.538 24408.98...         0\n4    36           0 POLYGON ((2667.538 25708.01...         0\n5    37           0 POLYGON ((2667.538 27007.05...         0\n6    60           0 POLYGON ((3042.538 22460.42...         0\n\n\nBased on the user specification, we can filter this data before creating plots and histograms. As an demonstration, we will look at Changi Airpot!\n\ntmap_mode(\"view\")\ntm_shape(hex_grid_pa) +\n  tm_fill(col = \"PLN_AREA_N\",\n          id= \"PLN_AREA_N\")+\n  tm_borders(col = \"grey\")+\n  tm_layout(legend.title.size = 1,\n            legend.text.size = 0.6,\n            frame = TRUE)\n\n\n\n\n\n\n\nmpsz_sf &lt;- st_read(\"~/IS415-GAA/Take-home_Ex/Take-home_Ex03/data/mpsz_sf.shp\")\n\nReading layer `mpsz_sf' from data source \n  `/Users/khantminnaing/IS415-GAA/Take-home_Ex/Take-home_Ex03/data/mpsz_sf.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\nsz_input &lt;- \"PUNGGOL\"\nsz_sf &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == sz_input)\nsz_boundary &lt;- st_combine(sz_sf)\nsz_hex &lt;- st_intersection(hex_inflow_morn, sz_boundary)\nintersection_list = hex_inflow_morn$index[lengths(st_intersects(hex_inflow_morn, sz_hex)) &gt; 0]\nhex_inflow_morn_temp = hex_inflow_morn %&gt;%\n  filter(index %in% intersection_list)\n\n\ntmap_mode(\"view\")\ntm_shape(hex_inflow_morn_temp) +\n  tm_fill(col = \"TOTAL_TRIPS\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Inflow Trip Count\")+\n  tm_borders(col = \"grey\")+\n  tm_layout(legend.title.size = 1,\n            legend.text.size = 0.6,\n            frame = TRUE)\n\n\n\n\n\n\n\nsz_input &lt;- \"HOUGANG\"\nsz_sf &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == sz_input)\nsz_boundary &lt;- st_combine(sz_sf)\nsz_hex &lt;- st_intersection(hex_inflow_morn, sz_boundary)\nintersection_list = hex_inflow_morn$index[lengths(st_intersects(hex_inflow_morn, sz_hex)) &gt; 0]\nhex_inflow_morn_temp = hex_inflow_morn %&gt;%\n  filter(index %in% intersection_list)\ntmap_mode(\"view\")\ntm_shape(hex_inflow_morn_temp) +\n  tm_fill(col = \"TOTAL_TRIPS\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Inflow Trip Count\")+\n  tm_borders(col = \"grey\")+\n  tm_layout(legend.title.size = 1,\n            legend.text.size = 0.6,\n            frame = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#poisson-regression",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#poisson-regression",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.1 Poisson Regression",
    "text": "4.1 Poisson Regression\nSince Poisson Regression is based on log and log 0 is undefined, it is important to ensure that no 0 values in the explanatory variables before running the model. We need to transform the zero values so that log transformation can be done correctly for the explanatory variables according to Poisson Regression.\n\nflow_data_morn &lt;- flow_data_morn %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_morn_nov &lt;- flow_data_morn_nov %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_morn_dec &lt;- flow_data_morn_dec %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_morn_jan &lt;- flow_data_morn_jan %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_even &lt;- flow_data_even %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_even_nov &lt;- flow_data_even_nov %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_even_dec &lt;- flow_data_even_dec %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))\n\nflow_data_even_jan &lt;- flow_data_even_jan %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), ~ ifelse(. == 0, 0.99, .))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#applying-log-transformation-to-explanatory-variables",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#applying-log-transformation-to-explanatory-variables",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "Applying log() Transformation to Explanatory Variables",
    "text": "Applying log() Transformation to Explanatory Variables\n\nflow_data_morn_jan_log &lt;- flow_data_morn_jan %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), log) %&gt;%\n  mutate(dist = log(dist))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#origin-production-constrained-spatial-interaction-model",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#origin-production-constrained-spatial-interaction-model",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.7 Origin (Production) Constrained Spatial Interaction Model",
    "text": "4.7 Origin (Production) Constrained Spatial Interaction Model\nIn this section, we will fit an origin constrained Spatial Interaction Model (SIM). For origin constrained SIM, only explanatory variables representing the attractiveness at the destinations will be used. This is because such models emphasize the limitations or capacities of the origins rather than the demand or attractiveness of the destinations. The capacity or limitation at the origin sites determines the potential for generating interactions or flows.\n\norcSIM_weekday_morn &lt;- glm(TOTAL_TRIPS ~ ORIGIN_hex + d_biz_count + d_school_count + d_fin_count + d_hc_count + d_busstop_count + d_housing_count + d_leisure_recre_count + d_retail_count + d_entertn_count + d_food_bev_count + dist - 1,\n              family = poisson(link = \"log\"),\n              data = flow_data_weekday_morn_log,\n              na.action = na.exclude)\n\nIn the formula argument, we specify our response variable TOTAL_TRIPS and our explanatory variables. The explanatory variables include ORIGIN_hex, various destination counts (e.g., d_biz_count, d_school_count, etc.), and dist. The -1 at the end of the formula is used to remove the intercept that is inserted by glm into the model by default. Since the origin has already been constrained, the concept of an intercept would not be relevant.\nWe will have a look at the results of the origin constrained spatial interaction model that we have just fitted.\n\norcSIM_weekday_morn\n\n\nCall:  glm(formula = TOTAL_TRIPS ~ ORIGIN_hex + d_biz_count + d_school_count + \n    d_fin_count + d_hc_count + d_busstop_count + d_housing_count + \n    d_leisure_recre_count + d_retail_count + d_entertn_count + \n    d_food_bev_count + dist - 1, family = poisson(link = \"log\"), \n    data = flow_data_weekday_morn_log, na.action = na.exclude)\n\nCoefficients:\n         ORIGIN_hex29           ORIGIN_hex31           ORIGIN_hex39  \n            13.941726              14.686527              14.511614  \n         ORIGIN_hex40           ORIGIN_hex41           ORIGIN_hex49  \n            15.778904              12.548848              15.481746  \n         ORIGIN_hex50           ORIGIN_hex51           ORIGIN_hex52  \n            14.448042              12.540246              15.580428  \n         ORIGIN_hex59           ORIGIN_hex60           ORIGIN_hex61  \n            16.418298              14.914117              15.570167  \n         ORIGIN_hex62           ORIGIN_hex63           ORIGIN_hex72  \n            14.939332              11.699034              13.298299  \n         ORIGIN_hex73           ORIGIN_hex74           ORIGIN_hex75  \n            12.827047              13.880245              13.115962  \n         ORIGIN_hex83           ORIGIN_hex84           ORIGIN_hex85  \n            15.843727              14.930012              14.157577  \n         ORIGIN_hex86           ORIGIN_hex87           ORIGIN_hex88  \n            12.878800              13.235495              13.713483  \n         ORIGIN_hex89           ORIGIN_hex96           ORIGIN_hex97  \n            14.333183              15.705128              13.478527  \n         ORIGIN_hex99          ORIGIN_hex100          ORIGIN_hex101  \n            15.656556              17.069264              17.633010  \n        ORIGIN_hex112          ORIGIN_hex113          ORIGIN_hex114  \n            12.712998              14.335755               9.490448  \n        ORIGIN_hex124          ORIGIN_hex125          ORIGIN_hex126  \n            11.158036              14.405082              12.649131  \n        ORIGIN_hex135          ORIGIN_hex136          ORIGIN_hex137  \n            13.751430              17.726486              15.109572  \n        ORIGIN_hex145          ORIGIN_hex146          ORIGIN_hex147  \n            14.442351              13.414143              14.671502  \n        ORIGIN_hex155          ORIGIN_hex156          ORIGIN_hex157  \n            12.901492              13.555591              12.787999  \n        ORIGIN_hex168          ORIGIN_hex169          ORIGIN_hex170  \n            13.614060              16.045080              13.388980  \n        ORIGIN_hex181          ORIGIN_hex182          ORIGIN_hex183  \n            14.431850              13.136444              16.318944  \n        ORIGIN_hex197          ORIGIN_hex198          ORIGIN_hex199  \n            13.374041              12.843618              15.123314  \n        ORIGIN_hex213          ORIGIN_hex214          ORIGIN_hex215  \n            14.133948              13.792650              13.679009  \n        ORIGIN_hex231          ORIGIN_hex232          ORIGIN_hex233  \n            11.864972              12.649000              13.923933  \n        ORIGIN_hex249          ORIGIN_hex250          ORIGIN_hex252  \n            14.318927              12.590541              13.560720  \n        ORIGIN_hex265          ORIGIN_hex266          ORIGIN_hex267  \n            15.251744              14.302182              18.523794  \n        ORIGIN_hex268          ORIGIN_hex269          ORIGIN_hex283  \n            14.848553              15.230772              12.871893  \n        ORIGIN_hex284          ORIGIN_hex286          ORIGIN_hex287  \n            14.823941              14.882120              14.957886  \n        ORIGIN_hex300          ORIGIN_hex301          ORIGIN_hex302  \n            14.685108              14.045611              14.496621  \n        ORIGIN_hex303          ORIGIN_hex304          ORIGIN_hex319  \n            17.099615              15.809899              14.844829  \n        ORIGIN_hex320          ORIGIN_hex321          ORIGIN_hex322  \n            15.050513              16.869942              14.963639  \n        ORIGIN_hex323          ORIGIN_hex334          ORIGIN_hex335  \n            12.706506              13.272007              14.371458  \n        ORIGIN_hex336          ORIGIN_hex337          ORIGIN_hex339  \n            14.782400              17.557906              14.237878  \n        ORIGIN_hex340          ORIGIN_hex351          ORIGIN_hex352  \n            15.203247              16.391398              13.927078  \n        ORIGIN_hex353          ORIGIN_hex354          ORIGIN_hex355  \n            16.592074              17.671888              15.454003  \n        ORIGIN_hex356          ORIGIN_hex357          ORIGIN_hex367  \n            11.976002              12.362910              15.152006  \n        ORIGIN_hex368          ORIGIN_hex369          ORIGIN_hex370  \n            14.709301              14.026531              18.372584  \n        ORIGIN_hex371          ORIGIN_hex373          ORIGIN_hex374  \n            18.027871              12.676713              11.958584  \n        ORIGIN_hex375          ORIGIN_hex376          ORIGIN_hex377  \n            13.566971              11.934741              13.742155  \n        ORIGIN_hex385          ORIGIN_hex386          ORIGIN_hex387  \n            10.686091              12.905636              14.193445  \n        ORIGIN_hex388          ORIGIN_hex390          ORIGIN_hex393  \n            17.644868              12.684819              14.400442  \n        ORIGIN_hex394          ORIGIN_hex395          ORIGIN_hex402  \n            15.097358              12.594786              14.357819  \n        ORIGIN_hex403          ORIGIN_hex404          ORIGIN_hex405  \n            14.228550              18.904142              16.998255  \n        ORIGIN_hex407          ORIGIN_hex411          ORIGIN_hex412  \n            10.682999              11.468764              12.235389  \n        ORIGIN_hex413          ORIGIN_hex419          ORIGIN_hex420  \n            16.169922              13.705286              15.350403  \n        ORIGIN_hex421          ORIGIN_hex422          ORIGIN_hex424  \n            13.784245              17.730347              11.862284  \n        ORIGIN_hex430          ORIGIN_hex437          ORIGIN_hex438  \n            13.092312              13.679218              15.626560  \n        ORIGIN_hex439          ORIGIN_hex440          ORIGIN_hex442  \n            16.397451              16.746905              14.310808  \n        ORIGIN_hex453          ORIGIN_hex454          ORIGIN_hex455  \n            13.896835              12.315058              14.040773  \n        ORIGIN_hex456          ORIGIN_hex471          ORIGIN_hex472  \n            17.900772              15.489828              16.898744  \n        ORIGIN_hex473          ORIGIN_hex474          ORIGIN_hex476  \n            16.760433              17.906871              16.957158  \n        ORIGIN_hex487          ORIGIN_hex488          ORIGIN_hex489  \n            15.443727              13.906721              17.307152  \n        ORIGIN_hex490          ORIGIN_hex504          ORIGIN_hex505  \n            16.132467              15.997330              17.133876  \n        ORIGIN_hex506          ORIGIN_hex508          ORIGIN_hex518  \n            17.509539              13.131467              14.877898  \n        ORIGIN_hex521          ORIGIN_hex522          ORIGIN_hex524  \n            16.657164              12.577267              13.372622  \n        ORIGIN_hex533          ORIGIN_hex534          ORIGIN_hex536  \n            13.631265              16.066051              16.532694  \n        ORIGIN_hex537          ORIGIN_hex539          ORIGIN_hex549  \n            17.006909              16.997830              17.316585  \n        ORIGIN_hex550          ORIGIN_hex551          ORIGIN_hex552  \n            17.211997              17.527378              17.243546  \n        ORIGIN_hex554          ORIGIN_hex555          ORIGIN_hex559  \n            18.379012              12.273256              12.981431  \n        ORIGIN_hex562          ORIGIN_hex564          ORIGIN_hex565  \n            13.758394              14.712020              16.454318  \n        ORIGIN_hex566          ORIGIN_hex567          ORIGIN_hex568  \n            17.085712              16.411728              18.284361  \n        ORIGIN_hex569          ORIGIN_hex577          ORIGIN_hex579  \n            16.586100              11.873080              16.322517  \n        ORIGIN_hex580          ORIGIN_hex581          ORIGIN_hex582  \n            17.091905              17.337976              17.214483  \n        ORIGIN_hex583          ORIGIN_hex584          ORIGIN_hex585  \n            16.227926              17.108052              17.942843  \n        ORIGIN_hex586          ORIGIN_hex588          ORIGIN_hex589  \n            16.458943              13.222997              14.602615  \n        ORIGIN_hex593          ORIGIN_hex594          ORIGIN_hex595  \n            13.996657              16.143341              15.492652  \n        ORIGIN_hex596          ORIGIN_hex597          ORIGIN_hex598  \n            17.705209              17.018654              17.850524  \n        ORIGIN_hex599          ORIGIN_hex600          ORIGIN_hex601  \n            17.493733              17.978466              17.434516  \n        ORIGIN_hex603          ORIGIN_hex604          ORIGIN_hex609  \n            13.538053              15.070148              14.075396  \n        ORIGIN_hex610          ORIGIN_hex611          ORIGIN_hex612  \n            14.375296              16.078312              14.962767  \n        ORIGIN_hex613          ORIGIN_hex614          ORIGIN_hex615  \n            16.483729              17.047782              16.759729  \n        ORIGIN_hex616          ORIGIN_hex617          ORIGIN_hex618  \n            17.726710              16.541255              18.493332  \n        ORIGIN_hex619          ORIGIN_hex620          ORIGIN_hex625  \n            14.119358              12.987757              13.520039  \n        ORIGIN_hex626          ORIGIN_hex627          ORIGIN_hex628  \n            15.440559              14.767378              16.005124  \n        ORIGIN_hex629          ORIGIN_hex630          ORIGIN_hex631  \n            18.196375              17.020451              16.004897  \n        ORIGIN_hex632          ORIGIN_hex633          ORIGIN_hex634  \n            16.176995              15.217277              15.002547  \n        ORIGIN_hex635          ORIGIN_hex636          ORIGIN_hex643  \n            13.438600              14.515111              16.469511  \n        ORIGIN_hex644          ORIGIN_hex645          ORIGIN_hex646  \n            16.461607              15.105560              17.751026  \n        ORIGIN_hex649          ORIGIN_hex650          ORIGIN_hex651  \n            16.889481              14.802467              16.281555  \n        ORIGIN_hex652          ORIGIN_hex653          ORIGIN_hex654  \n            14.087886              12.105089              12.745382  \n        ORIGIN_hex659          ORIGIN_hex660          ORIGIN_hex661  \n            16.314851              16.810256              15.060689  \n        ORIGIN_hex662          ORIGIN_hex663          ORIGIN_hex665  \n            16.094337              16.547124              15.315637  \n        ORIGIN_hex666          ORIGIN_hex668          ORIGIN_hex669  \n            17.295610              14.159800              16.718666  \n        ORIGIN_hex670          ORIGIN_hex676          ORIGIN_hex677  \n            15.898829              16.887221              16.928924  \n        ORIGIN_hex678          ORIGIN_hex680          ORIGIN_hex681  \n            17.290542              14.728050              17.129593  \n        ORIGIN_hex682          ORIGIN_hex683          ORIGIN_hex687  \n            15.744141              18.133636              18.494135  \n        ORIGIN_hex688          ORIGIN_hex693          ORIGIN_hex694  \n            15.390381              15.944302              17.741531  \n        ORIGIN_hex695          ORIGIN_hex696          ORIGIN_hex697  \n            18.829296              15.471369              16.258686  \n        ORIGIN_hex698          ORIGIN_hex699          ORIGIN_hex700  \n            14.840163              16.779738              16.824952  \n        ORIGIN_hex701          ORIGIN_hex703          ORIGIN_hex706  \n            17.904865              12.879872              19.579615  \n        ORIGIN_hex711          ORIGIN_hex712          ORIGIN_hex713  \n            15.085365              17.288895              16.110780  \n        ORIGIN_hex715          ORIGIN_hex716          ORIGIN_hex718  \n            16.791325              16.035828              17.205091  \n        ORIGIN_hex722          ORIGIN_hex723          ORIGIN_hex729  \n            16.422833              16.915723              15.749121  \n        ORIGIN_hex730          ORIGIN_hex731          ORIGIN_hex732  \n            11.246370              16.433882              15.708651  \n        ORIGIN_hex733          ORIGIN_hex735          ORIGIN_hex736  \n            17.056827              14.742062              18.322596  \n        ORIGIN_hex737          ORIGIN_hex739          ORIGIN_hex741  \n            17.500627              15.314887              18.058900  \n        ORIGIN_hex742          ORIGIN_hex746          ORIGIN_hex747  \n            17.639043              15.949597              14.222913  \n        ORIGIN_hex748          ORIGIN_hex749          ORIGIN_hex750  \n            17.243831              16.292277              15.314301  \n        ORIGIN_hex751          ORIGIN_hex753          ORIGIN_hex754  \n            16.011234              16.916974              15.869324  \n        ORIGIN_hex758          ORIGIN_hex759          ORIGIN_hex760  \n            16.185841              17.590699              13.796223  \n        ORIGIN_hex764          ORIGIN_hex766          ORIGIN_hex767  \n            17.558951              16.757200              15.802759  \n        ORIGIN_hex769          ORIGIN_hex775          ORIGIN_hex777  \n            16.134404              15.558900              17.145237  \n        ORIGIN_hex778          ORIGIN_hex782          ORIGIN_hex783  \n            17.457578              13.933052              16.234259  \n        ORIGIN_hex784          ORIGIN_hex785          ORIGIN_hex786  \n            17.206332              14.845523              18.094206  \n        ORIGIN_hex794          ORIGIN_hex795          ORIGIN_hex796  \n            14.694122              18.853688              18.207146  \n        ORIGIN_hex799          ORIGIN_hex800          ORIGIN_hex801  \n            16.082331              16.151337              14.860399  \n        ORIGIN_hex802          ORIGIN_hex810          ORIGIN_hex812  \n            15.348788              12.570259              17.661687  \n        ORIGIN_hex813          ORIGIN_hex816          ORIGIN_hex817  \n            16.923569              16.441912              15.711776  \n        ORIGIN_hex818          ORIGIN_hex819          ORIGIN_hex820  \n            14.263621              17.569668              14.997069  \n        ORIGIN_hex821          ORIGIN_hex827          ORIGIN_hex829  \n            15.841442              14.670104              15.720891  \n        ORIGIN_hex830          ORIGIN_hex831          ORIGIN_hex835  \n            17.667153              16.898253              15.088688  \n        ORIGIN_hex836          ORIGIN_hex837          ORIGIN_hex838  \n            16.142949              15.487210              15.747397  \n        ORIGIN_hex847          ORIGIN_hex848          ORIGIN_hex849  \n            17.735052              17.663137              15.130504  \n        ORIGIN_hex851          ORIGIN_hex853          ORIGIN_hex854  \n            15.361874              15.640549              15.580845  \n        ORIGIN_hex856          ORIGIN_hex863          ORIGIN_hex864  \n            16.526579              14.885474              15.370808  \n        ORIGIN_hex865          ORIGIN_hex866          ORIGIN_hex867  \n            18.493684              17.028145               8.989600  \n        ORIGIN_hex868          ORIGIN_hex869          ORIGIN_hex870  \n            14.269283              15.386265              16.608807  \n        ORIGIN_hex871          ORIGIN_hex873          ORIGIN_hex882  \n            17.590288              15.126386              18.014057  \n        ORIGIN_hex883          ORIGIN_hex884          ORIGIN_hex885  \n            17.725318              16.106893              12.387087  \n        ORIGIN_hex887          ORIGIN_hex888          ORIGIN_hex889  \n            15.871955              16.633082              15.969107  \n        ORIGIN_hex890          ORIGIN_hex899          ORIGIN_hex901  \n            15.906660              14.505301              17.346807  \n        ORIGIN_hex902          ORIGIN_hex903          ORIGIN_hex905  \n            17.112726              14.939543              17.017832  \n        ORIGIN_hex906          ORIGIN_hex907          ORIGIN_hex909  \n            16.329331              16.982982              16.597466  \n        ORIGIN_hex910          ORIGIN_hex917          ORIGIN_hex920  \n            15.983674              12.577237              17.731915  \n        ORIGIN_hex922          ORIGIN_hex925          ORIGIN_hex926  \n            14.252100              16.857232              16.566994  \n        ORIGIN_hex927          ORIGIN_hex928          ORIGIN_hex929  \n            16.302026              14.069325              15.766700  \n        ORIGIN_hex936          ORIGIN_hex937          ORIGIN_hex940  \n            14.942155              13.734781              16.754238  \n        ORIGIN_hex941          ORIGIN_hex944          ORIGIN_hex945  \n            15.962533              15.796854              16.576152  \n        ORIGIN_hex946          ORIGIN_hex947          ORIGIN_hex949  \n            16.268667              14.389526              15.294318  \n        ORIGIN_hex955          ORIGIN_hex959          ORIGIN_hex960  \n            13.321699              17.035019              17.439962  \n        ORIGIN_hex962          ORIGIN_hex963          ORIGIN_hex964  \n            14.217816              16.502438              16.799492  \n        ORIGIN_hex965          ORIGIN_hex966          ORIGIN_hex968  \n            17.610421              14.782014              16.303432  \n        ORIGIN_hex969          ORIGIN_hex974          ORIGIN_hex975  \n            12.981431              11.998804              16.190043  \n        ORIGIN_hex978          ORIGIN_hex979          ORIGIN_hex980  \n            15.040986              18.468961              16.290940  \n        ORIGIN_hex983          ORIGIN_hex984          ORIGIN_hex985  \n            17.399461              15.938544              15.296994  \n        ORIGIN_hex986          ORIGIN_hex989          ORIGIN_hex993  \n            15.823569              13.511580              12.576805  \n        ORIGIN_hex994          ORIGIN_hex995          ORIGIN_hex996  \n            17.525395              13.642925              12.447830  \n        ORIGIN_hex998          ORIGIN_hex999         ORIGIN_hex1002  \n            17.337974              16.835998              11.170660  \n       ORIGIN_hex1004         ORIGIN_hex1005         ORIGIN_hex1006  \n            17.257431              17.775195              14.798336  \n       ORIGIN_hex1008         ORIGIN_hex1013         ORIGIN_hex1015  \n            16.987636              15.591885              15.785615  \n       ORIGIN_hex1016         ORIGIN_hex1017         ORIGIN_hex1018  \n            14.053078              16.468865              16.531290  \n       ORIGIN_hex1019         ORIGIN_hex1023         ORIGIN_hex1024  \n            16.835881              15.561017              17.173393  \n       ORIGIN_hex1025         ORIGIN_hex1026         ORIGIN_hex1027  \n            16.471941              15.656089              15.712516  \n       ORIGIN_hex1028         ORIGIN_hex1029         ORIGIN_hex1031  \n            13.510506              11.737981              15.589593  \n       ORIGIN_hex1032         ORIGIN_hex1034         ORIGIN_hex1035  \n            16.238516              13.638489              13.265115  \n       ORIGIN_hex1036         ORIGIN_hex1037         ORIGIN_hex1038  \n            15.968548              18.327185              16.897474  \n       ORIGIN_hex1039         ORIGIN_hex1044         ORIGIN_hex1045  \n            14.388199              16.265022              16.307244  \n       ORIGIN_hex1046         ORIGIN_hex1047         ORIGIN_hex1048  \n            15.526123              15.357702              14.947427  \n       ORIGIN_hex1051         ORIGIN_hex1052         ORIGIN_hex1053  \n            16.537455              15.903160              15.486591  \n       ORIGIN_hex1056         ORIGIN_hex1057         ORIGIN_hex1058  \n            18.316466              16.990604              16.660822  \n       ORIGIN_hex1059         ORIGIN_hex1063         ORIGIN_hex1065  \n            17.925714              12.629304              14.115007  \n       ORIGIN_hex1067         ORIGIN_hex1068         ORIGIN_hex1069  \n            16.279083              15.267579              16.071086  \n       ORIGIN_hex1070         ORIGIN_hex1071         ORIGIN_hex1072  \n            16.404002              16.414358              16.890049  \n       ORIGIN_hex1073         ORIGIN_hex1075         ORIGIN_hex1076  \n            14.866919              15.282488              17.246158  \n       ORIGIN_hex1077         ORIGIN_hex1078         ORIGIN_hex1079  \n            17.860944              17.261760              16.045694  \n       ORIGIN_hex1083         ORIGIN_hex1085         ORIGIN_hex1086  \n            16.489408              15.455811              16.682858  \n       ORIGIN_hex1088         ORIGIN_hex1089         ORIGIN_hex1090  \n            15.200457              14.574898              16.175910  \n       ORIGIN_hex1091         ORIGIN_hex1092         ORIGIN_hex1093  \n            17.238538              17.511649              16.024937  \n       ORIGIN_hex1095         ORIGIN_hex1096         ORIGIN_hex1097  \n            18.051446              18.422598              17.190644  \n       ORIGIN_hex1099         ORIGIN_hex1103         ORIGIN_hex1104  \n            16.705139              14.469390              15.131660  \n       ORIGIN_hex1106         ORIGIN_hex1107         ORIGIN_hex1108  \n            16.332404              17.372863              16.618729  \n       ORIGIN_hex1109         ORIGIN_hex1110         ORIGIN_hex1111  \n            16.203701              16.634239              16.824714  \n       ORIGIN_hex1112         ORIGIN_hex1115         ORIGIN_hex1116  \n            16.590934              17.732854              18.821337  \n       ORIGIN_hex1124         ORIGIN_hex1125         ORIGIN_hex1126  \n            15.540055              16.520968              17.763893  \n       ORIGIN_hex1127         ORIGIN_hex1128         ORIGIN_hex1129  \n            15.016002              17.937846              17.547068  \n       ORIGIN_hex1130         ORIGIN_hex1131         ORIGIN_hex1133  \n            18.173383              14.674195              18.063239  \n       ORIGIN_hex1134         ORIGIN_hex1142         ORIGIN_hex1143  \n            18.325317              17.124013              16.151998  \n       ORIGIN_hex1144         ORIGIN_hex1145         ORIGIN_hex1146  \n            16.140934              16.932807              17.306902  \n       ORIGIN_hex1147         ORIGIN_hex1148         ORIGIN_hex1149  \n            16.943247              16.366466              14.153610  \n       ORIGIN_hex1152         ORIGIN_hex1153         ORIGIN_hex1161  \n            17.251356              17.515032              14.206411  \n       ORIGIN_hex1162         ORIGIN_hex1163         ORIGIN_hex1164  \n            16.985970              17.045217              17.021968  \n       ORIGIN_hex1165         ORIGIN_hex1166         ORIGIN_hex1168  \n            15.956534              18.039248              15.606248  \n       ORIGIN_hex1171         ORIGIN_hex1178         ORIGIN_hex1179  \n            18.000270              16.113331              16.410992  \n       ORIGIN_hex1180         ORIGIN_hex1181         ORIGIN_hex1182  \n            17.106675              16.758149              17.443294  \n       ORIGIN_hex1183         ORIGIN_hex1187         ORIGIN_hex1194  \n            17.388785              16.313968              14.005661  \n       ORIGIN_hex1196         ORIGIN_hex1197         ORIGIN_hex1198  \n            16.168565              18.444903              16.326346  \n       ORIGIN_hex1199         ORIGIN_hex1200         ORIGIN_hex1202  \n            14.308318              17.633565              16.255573  \n       ORIGIN_hex1203         ORIGIN_hex1205         ORIGIN_hex1212  \n            14.665980              15.769942              15.894448  \n       ORIGIN_hex1213         ORIGIN_hex1214         ORIGIN_hex1215  \n            17.140770              16.639706              15.651148  \n       ORIGIN_hex1216         ORIGIN_hex1217         ORIGIN_hex1218  \n            15.182060              15.083403              14.645178  \n       ORIGIN_hex1220         ORIGIN_hex1221         ORIGIN_hex1226  \n            12.190644              13.603521              15.321408  \n       ORIGIN_hex1227         ORIGIN_hex1228         ORIGIN_hex1229  \n            14.808829              17.737910              17.374194  \n       ORIGIN_hex1230         ORIGIN_hex1231         ORIGIN_hex1232  \n            15.021460              15.807932              15.938570  \n       ORIGIN_hex1233         ORIGIN_hex1234         ORIGIN_hex1235  \n            15.554207              14.264822              13.310347  \n       ORIGIN_hex1236         ORIGIN_hex1243         ORIGIN_hex1244  \n            12.019698              14.230935              17.622531  \n       ORIGIN_hex1245         ORIGIN_hex1246         ORIGIN_hex1247  \n            17.019429              16.791324              16.459718  \n       ORIGIN_hex1248         ORIGIN_hex1249         ORIGIN_hex1251  \n            14.874189              16.217108              13.660673  \n       ORIGIN_hex1252         ORIGIN_hex1257         ORIGIN_hex1258  \n            13.387452              16.705565              16.995316  \n       ORIGIN_hex1259         ORIGIN_hex1260         ORIGIN_hex1261  \n            16.346664              16.643829              15.831271  \n       ORIGIN_hex1262         ORIGIN_hex1263         ORIGIN_hex1264  \n            17.024770              17.312161              15.659703  \n       ORIGIN_hex1265         ORIGIN_hex1266         ORIGIN_hex1267  \n            17.436759              17.443253              15.152703  \n       ORIGIN_hex1272         ORIGIN_hex1273         ORIGIN_hex1274  \n            14.680517              15.989261              15.216407  \n       ORIGIN_hex1275         ORIGIN_hex1276         ORIGIN_hex1277  \n            15.702071              17.645803              16.498808  \n       ORIGIN_hex1278         ORIGIN_hex1279         ORIGIN_hex1280  \n            17.819289              16.530316              16.797529  \n       ORIGIN_hex1281         ORIGIN_hex1285         ORIGIN_hex1286  \n            12.843386              11.792834              16.577600  \n       ORIGIN_hex1287         ORIGIN_hex1288         ORIGIN_hex1289  \n            15.608486              16.730238              15.824725  \n       ORIGIN_hex1290         ORIGIN_hex1291         ORIGIN_hex1292  \n            15.828193              17.092593              17.237887  \n       ORIGIN_hex1293         ORIGIN_hex1299         ORIGIN_hex1300  \n            17.187038              16.272499              17.272346  \n       ORIGIN_hex1301         ORIGIN_hex1302         ORIGIN_hex1303  \n            16.023360              14.518261              16.467258  \n       ORIGIN_hex1305         ORIGIN_hex1306         ORIGIN_hex1307  \n            17.907382              17.816466              17.410182  \n       ORIGIN_hex1312         ORIGIN_hex1313         ORIGIN_hex1314  \n            16.298185              17.125249              15.896842  \n       ORIGIN_hex1315         ORIGIN_hex1316         ORIGIN_hex1317  \n            16.438264              14.549282              16.716732  \n       ORIGIN_hex1318         ORIGIN_hex1319         ORIGIN_hex1320  \n            17.026531              17.174566              17.677045  \n       ORIGIN_hex1326         ORIGIN_hex1327         ORIGIN_hex1328  \n            15.960319              16.789044              15.466883  \n       ORIGIN_hex1329         ORIGIN_hex1330         ORIGIN_hex1331  \n            17.280776              17.670690              17.275482  \n       ORIGIN_hex1332         ORIGIN_hex1333         ORIGIN_hex1336  \n            16.416664              17.170094              15.474541  \n       ORIGIN_hex1337         ORIGIN_hex1338         ORIGIN_hex1339  \n            16.378142              16.865610              15.104971  \n       ORIGIN_hex1340         ORIGIN_hex1341         ORIGIN_hex1342  \n            13.457586              16.646999              17.737851  \n       ORIGIN_hex1343         ORIGIN_hex1344         ORIGIN_hex1345  \n            16.989639              17.500813              18.077687  \n       ORIGIN_hex1348         ORIGIN_hex1349         ORIGIN_hex1350  \n            16.032886              16.507581              14.926218  \n       ORIGIN_hex1351         ORIGIN_hex1352         ORIGIN_hex1353  \n            14.689424              13.201876              17.409218  \n       ORIGIN_hex1354         ORIGIN_hex1355         ORIGIN_hex1356  \n            17.214238              17.631644              16.856051  \n       ORIGIN_hex1357         ORIGIN_hex1359         ORIGIN_hex1360  \n            17.009585              14.546477              16.645838  \n       ORIGIN_hex1362         ORIGIN_hex1363         ORIGIN_hex1365  \n            16.256283              14.137071              17.266998  \n       ORIGIN_hex1366         ORIGIN_hex1367         ORIGIN_hex1368  \n            16.875438              17.720569              18.070349  \n       ORIGIN_hex1369         ORIGIN_hex1370         ORIGIN_hex1371  \n            15.750391              16.507398              16.862575  \n       ORIGIN_hex1372         ORIGIN_hex1373         ORIGIN_hex1374  \n            17.475842              14.681814              15.317118  \n       ORIGIN_hex1375         ORIGIN_hex1376         ORIGIN_hex1377  \n            14.173151              17.198511              17.539748  \n       ORIGIN_hex1378         ORIGIN_hex1379         ORIGIN_hex1382  \n            17.992754              15.965411              16.112638  \n       ORIGIN_hex1383         ORIGIN_hex1384         ORIGIN_hex1388  \n            17.360037              16.447788              16.743708  \n       ORIGIN_hex1389         ORIGIN_hex1390         ORIGIN_hex1391  \n            18.617362              17.829950              16.962478  \n       ORIGIN_hex1392         ORIGIN_hex1393         ORIGIN_hex1394  \n            16.936814              16.444998              15.652475  \n       ORIGIN_hex1395         ORIGIN_hex1397         ORIGIN_hex1399  \n            14.115114              15.205715              17.034757  \n       ORIGIN_hex1400         ORIGIN_hex1401         ORIGIN_hex1402  \n            17.105069              14.646578              15.444933  \n       ORIGIN_hex1404         ORIGIN_hex1405         ORIGIN_hex1406  \n            16.227093              16.895880              17.652443  \n       ORIGIN_hex1409         ORIGIN_hex1410         ORIGIN_hex1411  \n            15.636668              11.433354              16.828489  \n       ORIGIN_hex1412         ORIGIN_hex1413         ORIGIN_hex1414  \n            17.122658              12.639092              16.673068  \n       ORIGIN_hex1415         ORIGIN_hex1416         ORIGIN_hex1417  \n            14.506310              16.400640              16.164204  \n       ORIGIN_hex1420         ORIGIN_hex1422         ORIGIN_hex1426  \n            11.291291              17.080428              16.122460  \n       ORIGIN_hex1427         ORIGIN_hex1428         ORIGIN_hex1429  \n            15.582142              16.538496              15.043929  \n       ORIGIN_hex1432         ORIGIN_hex1433         ORIGIN_hex1434  \n            12.998846              15.902035              17.530742  \n       ORIGIN_hex1437         ORIGIN_hex1438         ORIGIN_hex1439  \n            15.801557              17.502380              17.352608  \n       ORIGIN_hex1442         ORIGIN_hex1444         ORIGIN_hex1446  \n            16.287854              17.215700              16.356398  \n       ORIGIN_hex1447         ORIGIN_hex1448         ORIGIN_hex1450  \n            17.343619              17.375499              12.080689  \n       ORIGIN_hex1451         ORIGIN_hex1456         ORIGIN_hex1457  \n            14.117698              15.507512              17.558390  \n       ORIGIN_hex1458         ORIGIN_hex1459         ORIGIN_hex1460  \n            16.542813              17.608407              15.953953  \n       ORIGIN_hex1461         ORIGIN_hex1465         ORIGIN_hex1466  \n            16.488393              16.163076              17.322503  \n       ORIGIN_hex1467         ORIGIN_hex1468         ORIGIN_hex1469  \n            17.431679              16.680450              17.831600  \n       ORIGIN_hex1470         ORIGIN_hex1471         ORIGIN_hex1475  \n            16.606411              17.249899              16.309334  \n       ORIGIN_hex1476         ORIGIN_hex1477         ORIGIN_hex1478  \n            17.123746              16.120480              17.391347  \n       ORIGIN_hex1479         ORIGIN_hex1480         ORIGIN_hex1481  \n            17.359886              17.405422              15.159810  \n       ORIGIN_hex1484         ORIGIN_hex1485         ORIGIN_hex1486  \n            16.992488              17.214747              17.004646  \n       ORIGIN_hex1487         ORIGIN_hex1488         ORIGIN_hex1489  \n            17.388341              17.088861              17.399317  \n       ORIGIN_hex1491         ORIGIN_hex1492         ORIGIN_hex1493  \n            16.370831              17.454187              13.870729  \n       ORIGIN_hex1494         ORIGIN_hex1495         ORIGIN_hex1496  \n            18.094034              17.132408              17.966495  \n       ORIGIN_hex1497         ORIGIN_hex1499         ORIGIN_hex1500  \n            15.233620              17.240729              15.453620  \n       ORIGIN_hex1501         ORIGIN_hex1502         ORIGIN_hex1503  \n            16.077557              17.482216              15.834722  \n       ORIGIN_hex1504         ORIGIN_hex1506         ORIGIN_hex1507  \n            16.481122              16.046127              17.800950  \n       ORIGIN_hex1508         ORIGIN_hex1509         ORIGIN_hex1510  \n            16.698743              16.864839              16.553998  \n       ORIGIN_hex1511         ORIGIN_hex1513         ORIGIN_hex1514  \n            17.876134              14.019724              16.600677  \n       ORIGIN_hex1515         ORIGIN_hex1516         ORIGIN_hex1517  \n            15.336025              16.952507              17.437992  \n       ORIGIN_hex1518         ORIGIN_hex1522         ORIGIN_hex1523  \n            17.529659              15.642628              17.169452  \n       ORIGIN_hex1524         ORIGIN_hex1525         ORIGIN_hex1528  \n            17.093399              17.505508              16.436929  \n       ORIGIN_hex1529         ORIGIN_hex1530         ORIGIN_hex1531  \n            16.634541              17.518844              17.467879  \n       ORIGIN_hex1532         ORIGIN_hex1534         ORIGIN_hex1535  \n            17.675980              13.412019              16.040201  \n       ORIGIN_hex1536         ORIGIN_hex1537         ORIGIN_hex1538  \n            16.293275              17.041024              17.285379  \n       ORIGIN_hex1540         ORIGIN_hex1541         ORIGIN_hex1542  \n            13.999549              16.403323              17.484509  \n       ORIGIN_hex1543         ORIGIN_hex1544         ORIGIN_hex1546  \n            16.445268              17.040666              13.604972  \n       ORIGIN_hex1547         ORIGIN_hex1548         ORIGIN_hex1549  \n            12.548487              14.620608              15.333143  \n       ORIGIN_hex1550         ORIGIN_hex1551         ORIGIN_hex1553  \n            16.322607              13.821933              14.513254  \n       ORIGIN_hex1554         ORIGIN_hex1555         ORIGIN_hex1556  \n            15.959938              14.946524              15.219641  \n       ORIGIN_hex1557         ORIGIN_hex1563         ORIGIN_hex1564  \n            13.536711              15.066083              15.121793  \n       ORIGIN_hex1565         ORIGIN_hex1570         ORIGIN_hex1571  \n            14.600555              15.815852              13.526481  \n       ORIGIN_hex1572         ORIGIN_hex1575         ORIGIN_hex1579  \n            14.400076              17.271088              15.722251  \n       ORIGIN_hex1583         ORIGIN_hex1584         ORIGIN_hex1587  \n            15.684437              17.492428              16.070496  \n       ORIGIN_hex1588         ORIGIN_hex1592         ORIGIN_hex1594  \n            16.907067              16.901499              16.394029  \n       ORIGIN_hex1602         ORIGIN_hex1603         ORIGIN_hex1608  \n            12.497485              15.908894              16.867101  \n       ORIGIN_hex1609         ORIGIN_hex1616         ORIGIN_hex1623  \n            16.939043              15.914756              15.966861  \n       ORIGIN_hex1630         ORIGIN_hex1643         ORIGIN_hex1644  \n            11.262349              15.911123              17.598697  \n       ORIGIN_hex1665            d_biz_count         d_school_count  \n            15.216507               0.134182              -0.223142  \n          d_fin_count             d_hc_count        d_busstop_count  \n             0.429166               0.007091               0.250849  \n      d_housing_count  d_leisure_recre_count         d_retail_count  \n            -0.043324              -0.015484               0.093737  \n      d_entertn_count       d_food_bev_count                   dist  \n             0.211118              -0.272079              -1.561917  \n\nDegrees of Freedom: 51926 Total (i.e. Null);  51119 Residual\nNull Deviance:      2.38e+08 \nResidual Deviance: 30360000     AIC: 30640000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#original-constraint-flow-estimation-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#original-constraint-flow-estimation-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.10 Original Constraint Flow Estimation Map",
    "text": "4.10 Original Constraint Flow Estimation Map\nLooking at the model outputs above, the results are not intuitive enough for users, especially those without background to understand and interpret. In this session, we will test and try different visualization approaches to better present the model results.\nparticularly, we are interested to test a R package called stplanr which provides functions for transport modelling such as creating geographic “desire lines” from origin-destination data.\n\npacman::p_load(stplanr)\n\nDesire lines are a powerful tool for visualizing the flow of movement between different locations. However, they are not suitable for visualizing intra-regional movements, where both the origin and destination belong to a single spatial unit. Therefore, we need to filter out intra-regional trajectories from our dataset first.\n\nod_plot &lt;- fitted_flow_weekday_morn[fitted_flow_weekday_morn$ORIGIN_hex!=fitted_flow_weekday_morn$DESTIN_hex,]\n\nNext, we will create a flowLine object which compiles all the desire lines from our dataset. This is done using the od2line() function from the stplanr package.\n\nflowLine &lt;- od2line(flow = od_plot, \n                    zones = hex_grid_pa_sz,\n                    zone_code = \"index\")\nflowLine &lt;- st_join(flowLine, hex_grid_pa_sz,by = \"index\")\n\nNext, we will filter out the desire lines with flow volume less than 5000. This is to ensure that we only visualize the most significant flow lines.\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nDuring the initial stages of my prototyping, I hadn’t considered the idea of filtering the desire lines based on flow volume. However, as I iterated and refined my approach, I realized that having too many lines on a single map could potentially limit its interpretability, making it difficult to discern meaningful patterns. This led me to the conclusion that it might be more effective to focus on flows with significant volume. By doing so, I could reduce the clutter and make the map more readable, allowing for a more detailed and focused analysis.\n\n\n\norc_flowLine &lt;- flowLine %&gt;% filter(ORCEstimatedFlow &gt; 5000)\n\nNext, we will generate maps for inflow and outflow, akin to our previous approach. However, this time, we will utilize the fitted values derived from the Origin Constrained SIM model. The process involves aggregating the dataset based on either the ORIGIN_hex or DESTIN_hex. Subsequently, we will integrate this aggregated data with the hexagonal grid data for visualization purposes.\n\norc_fitted_inflow &lt;- aggregate(fitted_flow_weekday_morn$ORCEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$DESTIN_hex), FUN=sum)\ncolnames(orc_fitted_inflow) &lt;- c(\"index\", \"Estimated_InFlow\")\norc_fitted_inflow_hex &lt;- left_join(hex_grid_pa_sz, orc_fitted_inflow, by = 'index')\norc_fitted_inflow_hex$Estimated_InFlow &lt;- ifelse(is.na(orc_fitted_inflow_hex$Estimated_InFlow), 0, orc_fitted_inflow_hex$Estimated_InFlow)\n\norc_fitted_outflow &lt;- aggregate(fitted_flow_weekday_morn$ORCEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$ORIGIN_hex), FUN=sum)\ncolnames(orc_fitted_outflow) &lt;- c(\"index\", \"Estimated_OutFlow\")\norc_fitted_outflow_hex &lt;- left_join(hex_grid_pa_sz, orc_fitted_outflow, by = 'index')\norc_fitted_outflow_hex$Estimated_OutFlow &lt;- ifelse(is.na(orc_fitted_outflow_hex$Estimated_OutFlow), 0, orc_fitted_outflow_hex$Estimated_OutFlow)\n\nFinally, we will use the appropriate tmap functions to create an interactive map with four layers. Each layer serves a unique purpose and adds a different dimension to our visualization.\n\nhex_grid_pa_sz: This layer serves as the base map for the analytical hexagons used in our modeling. It forms the foundation upon which we’ll overlay our desire lines and other data.\norc_fitted_inflow_hex: This layer represents the estimated volume of inflow for each hexagonal unit, as determined by our Origin Constrained Spatial Interaction Model (SIM). It will give us a visual representation of where people are coming from.\norc_fitted_outflow_hex: Similarly, this layer represents the estimated volume of outflow for each hexagonal unit, also based on our Origin Constrained SIM. This will help us understand where people are going.\norc_flowLine: This layer represents the estimated desire lines resulting from our Origin Constrained SIM. These lines will be represented in differing colors and line widths based on trip volume, providing a visual representation of the flow between different hexagons.\n\n\ntmap_mode(\"view\")\ntm_shape(hex_grid_pa_sz) +\n  tm_fill(col=\"#f2ffff\",\n          id = \"PLN_AREA_N\") +\n  tm_borders(col = \"grey\") +\ntm_shape(orc_fitted_inflow_hex) +\n  tm_fill(col = \"Estimated_InFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Inflow Volume\",\n          id = \"Estimated_InFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(orc_fitted_outflow_hex) +\n  tm_fill(col = \"Estimated_OutFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Outflow Volume\",\n          id = \"Estimated_OutFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(orc_flowLine) +\n  tm_lines(lwd = \"ORCEstimatedFlow\",\n           col = \"ORCEstimatedFlow\",\n           palette = c(\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"pretty\",\n          scale = c(1,2,3,4,5,7,9),\n          n = 6,\n          title.lwd = \"Orgin Constrained Flow\",\n          id = \"ORCEstimatedFlow\")\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nDuring my prototyping process, I’ve been considering the user experience quite a bit. Even though I’ve overlaid all four layers in the current stage, I’m contemplating a more interactive approach for our Shiny application by allowing them to choose the layers they want to see. They could select one layer at a time or any combination of layers, depending on what they’re interested in. This way, they can customize the map to their preferences and focus on the data that’s most relevant to them.\nThis approach would not only make the application more interactive but also more user-friendly. Users could explore different layers at their own pace, delve deeper into the ones they find interesting, and ultimately gain a more personalized understanding of the data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#destination-constraint-flow-estimation-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#destination-constraint-flow-estimation-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.11 Destination Constraint Flow Estimation Map",
    "text": "4.11 Destination Constraint Flow Estimation Map\nSimilar to what we did with origin constrained SIM model, we will follow the same procedure for plotting destination constrained SIM model results.\n\ndes_flowLine &lt;- flowLine %&gt;% filter(DESEstimatedFlow &gt; 5000)\n\n\ndes_fitted_inflow &lt;- aggregate(fitted_flow_weekday_morn$DESEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$DESTIN_hex), FUN=sum)\ncolnames(des_fitted_inflow) &lt;- c(\"index\", \"Estimated_InFlow\")\ndes_fitted_inflow_hex &lt;- left_join(hex_grid_pa_sz, des_fitted_inflow, by = 'index')\ndes_fitted_inflow_hex$Estimated_InFlow &lt;- ifelse(is.na(des_fitted_inflow_hex$Estimated_InFlow), 0, des_fitted_inflow_hex$Estimated_InFlow)\n\ndes_fitted_outflow &lt;- aggregate(fitted_flow_weekday_morn$DESEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$ORIGIN_hex), FUN=sum)\ncolnames(des_fitted_outflow) &lt;- c(\"index\", \"Estimated_OutFlow\")\ndes_fitted_outflow_hex &lt;- left_join(hex_grid_pa_sz, des_fitted_outflow, by = 'index')\ndes_fitted_outflow_hex$Estimated_OutFlow &lt;- ifelse(is.na(des_fitted_outflow_hex$Estimated_OutFlow), 0, des_fitted_outflow_hex$Estimated_OutFlow)\n\n\ntmap_mode(\"view\")\ntm_shape(hex_grid_pa_sz) +\n  tm_fill(col=\"#f2ffff\",\n          id = \"PLN_AREA_N\") +\n  tm_borders(col = \"grey\") +\ntm_shape(des_fitted_inflow_hex) +\n  tm_fill(col = \"Estimated_InFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Inflow Volume\",\n          id = \"Estimated_InFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(des_fitted_outflow_hex) +\n  tm_fill(col = \"Estimated_OutFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Outflow Volume\",\n          id = \"Estimated_OutFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(orc_flowLine) +\n  tm_lines(lwd = \"DESEstimatedFlow\",\n           col = \"DESEstimatedFlow\",\n           palette = c(\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"pretty\",\n          scale = c(1,2,3,4,5,7,9),\n          n = 6,\n          title.lwd = \"Destination Constrained Flow\",\n          id = \"DESEstimatedFlow\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-user-input-options-for-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-user-input-options-for-shiny-application",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.5 Exploring User Input Options for Shiny Application",
    "text": "4.5 Exploring User Input Options for Shiny Application\nFrom the exercise above, we may include the following user specification and model calibration options for our Shiny application.\n\n\n\n\n\n\n\nInput\nOptions\n\n\n\n\nFlow Type\nIncoming Flow, Outgoing Flow\n\n\nTime Period\nWeekday - Morning Peak , Weekday - Evening Peak, Weekend/Holiday - Morning Peak, Weekend/Holiday - Evening Peak\n\n\nZooming Level\nOverall (Singapore) , Planning Area, Subzone\n\n\nArea of Interest\n\nNA if Overall (Singapore) is selected for Zooming Level\nPlanning Area Name if Planning Area is selected for Zooming Level\nSubzone Name if Subzone is selected for Zooming Level\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nInitially, I had planned to use a slider for the time period, allowing users to specify the specific range of hours they wanted to examine. This seemed like the most viable approach. However, I soon realized that this could lead to heavy computation, especially if a user selected a very wide range (or possibly the entire dataset). This could potentially slow down our Shiny application, which would not be ideal for the user experience.\nAfter some thought, I decided to switch to pre-determined time intervals, specifically focusing on morning and evening peak hours. While it’s still important to consider non-peak hours, I’ve noticed that most policy discussions around transport modeling tend to emphasize peak hours. Therefore, I believe it’s more useful and logical to focus on these peak periods.\nFor the morning peak, I plan to use the time from 6-8 AM, and for the evening peak, I intend to use 5-7 PM. This way, we can analyze the inflow and outflow patterns during the most critical hours of the day, providing valuable insights for transportation planning and policy making.\n\n\nBelow is a rough outline of how the user interface for this tabset might look like."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparison",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparison",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "Comparison",
    "text": "Comparison\n\npacman::p_load(performance)\n\n\nmodel_list &lt;- list(originConstrained=orcSIM_weekday_morn,\n                   destinConstrained=desSIM_weekday_morn,\n                   doublyConstrained=dbcSIM_weekday_morn)\n\ncompare_performance(model_list, metrics = \"RMSE\")\n\n# Comparison of Model Performance Indices\n\nName              | Model |     RMSE\n------------------------------------\noriginConstrained |   glm | 1498.442\ndestinConstrained |   glm | 1468.283\ndoublyConstrained |   glm | 1137.973\n\n\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = ORCEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Origin Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")\n\n\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = DESEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Destination Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")\n\n\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = DBCEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Doubly Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#importing-datasets",
    "href": "Hands-on_Ex/hands_on10.html#importing-datasets",
    "title": "Hands-on Exercise 10",
    "section": "2.0 Importing Datasets",
    "text": "2.0 Importing Datasets\nFour data sets will be used in this hands-on exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\n\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\n\n\nAll the values of the cost related fields are in metres.\n\nmpsz &lt;- st_read(dsn = \"../data/geospatial\", layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nhexagons &lt;- st_read(dsn = \"../data/geospatial\", layer = \"hexagons\") \n\nReading layer `hexagons' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\neldercare &lt;- st_read(dsn = \"../data/geospatial\", layer = \"ELDERCARE\")\n\nReading layer `ELDERCARE' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThe data we've imported have undergone pre-processing using the QGIS software. This procedure can also be executed within the R environment by utilizing the appropriate functions and packages. However, it's important to note that this method can be computationally demanding and not advisable to use, especially when the study area and the raw datasets are huge."
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on10.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 10",
    "section": "3.1 Geospatial Data Wrangling",
    "text": "3.1 Geospatial Data Wrangling\n\n3.1.1 Updating CRS Information\nFirstly, we will update the newly imported sf objects with correct ESPF code (i.e., 3414)\n\nmpsz &lt;- st_transform(mpsz, 3414)\neldercare &lt;- st_transform(eldercare, 3414)\nhexagons &lt;- st_transform(hexagons, 3414)\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed sf objects by using st_crs() function of sf package.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(eldercare)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(hexagons)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n3.1.2 Cleaning and Updating Attribute Fields\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\n\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%\n  mutate(demand = 100)\n\n\n\n\n\n\n\nReflection\n\n\n\nFor the purpose of this hands-on exercise, a hypothetical constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used to achieve realistic and accurate results. This remains a significant challenge in real-world urban planning due to the limited on-ground data and infrequent survey works."
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#aspatial-data-wrangling",
    "href": "Hands-on_Ex/hands_on10.html#aspatial-data-wrangling",
    "title": "Hands-on Exercise 10",
    "section": "3.2 Aspatial Data Wrangling",
    "text": "3.2 Aspatial Data Wrangling\n\n3.2.1 Importing Distance Matrix\nFirstly, we will use read_cvs() of readr package to improt OD_Matrix.csv into R environemnt. The imported object is a tibble data.frame called ODMatrix.\n\nODMatrix &lt;- read_csv(\"../data/aspatial/OD_Matrix.csv\", skip = 0)\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.2.2 Tidying Distance Matrix\n\nhead(ODMatrix)\n\n# A tibble: 6 × 6\n  origin_id destination_id entry_cost network_cost exit_cost total_cost\n      &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1         1              1       668.       19847.      47.6     20562.\n2         1              2       668.       45027.      31.9     45727.\n3         1              3       668.       17644.     173.      18486.\n4         1              4       668.       36010.      92.2     36770.\n5         1              5       668.       31068.      64.6     31801.\n6         1              6       668.       31195.     117.      31980.\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the context of network analytics, particularly in Origin-Destination (OD) matrix computations, the terms entry_cost, network_cost, and exit_cost are used to describe different components of the total cost associated with a network route.\n\nEntry Cost: This is the cost associated with entering the network from a specific point or node. It could be thought of as the initial cost to start the journey on the network from a given origin.\nNetwork Cost: This is the cost incurred while traveling within the network. It represents the cost of moving from one node to another within the network, considering factors such as distance, time, or other relevant metrics.\nExit Cost: This is the cost associated with leaving the network to reach a specific destination point or node. It could be thought of as the final cost to end the journey on the network at a given destination.\n\nThe total cost of a network route is typically the sum of the Entry Cost, Network Cost, and Exit Cost. These costs are crucial in network analysis as they help in determining the most efficient or optimal paths between various points in a network.\n\n\nLooking at the data structure of ODMatrix as above, it organised the distance matrix columnwise. Such a structure is often termed as thin format. On the other hands, most of the modelling packages in R (including SpatialAcc that we are exploring in this exercise) expects a matrix look similar to the figure below. The rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\n\n\n\n\n\nTo achieve a dataset similar to the example above, we will use pivot_wider() function of tidyr package to transform the ODMatrix object from a thin format to a wide format using total_cost.\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%\n  pivot_wider(names_from = destination_id, values_from = total_cost)%&gt;%\n  select(c(-c('origin_id')))\n\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\ndistmat_km &lt;- as.matrix(distmat/1000)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#computing-hansens-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#computing-hansens-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "4.1 Computing Hansen’s Accessibility",
    "text": "4.1 Computing Hansen’s Accessibility\nIn this section, we will compute Hansen’s accessibility by using ac() function from SpatialAcc package.\n\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\n\n\n\n\n\nReflection\n\n\n\npower = 1 argument refers to the choice of distance decay function. This is often expressed through a mathematical function where the accessibility decreases with increasing distance.\nIn the context of spatial accessibility to services, for example, a larger distance-decay parameter indicates a stronger distance-decay effect on accessibility. This means that facilities located further away are less likely to be used than those closer.\n\n\n\n\n\n\n\n\nhead(acc_Hansen)\n\n  ac.hexagons.demand..eldercare.capacity..distmat_km..d0...50..\n1                                                  1.648313e-14\n2                                                  1.096143e-16\n3                                                  3.865857e-17\n4                                                  1.482856e-17\n5                                                  1.051348e-17\n6                                                  5.076391e-18\n\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\nacc_Hansen &lt;- tibble::as_tibble(acc_Hansen)\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\nhead(hexagon_Hansen)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 22756.33 xmax: 3244.888 ymax: 27756.33\nProjected CRS: SVY21 / Singapore TM\n  fid demand    accHansen                       geometry\n1   1    100 1.648313e-14 POLYGON ((2667.538 27506.33...\n2   2    100 1.096143e-16 POLYGON ((2667.538 25006.33...\n3   3    100 3.865857e-17 POLYGON ((2667.538 24506.33...\n4   4    100 1.482856e-17 POLYGON ((2667.538 24006.33...\n5   5    100 1.051348e-17 POLYGON ((2667.538 23506.33...\n6   6    100 5.076391e-18 POLYGON ((2667.538 23006.33...\n\n\nWe notice that hexagon_Hansen is a simple feature data frame and not a typical tibble data frame."
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#visualising-hansens-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#visualising-hansens-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "4.2 Visualising Hansen’s Accessibility",
    "text": "4.2 Visualising Hansen’s Accessibility\n\n4.2.1 Extracting Map Extent\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\nmapex &lt;- st_bbox(hexagons)\n\nNext, we will use relevant tmap functions to create a high cartographic quality accessibility to eldrecare centre in Singapore.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          palette = \"viridis\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualization",
    "href": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualization",
    "title": "Hands-on Exercise 10",
    "section": "4.3 Statistical Graphic Visualization",
    "text": "4.3 Statistical Graphic Visualization\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_Hansen sf data frame by using the code chunk below.\n\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz, \n                          join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2) +\n  ylab(\"Hansen's Accessibility Score\")+\n  xlab(\"Region\")\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#computing-kd2sfcas-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#computing-kd2sfcas-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "5.1 Computing KD2SFCA’s Accessibility",
    "text": "5.1 Computing KD2SFCA’s Accessibility\nIn this section, we are going to calculate geographical accessibility using the kernal density two-step floating catchment area (KD2SFCA) method. To implement this, we will use ac() of SpatialAcc with specifying the family argument to KD2SFCA. The output is saved in a data frame called acc_KD2SFCA.\n\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"\nacc_KD2SFCA &lt;- tibble::as_tibble(acc_KD2SFCA)\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#visualising-kd2sfcas-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#visualising-kd2sfcas-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "5.2 Visualising KD2SFCA’s Accessibility",
    "text": "5.2 Visualising KD2SFCA’s Accessibility\nNext, we will use relevant tmap functions to create a high cartographic quality accessibility to eldrecare centre in Singapore.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          palette = \"viridis\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualisation",
    "href": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualisation",
    "title": "Hands-on Exercise 10",
    "section": "5.3 Statistical Graphic Visualisation",
    "text": "5.3 Statistical Graphic Visualisation\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region. Firstly, we need to add the planning region field into hexagon_KD2SFCA sf data frame by using the code chunk below.\n\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)   +\n  ylab(\"KD2CFA Accessibility Score\")+\n  xlab(\"Region\")\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#computing-sam-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#computing-sam-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "6.1 Computing SAM Accessibility",
    "text": "6.1 Computing SAM Accessibility\nIn this section, we are going to calculate geographical accessibility using SAM method. To implement this, we will use ac() of SpatialAcc with specifying the family argument to SAM. The output is saved in a data frame called acc_SAM.\n\nacc_SAM &lt;- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km, \n                         d0 = 50,\n                         power = 2, \n                         family = \"SAM\"))\n\ncolnames(acc_SAM) &lt;- \"accSAM\"\nacc_SAM &lt;- tibble::as_tibble(acc_SAM)\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#visualising-sams-accessibility",
    "href": "Hands-on_Ex/hands_on10.html#visualising-sams-accessibility",
    "title": "Hands-on Exercise 10",
    "section": "6.2 Visualising SAM’s Accessibility",
    "text": "6.2 Visualising SAM’s Accessibility\nNext, we will use relevant tmap functions to create a high cartographic quality accessibility to eldrecare centre in Singapore.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_SAM,\n         bbox = mapex) + \n  tm_fill(col = \"accSAM\",\n          palette = \"viridis\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualisation-1",
    "href": "Hands-on_Ex/hands_on10.html#statistical-graphic-visualisation-1",
    "title": "Hands-on Exercise 10",
    "section": "6.3 Statistical Graphic Visualisation",
    "text": "6.3 Statistical Graphic Visualisation\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region. Firstly, we need to add the planning region field into hexagon_SAM sf data frame by using the code chunk below.\n\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz, \n                       join = st_intersects)\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2) +\n  ylab(\"SAM Accessibility Score\")+\n  xlab(\"Region\")\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "data/geospatial/ELDERCARE.html",
    "href": "data/geospatial/ELDERCARE.html",
    "title": "IS415 - Geospatial with Khant",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "data/geospatial/hexagons.html",
    "href": "data/geospatial/hexagons.html",
    "title": "IS415 - Geospatial with Khant",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-into-planning-areas-and-subzones",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-into-planning-areas-and-subzones",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.4 Zooming into Planning Areas and Subzones",
    "text": "4.4 Zooming into Planning Areas and Subzones\nIn this section, we will further zoom our inflow/outflow map into planning areas and subzones.\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thougths\nIn the previous section, my focus was on exploring the inflow and outflow maps for the entirety of Singapore Island. However, I realized that this broad perspective might not be intuitive enough to identify patterns at the local subzone levels.\nEspecially when we’re using analytical hexagons to represent the spatial units, it can pose a real challenge for users to understand the local details. For instance, figuring out which planning area or subzone a hexagon belongs to, or simply pinpointing a planning area or subzone’s location in Singapore, can be quite tricky.\nConsidering this, I thought of a potential enhancement to the user experience. What if I could allow users to specify the subzone they’re interested in? This way, they could delve into the details of that particular area, examining the inflow and outflow patterns more closely. Not only would this make the data more relevant and personalized for the user, but it could also reveal unique patterns and trends that might be overlooked in a broader analysis.\n\n\nBefore we go about testing, let’s have a quick look at how the structure of our inflow and outflow dataset looks like.\n\nhead(inflow_data_weekday_morn_hex)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2292.538 ymin: 21664.98 xmax: 3417.538 ymax: 28376.68\nProjected CRS: SVY21 / Singapore TM\n  PLN_AREA_N           SUBZONE_N index TOTAL_TRIPS\n1       TUAS TUAS VIEW EXTENSION     1           0\n2       TUAS TUAS VIEW EXTENSION     2           0\n3       TUAS TUAS VIEW EXTENSION     3           0\n4       TUAS TUAS VIEW EXTENSION     4           0\n5       TUAS TUAS VIEW EXTENSION     5           0\n6       TUAS TUAS VIEW EXTENSION     6           0\n                        geometry TRIPS_BIN\n1 POLYGON ((2667.538 22314.5,...         0\n2 POLYGON ((2667.538 23613.54...         0\n3 POLYGON ((2667.538 24912.58...         0\n4 POLYGON ((2667.538 26211.61...         0\n5 POLYGON ((2667.538 27510.65...         0\n6 POLYGON ((3042.538 21664.98...         0\n\n\nFrom the look of it, it appears that each hexagon has data on their respective planning area and subzone they belong to. This information will be useful when we want to create planning-area specific maps for inflow/outflow. We can simply filter our dataset to smaller, temporary data based on the user specification.\n\n4.4.1 Planning Area Level Zooming\nFirsly, let’s try creating inflow/outflow maps for planning area level. The user can specify a planning area, and we can filter the data accordingly before creating maps. We will create two functions, called inflow_pa_map and outflow_pa_map. Each function is designed to generate a specific type of map - inflow or outflow, respectively.\nWhen a planning area name is provided as input to these functions, they will filter the dataset to only include data relevant to the specified area. Leveraging the tmap package, these functions will then create an interactive map. It’s important to note that these maps will maintain the same color palette and breaks as our previous overall maps, ensuring consistency in our visual representations.\n\ninflow_pa_map &lt;- function(pa_input) { \n  inflow_pa_temp &lt;- inflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input)\n  tmap_mode(\"view\")\n  tm_shape(inflow_pa_temp) +\n    tm_fill(col = \"TOTAL_TRIPS\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Inflow Trip Count\",\n          id= \"TOTAL_TRIPS\")+\n  tm_borders(col = \"grey\")\n}\n\n\noutflow_pa_map &lt;- function(pa_input) { \n  outflow_pa_temp &lt;- outflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input)\n  tmap_mode(\"view\")\n  tm_shape(outflow_pa_temp) +\n    tm_fill(col = \"TOTAL_TRIPS\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Outflow Trip Count\",\n          id= \"TOTAL_TRIPS\")+\n  tm_borders(col = \"grey\")\n}\n\nAs a demonstration, we call the inflow_pa_map() function with “HOUGANG” and the outflow_pa_map() function with “BUKIT PANJANG” as inputs. to test whether the functions works well and produce desired outcomes.\n\ninflow_pa_map(\"HOUGANG\")\n\n\n\n\n\noutflow_pa_map(\"BUKIT PANJANG\")\n\n\n\n\n\n\nIn the similar vein, we will also need to update our distribution graphs to specific planning area. To achieve that, we will create four functions below - inflow_pa_graph, outflow_pa_graph , inflow_pa_nozerograph and outflow_pa_nozerograph .\n\ninflow_pa_graph &lt;- function(pa_input) { \n    inflow_pa_temp &lt;- inflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input)\n  ggplot(data = inflow_pa_temp,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\")\n}\n\noutflow_pa_graph &lt;- function(pa_input) { \n    outflow_pa_temp &lt;- outflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input)\n    ggplot(data = outflow_pa_temp,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\")\n}\n\ninflow_pa_nozerograph &lt;- function(pa_input) { \n    inflow_pa_temp_nozero &lt;- inflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input, TOTAL_TRIPS != 0)\n  ggplot(data = inflow_pa_temp_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero inflow trip)\")\n}\n\noutflow_pa_nozerograph &lt;- function(pa_input) { \n    outflow_pa_temp_nozero &lt;- outflow_data_weekday_morn_hex %&gt;% filter(PLN_AREA_N == pa_input, TOTAL_TRIPS != 0)\n  ggplot(data = outflow_pa_temp_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero outflow trip)\")\n}\n\nAs a demonstration, we call the inflow_pa_graph() & inflow_pa_nozerograph() functions with “HOUGANG” and the outflow_pa_graph() & outflow_pa_nozerograph() functions with “BUKIT PANJANG” as inputs to test whether the functions works well and produce desired outcomes.\n\ninflow_pa_graph(\"HOUGANG\")\n\n\n\ninflow_pa_nozerograph(\"HOUGANG\")\n\n\n\noutflow_pa_graph(\"BUKIT PANJANG\")\n\n\n\noutflow_pa_nozerograph(\"BUKIT PANJANG\")\n\n\n\n\n\n\n4.4.2 Subzone Level Zooming\nSimilar to what we did with planning area level zooming, we can create functions for inflow/outflow maps at subzone level. Below, we have create two functions inflow_sz_map() and outflow_sz_map() using similar approach.\n\ninflow_sz_map &lt;- function(sz_input) { \n  inflow_sz_temp &lt;- inflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input)\n  tmap_mode(\"view\")\n  tm &lt;- tm_shape(inflow_sz_temp) +\n    tm_fill(col = \"TOTAL_TRIPS\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Inflow Trip Count\",\n          id= \"TOTAL_TRIPS\")+\n  tm_borders(col = \"grey\")\n return(tm)\n}\n\n\noutflow_sz_map &lt;- function(sz_input) { \n  outflow_sz_temp &lt;- outflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input)\n  tmap_mode(\"view\")\n  tm &lt;- tm_shape(outflow_sz_temp) +\n    tm_fill(col = \"TOTAL_TRIPS\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Outflow Trip Count\",\n          id= \"TOTAL_TRIPS\")+\n  tm_borders(col = \"grey\")\n return(tm)\n}\n\nAs a demonstration, we call the inflow_sz_map() function with “CHANGI AIRPORT” and the outflow_sz_map() function with “TUAS NORTH” as inputs to test whether the functions works well and produce desired outcomes.\n\ninflow_sz_map(\"CHANGI AIRPORT\")\n\n\n\n\n\noutflow_sz_map(\"TUAS NORTH\")\n\n\n\n\n\n\nIn the similar vein, we will also need to update our distribution graphs to specific subzone. To achieve that, we will create four functions below - inflow_sz_graph, outflow_sz_graph , inflow_sz_nozerograph and outflow_sz_nozerograph .\n\ninflow_sz_graph &lt;- function(sz_input) { \n    inflow_sz_temp &lt;- inflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input)\n  ggplot(data = inflow_sz_temp,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\")\n}\n\noutflow_sz_graph &lt;- function(sz_input) { \n    outflow_sz_temp &lt;- outflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input)\n    ggplot(data = outflow_sz_temp,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map) +\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\")\n}\n\ninflow_sz_nozerograph &lt;- function(sz_input) { \n    inflow_sz_temp_nozero &lt;- inflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input, TOTAL_TRIPS != 0)\n  ggplot(data = inflow_sz_temp_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Inflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Inflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero inflow trip)\")\n}\n\noutflow_sz_nozerograph &lt;- function(sz_input) { \n    outflow_sz_temp_nozero &lt;- outflow_data_weekday_morn_hex %&gt;% filter(SUBZONE_N == sz_input, TOTAL_TRIPS != 0)\n  ggplot(data = outflow_sz_temp_nozero,\n       aes(y = TRIPS_BIN,fill = TRIPS_BIN)) +\n  geom_bar(show.legend = FALSE)+\n  xlab(\"Count of Analytical Hexagons\") +\n  ylab(\"Outflow Trip Count\") +  \n  scale_fill_manual(values=color_map)+\n  ggtitle(\"Distribution of Hexagons Per Each Outflow Trip Count Bin\", \n          subtitle = \"(excluding hexagons with zero outflow trip)\")\n}\n\nAs a demonstration, we call the inflow_sz_graph() & inflow_sz_nozerograph() functions with “CHANGI AIRPORT” and the outflow_sz_graph() & outflow_sz_nozerograph() functions with “TUAS NORTH” as inputs to test whether the functions works well and produce desired outcomes.\n\ninflow_sz_graph(\"CHANGI AIRPORT\")\n\n\n\ninflow_sz_nozerograph(\"CHANGI AIRPORT\")\n\n\n\noutflow_sz_graph(\"TUAS NORTH\")\n\n\n\noutflow_sz_nozerograph(\"TUAS NORTH\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#spatial-interaction-modelling",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#spatial-interaction-modelling",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.6 Spatial Interaction Modelling",
    "text": "4.6 Spatial Interaction Modelling\nIn this section, we will test how we can implement and visualise spatial interaction modelling of origin-destination public bus flow data. In particular, we are planning to test using glm() functions from stats package to fit generalised linear models.\n\nApplying log() Transformation to Explanatory Variables\nPoisson regression is often used in spatial interaction modelling of origin-destination (OD) bus flow due to the nature of the data and the statistical properties of the Poisson distribution. Since Poisson Regression is based on log, log transformation of explanatory variables need to be done before running the model.\nHence, we apply log transformation to all columns in the flow dataset which ends with _count. These columns represent the explanatory variables of origin and destination hexagons in our dataset. We intuitively know that these variables need to be transformed because they are count data, which are often skewed and can benefit from a log transformation to meet the assumptions of Poisson regression. We also apply a log transformation to the dist column, which represents the distance between hexagons. This is done because distance is a continuous variable that can also be skewed and can benefit from a log transformation.\n\nflow_data_weekday_morn_log &lt;- flow_data_weekday_morn %&gt;%\n  mutate_at(vars(ends_with(\"_count\")), log) %&gt;%\n  mutate(dist = log(dist))\n\nThe transformed data is stored in a new data frame flow_data_weekday_morn_log. This data frame is now ready for Poisson regression analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#destination-constraint-spatial-interaction-model",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#destination-constraint-spatial-interaction-model",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.8 Destination Constraint Spatial Interaction Model",
    "text": "4.8 Destination Constraint Spatial Interaction Model\nNext, we will fit a destination constrained Spatial Interaction Model (SIM). For destination constrained SIM, only explanatory variables which represent how propulsive the origins are will be used. This is because such models emphasize the demand or attractiveness of the destinations rather than the limitations or capacities of the origins. The demand or attractiveness of the destination sites determines the potential for generating interactions or flows.\n\ndesSIM_weekday_morn &lt;- glm(TOTAL_TRIPS ~ DESTIN_hex + o_biz_count + o_school_count + o_fin_count + o_hc_count + o_busstop_count + o_housing_count + o_leisure_recre_count + o_retail_count + o_entertn_count + o_food_bev_count + dist - 1,\n              family = poisson(link = \"log\"),\n              data = flow_data_weekday_morn_log,\n              na.action = na.exclude)\n\n\ndesSIM_weekday_morn\n\n\nCall:  glm(formula = TOTAL_TRIPS ~ DESTIN_hex + o_biz_count + o_school_count + \n    o_fin_count + o_hc_count + o_busstop_count + o_housing_count + \n    o_leisure_recre_count + o_retail_count + o_entertn_count + \n    o_food_bev_count + dist - 1, family = poisson(link = \"log\"), \n    data = flow_data_weekday_morn_log, na.action = na.exclude)\n\nCoefficients:\n         DESTIN_hex29           DESTIN_hex31           DESTIN_hex39  \n             16.29641               16.92718               18.31263  \n         DESTIN_hex40           DESTIN_hex41           DESTIN_hex49  \n             18.14769               15.95552               14.98285  \n         DESTIN_hex50           DESTIN_hex51           DESTIN_hex52  \n             18.01659               17.28346               17.60414  \n         DESTIN_hex59           DESTIN_hex60           DESTIN_hex61  \n             16.38878               15.07300               16.40978  \n         DESTIN_hex62           DESTIN_hex63           DESTIN_hex72  \n             16.90342               15.57061               15.45554  \n         DESTIN_hex73           DESTIN_hex74           DESTIN_hex75  \n             17.76024               16.69284               16.14773  \n         DESTIN_hex83           DESTIN_hex84           DESTIN_hex85  \n             16.66879               16.57992               15.71083  \n         DESTIN_hex86           DESTIN_hex87           DESTIN_hex88  \n             16.36325               17.48318               17.30290  \n         DESTIN_hex89           DESTIN_hex96           DESTIN_hex97  \n             17.40281               14.56358               16.02026  \n         DESTIN_hex99          DESTIN_hex100          DESTIN_hex101  \n             17.63771               16.19638               16.21299  \n        DESTIN_hex112          DESTIN_hex113          DESTIN_hex114  \n             17.47549               15.96331               13.80291  \n        DESTIN_hex124          DESTIN_hex125          DESTIN_hex126  \n             14.95177               17.16265               15.54569  \n        DESTIN_hex135          DESTIN_hex136          DESTIN_hex137  \n             18.06108               16.07555               16.42547  \n        DESTIN_hex145          DESTIN_hex146          DESTIN_hex147  \n             15.32486               16.20131               17.24990  \n        DESTIN_hex155          DESTIN_hex156          DESTIN_hex157  \n             13.96431               15.99718               15.74272  \n        DESTIN_hex168          DESTIN_hex169          DESTIN_hex170  \n             17.10045               16.67306               16.43496  \n        DESTIN_hex181          DESTIN_hex182          DESTIN_hex183  \n             17.03828               16.21434               14.57910  \n        DESTIN_hex197          DESTIN_hex198          DESTIN_hex199  \n             16.87873               17.25488               18.09667  \n        DESTIN_hex213          DESTIN_hex214          DESTIN_hex215  \n             16.41227               17.18189               17.86791  \n        DESTIN_hex231          DESTIN_hex232          DESTIN_hex233  \n             14.49765               15.87206               18.75080  \n        DESTIN_hex249          DESTIN_hex250          DESTIN_hex252  \n             16.48458               17.25271               17.29257  \n        DESTIN_hex265          DESTIN_hex266          DESTIN_hex267  \n             17.25747               17.62888               16.95999  \n        DESTIN_hex268          DESTIN_hex269          DESTIN_hex283  \n             16.87531               17.92285               15.32062  \n        DESTIN_hex284          DESTIN_hex286          DESTIN_hex287  \n             17.50197               11.85000               16.19406  \n        DESTIN_hex288          DESTIN_hex300          DESTIN_hex301  \n             10.42242               16.44809               17.87703  \n        DESTIN_hex302          DESTIN_hex303          DESTIN_hex304  \n             16.92462               16.59241               16.03264  \n        DESTIN_hex319          DESTIN_hex320          DESTIN_hex321  \n             16.48005               16.06172               15.00611  \n        DESTIN_hex322          DESTIN_hex323          DESTIN_hex334  \n             18.51881               10.94554               15.09693  \n        DESTIN_hex335          DESTIN_hex336          DESTIN_hex337  \n             17.94818               16.17114               16.67617  \n        DESTIN_hex339          DESTIN_hex340          DESTIN_hex351  \n             15.50465               16.00509               17.40285  \n        DESTIN_hex352          DESTIN_hex353          DESTIN_hex354  \n             16.60806               16.56099               16.89379  \n        DESTIN_hex355          DESTIN_hex356          DESTIN_hex357  \n             15.07799               13.60477               13.40582  \n        DESTIN_hex367          DESTIN_hex368          DESTIN_hex369  \n             16.40754               16.14068               15.19648  \n        DESTIN_hex370          DESTIN_hex371          DESTIN_hex373  \n             17.44499               16.47690               14.33728  \n        DESTIN_hex374          DESTIN_hex375          DESTIN_hex377  \n             15.20796               15.70589               13.76290  \n        DESTIN_hex385          DESTIN_hex386          DESTIN_hex387  \n             17.91030               17.87884               17.99494  \n        DESTIN_hex388          DESTIN_hex390          DESTIN_hex393  \n             16.01277               14.83431               14.01670  \n        DESTIN_hex394          DESTIN_hex395          DESTIN_hex402  \n             15.85652               15.78621               17.02728  \n        DESTIN_hex403          DESTIN_hex404          DESTIN_hex405  \n             16.44295               18.16550               16.12394  \n        DESTIN_hex407          DESTIN_hex411          DESTIN_hex412  \n             13.39033               13.65494               14.69220  \n        DESTIN_hex413          DESTIN_hex419          DESTIN_hex420  \n             15.10991               15.34646               16.68797  \n        DESTIN_hex421          DESTIN_hex422          DESTIN_hex424  \n             16.93458               16.40840               11.62166  \n        DESTIN_hex430          DESTIN_hex437          DESTIN_hex438  \n             12.80808               15.14744               16.04508  \n        DESTIN_hex439          DESTIN_hex440          DESTIN_hex442  \n             15.72230               15.31732               16.12263  \n        DESTIN_hex453          DESTIN_hex454          DESTIN_hex455  \n             16.49762               14.00112               15.80098  \n        DESTIN_hex456          DESTIN_hex471          DESTIN_hex472  \n             17.93885               16.54571               16.84128  \n        DESTIN_hex473          DESTIN_hex474          DESTIN_hex476  \n             15.94792               17.06285               16.38364  \n        DESTIN_hex487          DESTIN_hex488          DESTIN_hex489  \n             17.25917               15.00148               16.83524  \n        DESTIN_hex490          DESTIN_hex504          DESTIN_hex505  \n             14.59170               15.39796               14.00833  \n        DESTIN_hex506          DESTIN_hex508          DESTIN_hex518  \n             16.56109               15.91734               16.01372  \n        DESTIN_hex521          DESTIN_hex522          DESTIN_hex524  \n             16.39886               14.07722               13.88928  \n        DESTIN_hex533          DESTIN_hex534          DESTIN_hex536  \n             17.31610               17.02287               16.33386  \n        DESTIN_hex537          DESTIN_hex539          DESTIN_hex549  \n             15.90306               14.82157               15.90941  \n        DESTIN_hex550          DESTIN_hex551          DESTIN_hex552  \n             16.49194               16.58595               16.39809  \n        DESTIN_hex554          DESTIN_hex555          DESTIN_hex559  \n             16.11706               13.44166               15.74426  \n        DESTIN_hex562          DESTIN_hex564          DESTIN_hex565  \n             16.17834               15.16197               15.52028  \n        DESTIN_hex566          DESTIN_hex567          DESTIN_hex568  \n             16.15829               14.87059               15.87050  \n        DESTIN_hex569          DESTIN_hex577          DESTIN_hex579  \n             14.96313               15.92645               16.11964  \n        DESTIN_hex580          DESTIN_hex581          DESTIN_hex582  \n             17.73335               16.16215               15.53380  \n        DESTIN_hex583          DESTIN_hex584          DESTIN_hex585  \n             13.56601               16.23676               16.16663  \n        DESTIN_hex586          DESTIN_hex588          DESTIN_hex589  \n             15.08426               16.28597               17.43083  \n        DESTIN_hex593          DESTIN_hex594          DESTIN_hex595  \n             14.99029               15.73119               17.00372  \n        DESTIN_hex596          DESTIN_hex597          DESTIN_hex598  \n             15.66153               16.63142               15.78979  \n        DESTIN_hex599          DESTIN_hex600          DESTIN_hex601  \n             16.13289               17.85474               17.71287  \n        DESTIN_hex603          DESTIN_hex604          DESTIN_hex609  \n             15.49592               15.92531               16.30754  \n        DESTIN_hex610          DESTIN_hex611          DESTIN_hex612  \n             17.15631               15.31163               16.72062  \n        DESTIN_hex613          DESTIN_hex614          DESTIN_hex615  \n             15.76358               17.42514               15.89984  \n        DESTIN_hex616          DESTIN_hex617          DESTIN_hex618  \n             16.60105               14.91042               15.66958  \n        DESTIN_hex619          DESTIN_hex620          DESTIN_hex625  \n             17.18331               16.35699               15.90081  \n        DESTIN_hex626          DESTIN_hex627          DESTIN_hex628  \n             15.89966               17.38338               17.72436  \n        DESTIN_hex629          DESTIN_hex630          DESTIN_hex631  \n             17.10516               14.46864               16.22649  \n        DESTIN_hex632          DESTIN_hex633          DESTIN_hex634  \n             14.10692               14.61651               17.40261  \n        DESTIN_hex635          DESTIN_hex636          DESTIN_hex643  \n             13.69972               16.03074               16.88169  \n        DESTIN_hex644          DESTIN_hex645          DESTIN_hex646  \n             14.97006               17.05999               16.66125  \n        DESTIN_hex649          DESTIN_hex650          DESTIN_hex651  \n             15.95821               14.51967               17.48656  \n        DESTIN_hex652          DESTIN_hex653          DESTIN_hex654  \n             14.21041               16.20476               15.64389  \n        DESTIN_hex659          DESTIN_hex660          DESTIN_hex661  \n             16.55934               16.15706               16.61647  \n        DESTIN_hex662          DESTIN_hex663          DESTIN_hex665  \n             17.94784               15.82993               14.13371  \n        DESTIN_hex666          DESTIN_hex668          DESTIN_hex669  \n             15.87139               14.15045               16.09034  \n        DESTIN_hex670          DESTIN_hex676          DESTIN_hex677  \n             13.70738               16.13062               15.30605  \n        DESTIN_hex678          DESTIN_hex680          DESTIN_hex681  \n             16.00508               14.81002               16.29755  \n        DESTIN_hex682          DESTIN_hex683          DESTIN_hex687  \n             16.23482               17.97632               19.11726  \n        DESTIN_hex688          DESTIN_hex693          DESTIN_hex694  \n             16.76484               15.92313               16.78572  \n        DESTIN_hex695          DESTIN_hex696          DESTIN_hex697  \n             18.14810               16.15677               15.96672  \n        DESTIN_hex698          DESTIN_hex699          DESTIN_hex700  \n             14.75713               17.34129               14.74735  \n        DESTIN_hex701          DESTIN_hex703          DESTIN_hex706  \n             15.91709               16.00450               18.41535  \n        DESTIN_hex711          DESTIN_hex712          DESTIN_hex713  \n             17.17691               16.33640               15.74858  \n        DESTIN_hex715          DESTIN_hex716          DESTIN_hex718  \n             16.28766               14.51063               15.74221  \n        DESTIN_hex722          DESTIN_hex723          DESTIN_hex729  \n             15.24229               15.83301               16.39729  \n        DESTIN_hex730          DESTIN_hex731          DESTIN_hex732  \n             16.37457               14.95630               18.38733  \n        DESTIN_hex733          DESTIN_hex735          DESTIN_hex736  \n             17.72282               16.12168               16.94901  \n        DESTIN_hex737          DESTIN_hex739          DESTIN_hex741  \n             15.19692               15.82299               17.55703  \n        DESTIN_hex742          DESTIN_hex746          DESTIN_hex747  \n             16.64277               17.37722               16.95057  \n        DESTIN_hex748          DESTIN_hex749          DESTIN_hex750  \n             16.79783               15.05996               17.90711  \n        DESTIN_hex751          DESTIN_hex753          DESTIN_hex754  \n             15.94297               15.35211               12.79231  \n        DESTIN_hex758          DESTIN_hex759          DESTIN_hex760  \n             15.50789               16.26208               12.99378  \n        DESTIN_hex764          DESTIN_hex766          DESTIN_hex767  \n             17.26778               16.61536               14.72141  \n        DESTIN_hex769          DESTIN_hex775          DESTIN_hex777  \n             16.42438               16.88471               15.95020  \n        DESTIN_hex778          DESTIN_hex782          DESTIN_hex783  \n             17.40000               16.97862               16.86281  \n        DESTIN_hex784          DESTIN_hex785          DESTIN_hex786  \n             15.71715               14.03997               15.42869  \n        DESTIN_hex794          DESTIN_hex795          DESTIN_hex796  \n             13.12554               18.66140               17.00230  \n        DESTIN_hex799          DESTIN_hex800          DESTIN_hex801  \n             15.34349               16.95155               14.97225  \n        DESTIN_hex802          DESTIN_hex810          DESTIN_hex812  \n             15.18772               13.59443               16.65419  \n        DESTIN_hex813          DESTIN_hex816          DESTIN_hex817  \n             16.41613               16.11271               16.34578  \n        DESTIN_hex818          DESTIN_hex819          DESTIN_hex820  \n             16.53949               17.25407               14.09708  \n        DESTIN_hex821          DESTIN_hex827          DESTIN_hex829  \n             16.02286               17.17713               16.66256  \n        DESTIN_hex830          DESTIN_hex831          DESTIN_hex835  \n             15.75598               18.25914               16.54881  \n        DESTIN_hex836          DESTIN_hex837          DESTIN_hex838  \n             14.86879               14.96897               14.93903  \n        DESTIN_hex847          DESTIN_hex848          DESTIN_hex849  \n             15.70374               16.02890               17.64578  \n        DESTIN_hex851          DESTIN_hex853          DESTIN_hex854  \n             17.53841               15.98112               15.48962  \n        DESTIN_hex856          DESTIN_hex863          DESTIN_hex864  \n             16.29192               15.61683               14.38210  \n        DESTIN_hex865          DESTIN_hex866          DESTIN_hex867  \n             17.77067               17.27811               16.33260  \n        DESTIN_hex868          DESTIN_hex869          DESTIN_hex870  \n             15.44363               15.76305               15.59238  \n        DESTIN_hex871          DESTIN_hex873          DESTIN_hex882  \n             17.10580               15.95062               16.75090  \n        DESTIN_hex883          DESTIN_hex884          DESTIN_hex885  \n             16.05142               17.45023               15.42082  \n        DESTIN_hex887          DESTIN_hex888          DESTIN_hex889  \n             17.62944               16.72057               15.38100  \n        DESTIN_hex890          DESTIN_hex899          DESTIN_hex901  \n             15.06131               14.75402               16.47535  \n        DESTIN_hex902          DESTIN_hex903          DESTIN_hex905  \n             16.82661               17.14287               16.09321  \n        DESTIN_hex906          DESTIN_hex907          DESTIN_hex909  \n             15.80358               16.21766               16.20054  \n        DESTIN_hex910          DESTIN_hex917          DESTIN_hex920  \n             15.45505               11.58802               16.20011  \n        DESTIN_hex922          DESTIN_hex925          DESTIN_hex926  \n             17.56027               15.46908               17.02325  \n        DESTIN_hex927          DESTIN_hex928          DESTIN_hex929  \n             15.20056               15.08114               15.70970  \n        DESTIN_hex936          DESTIN_hex937          DESTIN_hex940  \n             14.69043               15.51881               17.89282  \n        DESTIN_hex941          DESTIN_hex944          DESTIN_hex945  \n             17.46466               15.67584               17.17368  \n        DESTIN_hex946          DESTIN_hex947          DESTIN_hex949  \n             16.44105               14.78962               15.64079  \n        DESTIN_hex955          DESTIN_hex959          DESTIN_hex960  \n             12.88820               16.51514               15.74441  \n        DESTIN_hex962          DESTIN_hex963          DESTIN_hex964  \n             15.11301               16.20886               16.14377  \n        DESTIN_hex965          DESTIN_hex966          DESTIN_hex968  \n             17.05712               15.48857               16.59771  \n        DESTIN_hex969          DESTIN_hex974          DESTIN_hex975  \n             14.43042               11.88350               16.31805  \n        DESTIN_hex978          DESTIN_hex979          DESTIN_hex980  \n             15.55900               17.72151               15.94269  \n        DESTIN_hex982          DESTIN_hex983          DESTIN_hex984  \n             14.36549               17.27748               17.50656  \n        DESTIN_hex985          DESTIN_hex986          DESTIN_hex989  \n             15.19641               16.34563               13.05545  \n        DESTIN_hex993          DESTIN_hex994          DESTIN_hex995  \n             16.02937               16.57536               14.32543  \n        DESTIN_hex996          DESTIN_hex998          DESTIN_hex999  \n             14.90239               15.75031               15.30624  \n       DESTIN_hex1002         DESTIN_hex1004         DESTIN_hex1005  \n             16.32575               15.46392               17.11310  \n       DESTIN_hex1006         DESTIN_hex1008         DESTIN_hex1013  \n             14.30454               17.48354               17.58215  \n       DESTIN_hex1015         DESTIN_hex1016         DESTIN_hex1017  \n             15.70443               14.60192               15.19016  \n       DESTIN_hex1018         DESTIN_hex1019         DESTIN_hex1023  \n             15.70165               14.99829               15.52574  \n       DESTIN_hex1024         DESTIN_hex1025         DESTIN_hex1026  \n             15.55176               15.79976               16.02717  \n       DESTIN_hex1027         DESTIN_hex1028         DESTIN_hex1029  \n             15.99254               14.55928               11.79943  \n       DESTIN_hex1031         DESTIN_hex1032         DESTIN_hex1034  \n             15.23884               15.79969               13.38729  \n       DESTIN_hex1035         DESTIN_hex1036         DESTIN_hex1037  \n             13.65753               13.79952               17.32804  \n       DESTIN_hex1038         DESTIN_hex1039         DESTIN_hex1044  \n             17.26623               14.59677               15.30480  \n       DESTIN_hex1045         DESTIN_hex1046         DESTIN_hex1047  \n             15.68987               15.50631               14.66666  \n       DESTIN_hex1048         DESTIN_hex1051         DESTIN_hex1052  \n             15.58845               16.38789               14.56731  \n       DESTIN_hex1053         DESTIN_hex1056         DESTIN_hex1057  \n             14.94105               18.25204               15.46625  \n       DESTIN_hex1058         DESTIN_hex1059         DESTIN_hex1063  \n             15.59751               16.08979               11.94224  \n       DESTIN_hex1065         DESTIN_hex1067         DESTIN_hex1068  \n             13.97198               16.79921               16.11692  \n       DESTIN_hex1069         DESTIN_hex1070         DESTIN_hex1071  \n             15.86251               16.28662               16.57756  \n       DESTIN_hex1072         DESTIN_hex1073         DESTIN_hex1075  \n             16.45791               15.50890               13.31486  \n       DESTIN_hex1076         DESTIN_hex1077         DESTIN_hex1078  \n             16.47403               16.45026               15.09669  \n       DESTIN_hex1079         DESTIN_hex1083         DESTIN_hex1085  \n             14.27037               16.16364               16.24419  \n       DESTIN_hex1086         DESTIN_hex1088         DESTIN_hex1089  \n             16.86594               16.34920               14.77913  \n       DESTIN_hex1090         DESTIN_hex1091         DESTIN_hex1092  \n             15.56674               16.54548               16.03890  \n       DESTIN_hex1093         DESTIN_hex1095         DESTIN_hex1096  \n             14.97487               16.74975               18.51822  \n       DESTIN_hex1097         DESTIN_hex1099         DESTIN_hex1103  \n             16.78929               15.62976               17.05497  \n       DESTIN_hex1104         DESTIN_hex1106         DESTIN_hex1107  \n             15.80158               16.38075               16.89758  \n       DESTIN_hex1108         DESTIN_hex1109         DESTIN_hex1110  \n             15.58815               15.84993               16.14068  \n       DESTIN_hex1111         DESTIN_hex1112         DESTIN_hex1115  \n             14.57433               15.81470               16.05808  \n       DESTIN_hex1116         DESTIN_hex1124         DESTIN_hex1125  \n             17.16894               16.58038               16.75230  \n       DESTIN_hex1126         DESTIN_hex1127         DESTIN_hex1128  \n             17.99302               15.12216               16.00799  \n       DESTIN_hex1129         DESTIN_hex1130         DESTIN_hex1131  \n             16.43584               16.95588               17.06439  \n       DESTIN_hex1133         DESTIN_hex1134         DESTIN_hex1142  \n             15.66198               16.31346               17.04235  \n       DESTIN_hex1143         DESTIN_hex1144         DESTIN_hex1145  \n             16.33132               16.04894               16.56583  \n       DESTIN_hex1146         DESTIN_hex1147         DESTIN_hex1148  \n             17.95070               15.33209               17.16456  \n       DESTIN_hex1149         DESTIN_hex1152         DESTIN_hex1153  \n             17.22080               15.53158               15.23291  \n       DESTIN_hex1161         DESTIN_hex1162         DESTIN_hex1163  \n             15.68438               16.44650               16.59687  \n       DESTIN_hex1164         DESTIN_hex1165         DESTIN_hex1166  \n             15.89317               15.39079               17.66286  \n       DESTIN_hex1168         DESTIN_hex1171         DESTIN_hex1178  \n             17.74807               16.10481               14.84058  \n       DESTIN_hex1179         DESTIN_hex1180         DESTIN_hex1181  \n             15.41889               15.94268               15.65323  \n       DESTIN_hex1182         DESTIN_hex1183         DESTIN_hex1187  \n             16.64364               17.19725               14.46070  \n       DESTIN_hex1194         DESTIN_hex1196         DESTIN_hex1197  \n             17.10145               15.20870               16.76871  \n       DESTIN_hex1198         DESTIN_hex1199         DESTIN_hex1200  \n             16.34679               14.85885               16.13751  \n       DESTIN_hex1202         DESTIN_hex1203         DESTIN_hex1205  \n             15.09981               15.61749               13.14147  \n       DESTIN_hex1212         DESTIN_hex1213         DESTIN_hex1214  \n             16.44243               15.91963               15.20517  \n       DESTIN_hex1215         DESTIN_hex1216         DESTIN_hex1217  \n             15.20102               14.79757               16.09384  \n       DESTIN_hex1218         DESTIN_hex1220         DESTIN_hex1221  \n             12.02286               13.47387               15.67443  \n       DESTIN_hex1226         DESTIN_hex1227         DESTIN_hex1228  \n             16.24826               15.03495               16.81403  \n       DESTIN_hex1229         DESTIN_hex1230         DESTIN_hex1231  \n             17.27326               14.51452               14.90120  \n       DESTIN_hex1232         DESTIN_hex1233         DESTIN_hex1234  \n             16.84640               17.01107               14.64603  \n       DESTIN_hex1235         DESTIN_hex1236         DESTIN_hex1243  \n             15.15918               13.82568               17.08061  \n       DESTIN_hex1244         DESTIN_hex1245         DESTIN_hex1246  \n             17.59165               16.34658               16.66235  \n       DESTIN_hex1247         DESTIN_hex1248         DESTIN_hex1249  \n             15.99462               16.30521               15.66722  \n       DESTIN_hex1251         DESTIN_hex1252         DESTIN_hex1257  \n             14.73730               14.44512               15.79583  \n       DESTIN_hex1258         DESTIN_hex1259         DESTIN_hex1260  \n             16.66376               16.58094               15.00128  \n       DESTIN_hex1261         DESTIN_hex1262         DESTIN_hex1263  \n             14.66524               15.74632               17.05141  \n       DESTIN_hex1264         DESTIN_hex1265         DESTIN_hex1266  \n             16.88180               15.79559               16.57847  \n       DESTIN_hex1267         DESTIN_hex1272         DESTIN_hex1273  \n             14.38521               15.81062               16.56383  \n       DESTIN_hex1274         DESTIN_hex1275         DESTIN_hex1276  \n             17.27810               14.31424               17.89279  \n       DESTIN_hex1277         DESTIN_hex1278         DESTIN_hex1279  \n             15.20153               17.00334               14.12549  \n       DESTIN_hex1280         DESTIN_hex1281         DESTIN_hex1285  \n             15.40898               14.28952               14.10558  \n       DESTIN_hex1286         DESTIN_hex1287         DESTIN_hex1288  \n             16.52225               16.01219               16.75140  \n       DESTIN_hex1289         DESTIN_hex1290         DESTIN_hex1291  \n             15.65424               15.32052               15.92627  \n       DESTIN_hex1292         DESTIN_hex1293         DESTIN_hex1299  \n             15.56009               15.60191               15.48524  \n       DESTIN_hex1300         DESTIN_hex1301         DESTIN_hex1302  \n             16.74544               16.86256               16.48860  \n       DESTIN_hex1303         DESTIN_hex1305         DESTIN_hex1306  \n             15.35543               16.52008               15.78356  \n       DESTIN_hex1307         DESTIN_hex1312         DESTIN_hex1313  \n             14.22095               16.14640               16.75410  \n       DESTIN_hex1314         DESTIN_hex1315         DESTIN_hex1316  \n             15.84239               16.53437               14.27942  \n       DESTIN_hex1317         DESTIN_hex1318         DESTIN_hex1319  \n             17.77337               15.29781               16.21879  \n       DESTIN_hex1320         DESTIN_hex1326         DESTIN_hex1327  \n             15.41495               15.24032               16.86571  \n       DESTIN_hex1328         DESTIN_hex1329         DESTIN_hex1330  \n             16.76643               16.74141               15.34557  \n       DESTIN_hex1331         DESTIN_hex1332         DESTIN_hex1333  \n             16.39562               14.65960               14.94066  \n       DESTIN_hex1336         DESTIN_hex1337         DESTIN_hex1338  \n             15.69548               15.71988               16.73143  \n       DESTIN_hex1339         DESTIN_hex1340         DESTIN_hex1341  \n             17.69232               15.39858               16.36274  \n       DESTIN_hex1342         DESTIN_hex1343         DESTIN_hex1344  \n             17.63913               17.21034               17.01577  \n       DESTIN_hex1345         DESTIN_hex1348         DESTIN_hex1349  \n             15.31524               15.96773               17.31238  \n       DESTIN_hex1350         DESTIN_hex1351         DESTIN_hex1352  \n             17.41589               16.30721               16.64085  \n       DESTIN_hex1353         DESTIN_hex1354         DESTIN_hex1355  \n             16.35558               15.64513               15.39467  \n       DESTIN_hex1356         DESTIN_hex1357         DESTIN_hex1359  \n             15.78402               14.86682               14.63079  \n       DESTIN_hex1360         DESTIN_hex1362         DESTIN_hex1363  \n             15.55259               17.40801               17.45032  \n       DESTIN_hex1365         DESTIN_hex1366         DESTIN_hex1367  \n             15.07951               15.19568               17.28028  \n       DESTIN_hex1368         DESTIN_hex1369         DESTIN_hex1370  \n             16.28239               14.27554               16.72367  \n       DESTIN_hex1371         DESTIN_hex1372         DESTIN_hex1373  \n             17.09392               17.04130               16.26204  \n       DESTIN_hex1374         DESTIN_hex1375         DESTIN_hex1376  \n             15.19424               13.96590               14.82529  \n       DESTIN_hex1377         DESTIN_hex1378         DESTIN_hex1379  \n             15.70340               15.92395               14.07656  \n       DESTIN_hex1382         DESTIN_hex1383         DESTIN_hex1384  \n             15.93269               16.91798               16.97811  \n       DESTIN_hex1388         DESTIN_hex1389         DESTIN_hex1390  \n             14.76255               17.15438               17.41365  \n       DESTIN_hex1391         DESTIN_hex1392         DESTIN_hex1393  \n             14.89898               17.35480               15.96185  \n       DESTIN_hex1394         DESTIN_hex1395         DESTIN_hex1397  \n             13.39193               14.66296               16.69654  \n       DESTIN_hex1399         DESTIN_hex1400         DESTIN_hex1401  \n             15.23187               15.24989               14.73293  \n       DESTIN_hex1402         DESTIN_hex1404         DESTIN_hex1405  \n             15.49332               15.03993               16.75270  \n       DESTIN_hex1406         DESTIN_hex1409         DESTIN_hex1410  \n             17.23378               17.20002               13.30387  \n       DESTIN_hex1411         DESTIN_hex1412         DESTIN_hex1413  \n             16.47340               15.44817               11.49031  \n       DESTIN_hex1414         DESTIN_hex1415         DESTIN_hex1416  \n             16.06104               14.08950               15.83026  \n       DESTIN_hex1417         DESTIN_hex1420         DESTIN_hex1422  \n             16.10552               13.12173               15.03269  \n       DESTIN_hex1426         DESTIN_hex1427         DESTIN_hex1428  \n             15.78987               16.66740               16.63393  \n       DESTIN_hex1429         DESTIN_hex1432         DESTIN_hex1433  \n             15.04646               15.76927               14.62921  \n       DESTIN_hex1434         DESTIN_hex1437         DESTIN_hex1438  \n             14.85470               16.15605               16.26119  \n       DESTIN_hex1439         DESTIN_hex1442         DESTIN_hex1444  \n             16.15452               16.73256               14.81276  \n       DESTIN_hex1446         DESTIN_hex1447         DESTIN_hex1448  \n             15.95868               17.24665               15.90376  \n       DESTIN_hex1450         DESTIN_hex1451         DESTIN_hex1456  \n             13.21318               14.57710               15.52288  \n       DESTIN_hex1457         DESTIN_hex1458         DESTIN_hex1459  \n             17.58072               14.47280               16.38068  \n       DESTIN_hex1460         DESTIN_hex1461         DESTIN_hex1465  \n             15.45427               17.48512               15.60079  \n       DESTIN_hex1466         DESTIN_hex1467         DESTIN_hex1468  \n             17.13795               16.78346               17.32593  \n       DESTIN_hex1469         DESTIN_hex1470         DESTIN_hex1471  \n             16.08859               16.34632               17.40516  \n       DESTIN_hex1475         DESTIN_hex1476         DESTIN_hex1477  \n             15.76939               16.17682               15.29621  \n       DESTIN_hex1478         DESTIN_hex1479         DESTIN_hex1480  \n             16.22616               15.89087               15.95430  \n       DESTIN_hex1481         DESTIN_hex1484         DESTIN_hex1485  \n             14.18592               15.63891               15.89463  \n       DESTIN_hex1486         DESTIN_hex1487         DESTIN_hex1488  \n             17.16933               15.66301               14.56335  \n       DESTIN_hex1489         DESTIN_hex1491         DESTIN_hex1492  \n             16.22799               15.09935               16.14070  \n       DESTIN_hex1493         DESTIN_hex1494         DESTIN_hex1495  \n             15.13821               18.24505               15.46343  \n       DESTIN_hex1496         DESTIN_hex1497         DESTIN_hex1499  \n             15.88831               15.14075               15.43285  \n       DESTIN_hex1500         DESTIN_hex1501         DESTIN_hex1502  \n             15.09123               15.53684               17.18306  \n       DESTIN_hex1503         DESTIN_hex1504         DESTIN_hex1506  \n             14.05674               16.39559               15.49875  \n       DESTIN_hex1507         DESTIN_hex1508         DESTIN_hex1509  \n             17.63195               17.20760               15.17774  \n       DESTIN_hex1510         DESTIN_hex1511         DESTIN_hex1513  \n             15.17863               18.30008               15.61082  \n       DESTIN_hex1514         DESTIN_hex1515         DESTIN_hex1516  \n             15.66820               17.50274               16.39025  \n       DESTIN_hex1517         DESTIN_hex1518         DESTIN_hex1522  \n             17.30607               15.26933               14.48982  \n       DESTIN_hex1523         DESTIN_hex1524         DESTIN_hex1525  \n             15.90264               15.41000               16.08385  \n       DESTIN_hex1528         DESTIN_hex1529         DESTIN_hex1530  \n             16.50118               15.37757               16.42357  \n       DESTIN_hex1531         DESTIN_hex1532         DESTIN_hex1534  \n             16.30016               15.54965               15.24733  \n       DESTIN_hex1535         DESTIN_hex1536         DESTIN_hex1537  \n             16.96461               16.00129               15.66600  \n       DESTIN_hex1538         DESTIN_hex1540         DESTIN_hex1541  \n             15.73754               17.42169               15.70294  \n       DESTIN_hex1542         DESTIN_hex1543         DESTIN_hex1544  \n             16.03747               16.03281               15.28209  \n       DESTIN_hex1546         DESTIN_hex1547         DESTIN_hex1548  \n             17.78386               14.34794               17.48345  \n       DESTIN_hex1549         DESTIN_hex1550         DESTIN_hex1551  \n             14.65718               16.73587               15.56303  \n       DESTIN_hex1553         DESTIN_hex1554         DESTIN_hex1555  \n             17.24317               17.26108               15.75489  \n       DESTIN_hex1556         DESTIN_hex1557         DESTIN_hex1563  \n             15.07187               17.47967               14.87263  \n       DESTIN_hex1564         DESTIN_hex1565         DESTIN_hex1570  \n             15.55012               15.95036               16.12173  \n       DESTIN_hex1571         DESTIN_hex1572         DESTIN_hex1575  \n             14.82349               15.18867               17.44411  \n       DESTIN_hex1579         DESTIN_hex1583         DESTIN_hex1584  \n             15.53720               15.36478               17.88059  \n       DESTIN_hex1587         DESTIN_hex1588         DESTIN_hex1592  \n             16.07606               16.79744               18.16684  \n       DESTIN_hex1594         DESTIN_hex1602         DESTIN_hex1603  \n             15.59905               13.56039               15.93855  \n       DESTIN_hex1608         DESTIN_hex1609         DESTIN_hex1616  \n             18.23757               17.67051               16.72603  \n       DESTIN_hex1623         DESTIN_hex1630         DESTIN_hex1643  \n             17.58242               15.22870               13.21342  \n       DESTIN_hex1644         DESTIN_hex1665            o_biz_count  \n             17.94311               17.30459               -0.10419  \n       o_school_count            o_fin_count             o_hc_count  \n              0.01728                0.24959                0.01680  \n      o_busstop_count        o_housing_count  o_leisure_recre_count  \n              0.42481                0.11145               -0.04701  \n       o_retail_count        o_entertn_count       o_food_bev_count  \n             -0.05542                0.11920                0.03067  \n                 dist  \n             -1.57648  \n\nDegrees of Freedom: 51926 Total (i.e. Null);  51118 Residual\nNull Deviance:      2.38e+08 \nResidual Deviance: 28350000     AIC: 28620000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#doubly-constraint-spatial-interaction-model",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#doubly-constraint-spatial-interaction-model",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.9 Doubly Constraint Spatial Interaction Model",
    "text": "4.9 Doubly Constraint Spatial Interaction Model\nIn this section, we will fit a doubly constrained Spatial Interaction Model (SIM). For doubly constrained SIM, both the attractiveness at the destinations and the propulsiveness at the origins are considered. The model is typically expressed in the form of a distance function between the origin and destination.\n\ndbcSIM_weekday_morn &lt;- glm(formula = TOTAL_TRIPS ~ \n                ORIGIN_hex + \n                DESTIN_hex + \n                dist,\n              family = poisson(link = \"log\"),\n              data = flow_data_weekday_morn_log,\n              na.action = na.exclude)\n\nIn the formula argument, we specify our response variable TOTAL_TRIPS and our explanatory variables. The explanatory variables include ORIGIN_hex, DESTIN_hex, and dist.\n\ndbcSIM_weekday_morn\n\n\nCall:  glm(formula = TOTAL_TRIPS ~ ORIGIN_hex + DESTIN_hex + dist, family = poisson(link = \"log\"), \n    data = flow_data_weekday_morn_log, na.action = na.exclude)\n\nCoefficients:\n   (Intercept)    ORIGIN_hex31    ORIGIN_hex39    ORIGIN_hex40    ORIGIN_hex41  \n     13.616923        3.108157        1.282386        2.764515       -0.751642  \n  ORIGIN_hex49    ORIGIN_hex50    ORIGIN_hex51    ORIGIN_hex52    ORIGIN_hex59  \n      4.517471        2.119243        0.109137        3.183700        3.218399  \n  ORIGIN_hex60    ORIGIN_hex61    ORIGIN_hex62    ORIGIN_hex63    ORIGIN_hex72  \n      0.348340        2.585629        2.482757       -0.542673        0.866867  \n  ORIGIN_hex73    ORIGIN_hex74    ORIGIN_hex75    ORIGIN_hex83    ORIGIN_hex84  \n      0.510129        1.420133        1.437850        3.266378        2.489560  \n  ORIGIN_hex85    ORIGIN_hex86    ORIGIN_hex87    ORIGIN_hex88    ORIGIN_hex89  \n      1.309737        0.233333        1.498716        2.292051        2.794073  \n  ORIGIN_hex96    ORIGIN_hex97    ORIGIN_hex99   ORIGIN_hex100   ORIGIN_hex101  \n      3.993111        1.516809        4.051616        5.876286        6.245135  \n ORIGIN_hex112   ORIGIN_hex113   ORIGIN_hex114   ORIGIN_hex124   ORIGIN_hex125  \n      1.825790        3.075366       -2.148361       -1.050485        3.154596  \n ORIGIN_hex126   ORIGIN_hex135   ORIGIN_hex136   ORIGIN_hex137   ORIGIN_hex145  \n      1.085151        3.011202        6.560826        3.869734        3.021261  \n ORIGIN_hex146   ORIGIN_hex147   ORIGIN_hex155   ORIGIN_hex156   ORIGIN_hex157  \n      1.320642        3.805546        1.006940        2.357705        1.549026  \n ORIGIN_hex168   ORIGIN_hex169   ORIGIN_hex170   ORIGIN_hex181   ORIGIN_hex182  \n      1.622059        4.953055        2.146072        2.320082        1.339977  \n ORIGIN_hex183   ORIGIN_hex197   ORIGIN_hex198   ORIGIN_hex199   ORIGIN_hex213  \n      5.278273        1.065849        1.541266        4.154037        1.778812  \n ORIGIN_hex214   ORIGIN_hex215   ORIGIN_hex231   ORIGIN_hex232   ORIGIN_hex233  \n      1.885077        2.672229        0.563557        1.120968        2.437003  \n ORIGIN_hex249   ORIGIN_hex250   ORIGIN_hex252   ORIGIN_hex265   ORIGIN_hex266  \n      2.503004        1.323100        1.225298        4.281566        2.575480  \n ORIGIN_hex267   ORIGIN_hex268   ORIGIN_hex269   ORIGIN_hex283   ORIGIN_hex284  \n      7.297937        3.969403        3.420953        0.833831        3.480314  \n ORIGIN_hex286   ORIGIN_hex287   ORIGIN_hex300   ORIGIN_hex301   ORIGIN_hex302  \n      2.571749        1.992990        2.601021        2.946429        3.501185  \n ORIGIN_hex303   ORIGIN_hex304   ORIGIN_hex319   ORIGIN_hex320   ORIGIN_hex321  \n      6.302940        3.363259        3.937064        4.048172        5.509173  \n ORIGIN_hex322   ORIGIN_hex323   ORIGIN_hex334   ORIGIN_hex335   ORIGIN_hex336  \n      3.093915        0.798616        0.360817        3.328856        3.573313  \n ORIGIN_hex337   ORIGIN_hex339   ORIGIN_hex340   ORIGIN_hex351   ORIGIN_hex352  \n      6.347291        3.438284        5.051709        5.189642        2.141634  \n ORIGIN_hex353   ORIGIN_hex354   ORIGIN_hex355   ORIGIN_hex356   ORIGIN_hex357  \n      5.475870        6.606664        2.855658        0.887057        0.852339  \n ORIGIN_hex367   ORIGIN_hex368   ORIGIN_hex369   ORIGIN_hex370   ORIGIN_hex371  \n      2.349429        2.299380        3.004457        7.049873        6.500478  \n ORIGIN_hex373   ORIGIN_hex374   ORIGIN_hex375   ORIGIN_hex376   ORIGIN_hex377  \n      1.456174        1.430720        3.141643        1.640500        2.437582  \n ORIGIN_hex385   ORIGIN_hex386   ORIGIN_hex387   ORIGIN_hex388   ORIGIN_hex390  \n     -1.649634        1.565683        2.842534        6.211599        0.992620  \n ORIGIN_hex393   ORIGIN_hex394   ORIGIN_hex395   ORIGIN_hex402   ORIGIN_hex403  \n      3.559711        4.982268        2.204878        2.232645        2.583991  \n ORIGIN_hex404   ORIGIN_hex405   ORIGIN_hex407   ORIGIN_hex411   ORIGIN_hex412  \n      7.804138        5.784762       -0.700720        1.524585        2.083496  \n ORIGIN_hex413   ORIGIN_hex419   ORIGIN_hex420   ORIGIN_hex421   ORIGIN_hex422  \n      5.297753        2.422578        3.409557        2.423697        6.340159  \n ORIGIN_hex424   ORIGIN_hex430   ORIGIN_hex437   ORIGIN_hex438   ORIGIN_hex439  \n      0.676274        2.068597        2.158178        4.259282        4.965663  \n ORIGIN_hex440   ORIGIN_hex442   ORIGIN_hex453   ORIGIN_hex454   ORIGIN_hex455  \n      5.488988        3.424790        2.563508        1.031783        2.994377  \n ORIGIN_hex456   ORIGIN_hex471   ORIGIN_hex472   ORIGIN_hex473   ORIGIN_hex474  \n      7.074776        4.218739        6.079784        5.711551        6.801711  \n ORIGIN_hex476   ORIGIN_hex487   ORIGIN_hex488   ORIGIN_hex489   ORIGIN_hex490  \n      6.396342        4.594346        2.807507        6.352599        4.941993  \n ORIGIN_hex504   ORIGIN_hex505   ORIGIN_hex506   ORIGIN_hex508   ORIGIN_hex518  \n      5.122761        5.919770        6.611424        2.386124        3.372878  \n ORIGIN_hex521   ORIGIN_hex522   ORIGIN_hex524   ORIGIN_hex533   ORIGIN_hex534  \n      5.743532        2.026695        2.640870        2.533237        4.904280  \n ORIGIN_hex536   ORIGIN_hex537   ORIGIN_hex539   ORIGIN_hex549   ORIGIN_hex550  \n      5.754481        6.114337        6.396797        6.324043        6.150883  \n ORIGIN_hex551   ORIGIN_hex552   ORIGIN_hex554   ORIGIN_hex555   ORIGIN_hex559  \n      6.645373        6.435831        7.936206        1.621963        2.204942  \n ORIGIN_hex562   ORIGIN_hex564   ORIGIN_hex565   ORIGIN_hex566   ORIGIN_hex567  \n      2.294885        3.782419        5.497732        6.237086        5.497976  \n ORIGIN_hex568   ORIGIN_hex569   ORIGIN_hex577   ORIGIN_hex579   ORIGIN_hex580  \n      7.731980        5.644009        0.805500        5.239358        6.070136  \n ORIGIN_hex581   ORIGIN_hex582   ORIGIN_hex583   ORIGIN_hex584   ORIGIN_hex585  \n      6.460469        6.399364        5.484094        6.315175        7.110345  \n ORIGIN_hex586   ORIGIN_hex588   ORIGIN_hex589   ORIGIN_hex593   ORIGIN_hex594  \n      5.707877        2.015736        3.851516        2.602769        5.181994  \n ORIGIN_hex595   ORIGIN_hex596   ORIGIN_hex597   ORIGIN_hex598   ORIGIN_hex599  \n      4.401954        6.471517        6.110269        6.967940        6.782857  \n ORIGIN_hex600   ORIGIN_hex601   ORIGIN_hex603   ORIGIN_hex604   ORIGIN_hex609  \n      7.417486        6.762482        2.133127        4.377790        2.816489  \n ORIGIN_hex610   ORIGIN_hex611   ORIGIN_hex612   ORIGIN_hex613   ORIGIN_hex614  \n      2.970151        4.986625        3.634936        5.399528        6.177346  \n ORIGIN_hex615   ORIGIN_hex616   ORIGIN_hex617   ORIGIN_hex618   ORIGIN_hex619  \n      5.916952        6.817746        5.468504        7.716098        2.864944  \n ORIGIN_hex620   ORIGIN_hex625   ORIGIN_hex626   ORIGIN_hex627   ORIGIN_hex628  \n      2.142215        2.233070        4.233454        3.411942        4.608153  \n ORIGIN_hex629   ORIGIN_hex630   ORIGIN_hex631   ORIGIN_hex632   ORIGIN_hex633  \n      7.198236        6.242075        5.170560        5.090560        3.910853  \n ORIGIN_hex634   ORIGIN_hex635   ORIGIN_hex636   ORIGIN_hex643   ORIGIN_hex644  \n      3.708353        1.943813        3.088617        5.154151        5.078427  \n ORIGIN_hex645   ORIGIN_hex646   ORIGIN_hex649   ORIGIN_hex650   ORIGIN_hex651  \n      3.612085        6.502796        6.011781        3.679306        5.107627  \n ORIGIN_hex652   ORIGIN_hex653   ORIGIN_hex654   ORIGIN_hex659   ORIGIN_hex660  \n      2.525633        1.355156        1.549869        4.844835        5.289456  \n ORIGIN_hex661   ORIGIN_hex662   ORIGIN_hex663   ORIGIN_hex665   ORIGIN_hex666  \n      3.610826        4.915898        5.579789        4.254335        6.295522  \n ORIGIN_hex668   ORIGIN_hex669   ORIGIN_hex670   ORIGIN_hex676   ORIGIN_hex677  \n      2.315667        4.942607        3.506751        5.605513        5.468181  \n ORIGIN_hex678   ORIGIN_hex680   ORIGIN_hex681   ORIGIN_hex682   ORIGIN_hex683  \n      5.657116        3.755243        6.142685        4.724671        7.395829  \n ORIGIN_hex687   ORIGIN_hex688   ORIGIN_hex693   ORIGIN_hex694   ORIGIN_hex695  \n      7.484405        3.021750        4.450179        6.287707        7.603166  \n ORIGIN_hex696   ORIGIN_hex697   ORIGIN_hex698   ORIGIN_hex699   ORIGIN_hex700  \n      3.533237        4.774685        3.799294        5.859449        5.768208  \n ORIGIN_hex701   ORIGIN_hex703   ORIGIN_hex706   ORIGIN_hex711   ORIGIN_hex712  \n      7.001605        1.494280        8.472036        3.522311        5.794016  \n ORIGIN_hex713   ORIGIN_hex715   ORIGIN_hex716   ORIGIN_hex718   ORIGIN_hex722  \n      4.331849        5.530903        4.913303        6.155446        5.020036  \n ORIGIN_hex723   ORIGIN_hex729   ORIGIN_hex730   ORIGIN_hex731   ORIGIN_hex732  \n      5.154208        4.249488       -0.676706        4.791285        4.313797  \n ORIGIN_hex733   ORIGIN_hex735   ORIGIN_hex736   ORIGIN_hex737   ORIGIN_hex739  \n      5.737168        3.652904        7.470399        6.748144        3.643431  \n ORIGIN_hex741   ORIGIN_hex742   ORIGIN_hex746   ORIGIN_hex747   ORIGIN_hex748  \n      6.564464        5.719904        4.277217        2.809785        5.767287  \n ORIGIN_hex749   ORIGIN_hex750   ORIGIN_hex751   ORIGIN_hex753   ORIGIN_hex754  \n      4.576561        3.697110        4.840363        5.808926        5.356103  \n ORIGIN_hex758   ORIGIN_hex759   ORIGIN_hex760   ORIGIN_hex764   ORIGIN_hex766  \n      4.812751        5.777508        2.167080        6.112824        5.353553  \n ORIGIN_hex767   ORIGIN_hex769   ORIGIN_hex775   ORIGIN_hex777   ORIGIN_hex778  \n      4.693406        4.654197        3.987131        5.543328        5.024799  \n ORIGIN_hex782   ORIGIN_hex783   ORIGIN_hex784   ORIGIN_hex785   ORIGIN_hex786  \n      2.168065        4.764671        5.856641        3.414738        6.634781  \n ORIGIN_hex794   ORIGIN_hex795   ORIGIN_hex796   ORIGIN_hex799   ORIGIN_hex800  \n      2.810848        7.837061        6.321947        4.423156        4.556793  \n ORIGIN_hex801   ORIGIN_hex802   ORIGIN_hex810   ORIGIN_hex812   ORIGIN_hex813  \n      3.397677        4.069969        0.006596        5.809800        4.718843  \n ORIGIN_hex816   ORIGIN_hex817   ORIGIN_hex818   ORIGIN_hex819   ORIGIN_hex820  \n      4.675665        4.371035        2.823024        6.355046        4.110540  \n ORIGIN_hex821   ORIGIN_hex827   ORIGIN_hex829   ORIGIN_hex830   ORIGIN_hex831  \n      4.665753        3.011651        4.472325        5.992520        5.154307  \n ORIGIN_hex835   ORIGIN_hex836   ORIGIN_hex837   ORIGIN_hex838   ORIGIN_hex847  \n      3.609314        4.812077        4.295062        4.722881        6.141961  \n ORIGIN_hex848   ORIGIN_hex849   ORIGIN_hex851   ORIGIN_hex853   ORIGIN_hex854  \n      6.046644        3.433145        3.667767        4.332216        4.224064  \n ORIGIN_hex856   ORIGIN_hex863   ORIGIN_hex864   ORIGIN_hex865   ORIGIN_hex866  \n      5.409492        3.279049        3.930718        7.203848        5.597089  \n ORIGIN_hex867   ORIGIN_hex868   ORIGIN_hex869   ORIGIN_hex870   ORIGIN_hex871  \n     -2.412780        2.330203        3.712496        5.249502        6.539811  \n ORIGIN_hex873   ORIGIN_hex882   ORIGIN_hex883   ORIGIN_hex884   ORIGIN_hex885  \n      4.044716        6.697557        6.139965        4.549370        1.088550  \n ORIGIN_hex887   ORIGIN_hex888   ORIGIN_hex889   ORIGIN_hex890   ORIGIN_hex899  \n      4.336156        5.212590        4.628424        4.638126        3.311433  \n ORIGIN_hex901   ORIGIN_hex902   ORIGIN_hex903   ORIGIN_hex905   ORIGIN_hex906  \n      5.901097        5.543477        3.357407        5.347449        4.608512  \n ORIGIN_hex907   ORIGIN_hex909   ORIGIN_hex910   ORIGIN_hex917   ORIGIN_hex920  \n      5.599818        5.496417        4.832996        1.712665        6.300146  \n ORIGIN_hex922   ORIGIN_hex925   ORIGIN_hex926   ORIGIN_hex927   ORIGIN_hex928  \n      2.824797        5.054287        5.194211        5.032163        2.874891  \n ORIGIN_hex929   ORIGIN_hex936   ORIGIN_hex937   ORIGIN_hex940   ORIGIN_hex941  \n      4.564464        3.667964        2.863301        5.264751        4.598075  \n ORIGIN_hex944   ORIGIN_hex945   ORIGIN_hex946   ORIGIN_hex947   ORIGIN_hex949  \n      4.127775        5.042834        4.904495        3.245281        4.255790  \n ORIGIN_hex955   ORIGIN_hex959   ORIGIN_hex960   ORIGIN_hex962   ORIGIN_hex963  \n      2.492899        5.806873        6.090583        2.587150        5.030036  \n ORIGIN_hex964   ORIGIN_hex965   ORIGIN_hex966   ORIGIN_hex968   ORIGIN_hex969  \n      5.094024        6.076668        3.568290        4.882531        1.889541  \n ORIGIN_hex974   ORIGIN_hex975   ORIGIN_hex978   ORIGIN_hex979   ORIGIN_hex980  \n      0.277375        5.157647        3.599007        7.311119        4.913297  \n ORIGIN_hex983   ORIGIN_hex984   ORIGIN_hex985   ORIGIN_hex986   ORIGIN_hex989  \n      6.092696        4.561877        4.001677        4.602953        2.429395  \n ORIGIN_hex993   ORIGIN_hex994   ORIGIN_hex995   ORIGIN_hex996   ORIGIN_hex998  \n      0.896046        6.747206        2.801903        1.715330        6.128571  \n ORIGIN_hex999  ORIGIN_hex1002  ORIGIN_hex1004  ORIGIN_hex1005  ORIGIN_hex1006  \n      5.581576        0.453172        5.602553        6.476839        3.797241  \nORIGIN_hex1008  ORIGIN_hex1013  ORIGIN_hex1015  ORIGIN_hex1016  ORIGIN_hex1017  \n      5.885483        4.739547        4.532675        3.121235        5.447631  \nORIGIN_hex1018  ORIGIN_hex1019  ORIGIN_hex1023  ORIGIN_hex1024  ORIGIN_hex1025  \n      5.207965        5.636500        3.988803        5.673004        5.209754  \nORIGIN_hex1026  ORIGIN_hex1027  ORIGIN_hex1028  ORIGIN_hex1029  ORIGIN_hex1031  \n      4.521470        4.284296        1.721053        0.219987        4.630951  \nORIGIN_hex1032  ORIGIN_hex1034  ORIGIN_hex1035  ORIGIN_hex1036  ORIGIN_hex1037  \n      5.003078        2.867719        1.572434        4.993633        7.352428  \nORIGIN_hex1038  ORIGIN_hex1039  ORIGIN_hex1044  ORIGIN_hex1045  ORIGIN_hex1046  \n      5.975796        3.043416        4.834171        5.088752        4.516462  \nORIGIN_hex1047  ORIGIN_hex1048  ORIGIN_hex1051  ORIGIN_hex1052  ORIGIN_hex1053  \n      3.916671        3.674640        5.373200        4.765551        4.264612  \nORIGIN_hex1056  ORIGIN_hex1057  ORIGIN_hex1058  ORIGIN_hex1059  ORIGIN_hex1063  \n      7.485690        5.747987        5.554830        6.873288        1.076369  \nORIGIN_hex1065  ORIGIN_hex1067  ORIGIN_hex1068  ORIGIN_hex1069  ORIGIN_hex1070  \n      2.684223        4.944552        3.896049        4.925303        5.368740  \nORIGIN_hex1071  ORIGIN_hex1072  ORIGIN_hex1073  ORIGIN_hex1075  ORIGIN_hex1076  \n      5.319544        5.832816        3.654284        3.749801        6.009215  \nORIGIN_hex1077  ORIGIN_hex1078  ORIGIN_hex1079  ORIGIN_hex1083  ORIGIN_hex1085  \n      6.730801        5.890375        4.810417        5.184913        4.400650  \nORIGIN_hex1086  ORIGIN_hex1088  ORIGIN_hex1089  ORIGIN_hex1090  ORIGIN_hex1091  \n      5.527132        3.800116        3.399453        5.018021        6.105192  \nORIGIN_hex1092  ORIGIN_hex1093  ORIGIN_hex1095  ORIGIN_hex1096  ORIGIN_hex1097  \n      6.157005        4.648733        6.744666        7.580549        6.135281  \nORIGIN_hex1099  ORIGIN_hex1103  ORIGIN_hex1104  ORIGIN_hex1106  ORIGIN_hex1107  \n      6.103983        3.230693        4.232010        5.230622        6.095340  \nORIGIN_hex1108  ORIGIN_hex1109  ORIGIN_hex1110  ORIGIN_hex1111  ORIGIN_hex1112  \n      5.280077        4.946745        5.478459        5.395796        5.257270  \nORIGIN_hex1115  ORIGIN_hex1116  ORIGIN_hex1124  ORIGIN_hex1125  ORIGIN_hex1126  \n      6.602856        7.742905        4.186497        5.306018        6.712183  \nORIGIN_hex1127  ORIGIN_hex1128  ORIGIN_hex1129  ORIGIN_hex1130  ORIGIN_hex1131  \n      3.799982        6.635490        6.276757        6.940560        3.168224  \nORIGIN_hex1133  ORIGIN_hex1134  ORIGIN_hex1142  ORIGIN_hex1143  ORIGIN_hex1144  \n      7.095715        7.176635        6.051460        5.088259        4.684013  \nORIGIN_hex1145  ORIGIN_hex1146  ORIGIN_hex1147  ORIGIN_hex1148  ORIGIN_hex1149  \n      5.637105        6.211563        5.469014        4.917357        2.431127  \nORIGIN_hex1152  ORIGIN_hex1153  ORIGIN_hex1161  ORIGIN_hex1162  ORIGIN_hex1163  \n      6.291306        6.412164        3.256668        5.778929        5.663600  \nORIGIN_hex1164  ORIGIN_hex1165  ORIGIN_hex1166  ORIGIN_hex1168  ORIGIN_hex1171  \n      5.809170        4.683536        6.895264        4.267033        7.201534  \nORIGIN_hex1178  ORIGIN_hex1179  ORIGIN_hex1180  ORIGIN_hex1181  ORIGIN_hex1182  \n      4.933274        5.124088        5.940962        5.432343        6.158984  \nORIGIN_hex1183  ORIGIN_hex1187  ORIGIN_hex1194  ORIGIN_hex1196  ORIGIN_hex1197  \n      5.934290        5.277965        2.831195        4.618703        7.387951  \nORIGIN_hex1198  ORIGIN_hex1199  ORIGIN_hex1200  ORIGIN_hex1202  ORIGIN_hex1203  \n      5.155471        2.882618        6.273532        4.802661        3.855351  \nORIGIN_hex1205  ORIGIN_hex1212  ORIGIN_hex1213  ORIGIN_hex1214  ORIGIN_hex1215  \n      4.918812        4.736029        5.822503        5.289567        4.395017  \nORIGIN_hex1216  ORIGIN_hex1217  ORIGIN_hex1218  ORIGIN_hex1220  ORIGIN_hex1221  \n      4.188166        3.650315        3.045945        1.292917        2.992053  \nORIGIN_hex1226  ORIGIN_hex1227  ORIGIN_hex1228  ORIGIN_hex1229  ORIGIN_hex1230  \n      4.205201        3.533404        6.442040        5.619445        3.671046  \nORIGIN_hex1231  ORIGIN_hex1232  ORIGIN_hex1233  ORIGIN_hex1234  ORIGIN_hex1235  \n      4.672352        4.754175        4.304192        3.136180        2.465481  \nORIGIN_hex1236  ORIGIN_hex1243  ORIGIN_hex1244  ORIGIN_hex1245  ORIGIN_hex1246  \n      1.363680        2.876792        6.339869        5.848773        5.611092  \nORIGIN_hex1247  ORIGIN_hex1248  ORIGIN_hex1249  ORIGIN_hex1251  ORIGIN_hex1252  \n      5.286027        3.532790        5.047053        2.972384        2.501188  \nORIGIN_hex1257  ORIGIN_hex1258  ORIGIN_hex1259  ORIGIN_hex1260  ORIGIN_hex1261  \n      5.694904        5.911076        4.779099        5.216067        4.526783  \nORIGIN_hex1262  ORIGIN_hex1263  ORIGIN_hex1264  ORIGIN_hex1265  ORIGIN_hex1266  \n      5.835086        6.229764        4.413197        6.519695        6.712137  \nORIGIN_hex1267  ORIGIN_hex1272  ORIGIN_hex1273  ORIGIN_hex1274  ORIGIN_hex1275  \n      4.602652        3.405355        4.661204        3.838529        4.533025  \nORIGIN_hex1276  ORIGIN_hex1277  ORIGIN_hex1278  ORIGIN_hex1279  ORIGIN_hex1280  \n      6.622510        5.406593        6.634957        5.418719        6.000338  \nORIGIN_hex1281  ORIGIN_hex1285  ORIGIN_hex1286  ORIGIN_hex1287  ORIGIN_hex1288  \n      2.057143        0.674091        5.415655        4.320850        5.474090  \nORIGIN_hex1289  ORIGIN_hex1290  ORIGIN_hex1291  ORIGIN_hex1292  ORIGIN_hex1293  \n      4.577832        4.726606        5.982332        6.107349        6.317009  \nORIGIN_hex1299  ORIGIN_hex1300  ORIGIN_hex1301  ORIGIN_hex1302  ORIGIN_hex1303  \n      5.062059        6.094828        4.851339        3.370373        5.255177  \nORIGIN_hex1305  ORIGIN_hex1306  ORIGIN_hex1307  ORIGIN_hex1312  ORIGIN_hex1313  \n      6.724970        6.749458        6.769132        5.213089        5.996087  \nORIGIN_hex1314  ORIGIN_hex1315  ORIGIN_hex1316  ORIGIN_hex1317  ORIGIN_hex1318  \n      4.673893        5.104713        3.343826        5.798451        5.897398  \nORIGIN_hex1319  ORIGIN_hex1320  ORIGIN_hex1326  ORIGIN_hex1327  ORIGIN_hex1328  \n      6.086378        6.852998        4.714199        5.558754        4.225752  \nORIGIN_hex1329  ORIGIN_hex1330  ORIGIN_hex1331  ORIGIN_hex1332  ORIGIN_hex1333  \n      6.072341        6.553098        6.141622        5.394134        6.290751  \nORIGIN_hex1336  ORIGIN_hex1337  ORIGIN_hex1338  ORIGIN_hex1339  ORIGIN_hex1340  \n      4.322423        5.095634        5.550748        3.746325        2.198248  \nORIGIN_hex1341  ORIGIN_hex1342  ORIGIN_hex1343  ORIGIN_hex1344  ORIGIN_hex1345  \n      5.464324        6.757933        6.050359        6.546974        7.625523  \nORIGIN_hex1348  ORIGIN_hex1349  ORIGIN_hex1350  ORIGIN_hex1351  ORIGIN_hex1352  \n      4.834591        5.320905        3.584154        3.109326        1.088784  \nORIGIN_hex1353  ORIGIN_hex1354  ORIGIN_hex1355  ORIGIN_hex1356  ORIGIN_hex1357  \n      6.298465        6.111035        6.642484        5.914339        6.447298  \nORIGIN_hex1359  ORIGIN_hex1360  ORIGIN_hex1362  ORIGIN_hex1363  ORIGIN_hex1365  \n      3.148806        5.374744        4.860817        2.563830        6.227816  \nORIGIN_hex1366  ORIGIN_hex1367  ORIGIN_hex1368  ORIGIN_hex1369  ORIGIN_hex1370  \n      5.924298        6.770207        7.084605        5.108004        5.338646  \nORIGIN_hex1371  ORIGIN_hex1372  ORIGIN_hex1373  ORIGIN_hex1374  ORIGIN_hex1375  \n      5.595088        6.083336        3.201310        3.238688        2.657040  \nORIGIN_hex1376  ORIGIN_hex1377  ORIGIN_hex1378  ORIGIN_hex1379  ORIGIN_hex1382  \n      6.387990        6.568276        6.859141        5.166307        4.812123  \nORIGIN_hex1383  ORIGIN_hex1384  ORIGIN_hex1388  ORIGIN_hex1389  ORIGIN_hex1390  \n      5.941699        4.944346        5.982516        7.656599        7.029447  \nORIGIN_hex1391  ORIGIN_hex1392  ORIGIN_hex1393  ORIGIN_hex1394  ORIGIN_hex1395  \n      6.388524        5.759311        5.057960        4.070232        2.580886  \nORIGIN_hex1397  ORIGIN_hex1399  ORIGIN_hex1400  ORIGIN_hex1401  ORIGIN_hex1402  \n      3.595449        6.162915        6.034858        3.629365        4.633325  \nORIGIN_hex1404  ORIGIN_hex1405  ORIGIN_hex1406  ORIGIN_hex1409  ORIGIN_hex1410  \n      5.009325        5.598983        6.232842        4.362552       -0.162738  \nORIGIN_hex1411  ORIGIN_hex1412  ORIGIN_hex1413  ORIGIN_hex1414  ORIGIN_hex1415  \n      5.850066        6.047567        2.173233        5.496955        3.069481  \nORIGIN_hex1416  ORIGIN_hex1417  ORIGIN_hex1420  ORIGIN_hex1422  ORIGIN_hex1426  \n      4.733961        4.849896       -0.561518        6.129023        4.975477  \nORIGIN_hex1427  ORIGIN_hex1428  ORIGIN_hex1429  ORIGIN_hex1432  ORIGIN_hex1433  \n      4.202285        5.085925        3.510367        1.313440        4.880805  \nORIGIN_hex1434  ORIGIN_hex1437  ORIGIN_hex1438  ORIGIN_hex1439  ORIGIN_hex1442  \n      6.751362        4.319667        5.954476        5.841735        5.011017  \nORIGIN_hex1444  ORIGIN_hex1446  ORIGIN_hex1447  ORIGIN_hex1448  ORIGIN_hex1450  \n      6.668492        5.180672        6.050154        5.619075        0.308803  \nORIGIN_hex1451  ORIGIN_hex1456  ORIGIN_hex1457  ORIGIN_hex1458  ORIGIN_hex1459  \n      2.436820        4.211080        6.217279        5.001105        6.092825  \nORIGIN_hex1460  ORIGIN_hex1461  ORIGIN_hex1465  ORIGIN_hex1466  ORIGIN_hex1467  \n      4.439191        4.878575        5.083938        5.946808        6.035122  \nORIGIN_hex1468  ORIGIN_hex1469  ORIGIN_hex1470  ORIGIN_hex1471  ORIGIN_hex1475  \n      5.171141        6.325169        4.985384        5.862115        5.132027  \nORIGIN_hex1476  ORIGIN_hex1477  ORIGIN_hex1478  ORIGIN_hex1479  ORIGIN_hex1480  \n      5.709415        4.585918        5.637774        5.992637        5.796694  \nORIGIN_hex1481  ORIGIN_hex1484  ORIGIN_hex1485  ORIGIN_hex1486  ORIGIN_hex1487  \n      3.574712        5.635757        5.734097        5.529503        5.856408  \nORIGIN_hex1488  ORIGIN_hex1489  ORIGIN_hex1491  ORIGIN_hex1492  ORIGIN_hex1493  \n      5.581573        5.815972        5.178396        6.013572        2.293219  \nORIGIN_hex1494  ORIGIN_hex1495  ORIGIN_hex1496  ORIGIN_hex1497  ORIGIN_hex1499  \n      6.744280        5.691858        6.349061        3.298486        5.814802  \nORIGIN_hex1500  ORIGIN_hex1501  ORIGIN_hex1502  ORIGIN_hex1503  ORIGIN_hex1504  \n      3.758283        4.496083        5.984023        4.158731        4.810434  \nORIGIN_hex1506  ORIGIN_hex1507  ORIGIN_hex1508  ORIGIN_hex1509  ORIGIN_hex1510  \n      4.807011        6.409430        5.288863        5.371370        5.208048  \nORIGIN_hex1511  ORIGIN_hex1513  ORIGIN_hex1514  ORIGIN_hex1515  ORIGIN_hex1516  \n      6.639534        3.075866        4.924024        3.863231        5.449857  \nORIGIN_hex1517  ORIGIN_hex1518  ORIGIN_hex1522  ORIGIN_hex1523  ORIGIN_hex1524  \n      6.075965        5.984041        4.046809        5.759634        5.748298  \nORIGIN_hex1525  ORIGIN_hex1528  ORIGIN_hex1529  ORIGIN_hex1530  ORIGIN_hex1531  \n      5.990897        4.676222        5.162629        6.091706        6.096389  \nORIGIN_hex1532  ORIGIN_hex1534  ORIGIN_hex1535  ORIGIN_hex1536  ORIGIN_hex1537  \n      5.874469        1.759580        4.564342        4.813797        5.775810  \nORIGIN_hex1538  ORIGIN_hex1540  ORIGIN_hex1541  ORIGIN_hex1542  ORIGIN_hex1543  \n      5.906891        2.152557        4.810922        6.035864        5.090731  \nORIGIN_hex1544  ORIGIN_hex1546  ORIGIN_hex1547  ORIGIN_hex1548  ORIGIN_hex1549  \n      5.333651        2.153984        0.899748        2.888695        4.062877  \nORIGIN_hex1550  ORIGIN_hex1551  ORIGIN_hex1553  ORIGIN_hex1554  ORIGIN_hex1555  \n      4.865783        1.995477        3.096286        4.157224        3.473194  \nORIGIN_hex1556  ORIGIN_hex1557  ORIGIN_hex1563  ORIGIN_hex1564  ORIGIN_hex1565  \n      3.807251        2.259716        3.786958        3.494627        2.996990  \nORIGIN_hex1570  ORIGIN_hex1571  ORIGIN_hex1572  ORIGIN_hex1575  ORIGIN_hex1579  \n      4.808206        1.945338        2.915020        5.805359        4.020119  \nORIGIN_hex1583  ORIGIN_hex1584  ORIGIN_hex1587  ORIGIN_hex1588  ORIGIN_hex1592  \n      3.628316        5.501597        4.904405        4.978869        5.042713  \nORIGIN_hex1594  ORIGIN_hex1602  ORIGIN_hex1603  ORIGIN_hex1608  ORIGIN_hex1609  \n      3.996199       -0.212174        3.805208        5.019173        4.958845  \nORIGIN_hex1616  ORIGIN_hex1623  ORIGIN_hex1630  ORIGIN_hex1643  ORIGIN_hex1644  \n      2.877373        4.051563       -2.208282        4.540494        5.845861  \nORIGIN_hex1665    DESTIN_hex31    DESTIN_hex39    DESTIN_hex40    DESTIN_hex41  \n      3.025064       -0.187576        2.278522        2.544351       -0.237904  \n  DESTIN_hex49    DESTIN_hex50    DESTIN_hex51    DESTIN_hex52    DESTIN_hex59  \n     -0.831975        1.968793        1.060073        0.972849        1.139654  \n  DESTIN_hex60    DESTIN_hex61    DESTIN_hex62    DESTIN_hex63    DESTIN_hex72  \n     -1.812486        0.606916        1.259920       -1.035603       -0.386674  \n  DESTIN_hex73    DESTIN_hex74    DESTIN_hex75    DESTIN_hex83    DESTIN_hex84  \n      1.712728        0.495647       -1.061720        0.710929       -0.142231  \n  DESTIN_hex85    DESTIN_hex86    DESTIN_hex87    DESTIN_hex88    DESTIN_hex89  \n      0.050692       -0.016798       -0.207337       -0.068475       -0.494837  \n  DESTIN_hex96    DESTIN_hex97    DESTIN_hex99   DESTIN_hex100   DESTIN_hex101  \n     -1.703752        0.059205        0.878363       -0.530372       -0.085813  \n DESTIN_hex112   DESTIN_hex113   DESTIN_hex114   DESTIN_hex124   DESTIN_hex125  \n     -0.859497       -1.480781       -2.600253       -2.820572       -0.508959  \n DESTIN_hex126   DESTIN_hex135   DESTIN_hex136   DESTIN_hex137   DESTIN_hex145  \n     -2.426458        1.658903       -0.275628       -0.651344       -0.525296  \n DESTIN_hex146   DESTIN_hex147   DESTIN_hex155   DESTIN_hex156   DESTIN_hex157  \n     -1.532602        0.321893       -1.955133       -0.848893       -1.466001  \n DESTIN_hex168   DESTIN_hex169   DESTIN_hex170   DESTIN_hex181   DESTIN_hex182  \n      1.011349       -0.489911       -0.975182        0.972728       -0.658577  \n DESTIN_hex183   DESTIN_hex197   DESTIN_hex198   DESTIN_hex199   DESTIN_hex213  \n     -2.519065        0.957786       -0.381377        0.231234        0.057529  \n DESTIN_hex214   DESTIN_hex215   DESTIN_hex231   DESTIN_hex232   DESTIN_hex233  \n      0.506952        0.483172       -2.815611       -1.907584        0.195582  \n DESTIN_hex249   DESTIN_hex250   DESTIN_hex252   DESTIN_hex265   DESTIN_hex266  \n      0.039566       -0.678331        0.671413        0.652161        0.073165  \n DESTIN_hex267   DESTIN_hex268   DESTIN_hex269   DESTIN_hex283   DESTIN_hex284  \n      0.393636        0.132072        1.439334       -1.630653       -0.175125  \n DESTIN_hex286   DESTIN_hex287   DESTIN_hex288   DESTIN_hex300   DESTIN_hex301  \n     -5.007696       -0.732452       -6.341390       -0.023013        0.597229  \n DESTIN_hex302   DESTIN_hex303   DESTIN_hex304   DESTIN_hex319   DESTIN_hex320  \n     -0.515704       -0.456819       -0.702850       -0.678971       -1.149451  \n DESTIN_hex321   DESTIN_hex322   DESTIN_hex323   DESTIN_hex334   DESTIN_hex335  \n     -2.100774        1.825228       -6.362177       -1.759705        1.273782  \n DESTIN_hex336   DESTIN_hex337   DESTIN_hex339   DESTIN_hex340   DESTIN_hex351  \n     -0.838483       -0.511374       -1.495990       -0.939236        1.491645  \n DESTIN_hex352   DESTIN_hex353   DESTIN_hex354   DESTIN_hex355   DESTIN_hex356  \n     -0.391576       -0.533775       -0.242561       -2.040818       -3.428794  \n DESTIN_hex357   DESTIN_hex367   DESTIN_hex368   DESTIN_hex369   DESTIN_hex370  \n     -3.597187       -0.473484       -0.740286       -1.904012        0.129137  \n DESTIN_hex371   DESTIN_hex373   DESTIN_hex374   DESTIN_hex375   DESTIN_hex377  \n     -0.579523       -3.343109       -1.738363       -1.658665       -2.845712  \n DESTIN_hex385   DESTIN_hex386   DESTIN_hex387   DESTIN_hex388   DESTIN_hex390  \n      1.001161        0.096877        0.043416       -1.265986       -2.708018  \n DESTIN_hex393   DESTIN_hex394   DESTIN_hex395   DESTIN_hex402   DESTIN_hex403  \n     -2.686528       -0.807004       -1.634769        0.333962       -0.762848  \n DESTIN_hex404   DESTIN_hex405   DESTIN_hex407   DESTIN_hex411   DESTIN_hex412  \n      1.343963       -0.987975       -3.418935       -3.763640       -3.025822  \n DESTIN_hex413   DESTIN_hex419   DESTIN_hex420   DESTIN_hex421   DESTIN_hex422  \n     -1.694120       -1.128288       -0.346026       -0.424975       -0.898678  \n DESTIN_hex424   DESTIN_hex430   DESTIN_hex437   DESTIN_hex438   DESTIN_hex439  \n     -5.709741       -3.647132       -1.566824       -1.126878       -1.635478  \n DESTIN_hex440   DESTIN_hex442   DESTIN_hex453   DESTIN_hex454   DESTIN_hex455  \n     -1.781536       -1.043359        0.030305       -3.000408       -1.330545  \n DESTIN_hex456   DESTIN_hex471   DESTIN_hex472   DESTIN_hex473   DESTIN_hex474  \n      1.034798       -0.213517        0.102928       -1.080671        0.079060  \n DESTIN_hex476   DESTIN_hex487   DESTIN_hex488   DESTIN_hex489   DESTIN_hex490  \n     -0.930355        0.679860       -1.829050       -0.030506       -2.560693  \n DESTIN_hex504   DESTIN_hex505   DESTIN_hex506   DESTIN_hex508   DESTIN_hex518  \n     -1.430268       -2.844977       -0.357997       -1.942458       -0.776971  \n DESTIN_hex521   DESTIN_hex522   DESTIN_hex524   DESTIN_hex533   DESTIN_hex534  \n     -0.561137       -3.010794       -4.083158        0.756328        0.171510  \n DESTIN_hex536   DESTIN_hex537   DESTIN_hex539   DESTIN_hex549   DESTIN_hex550  \n     -0.537185       -1.059692       -3.023655       -0.674295       -0.257481  \n DESTIN_hex551   DESTIN_hex552   DESTIN_hex554   DESTIN_hex555   DESTIN_hex559  \n     -0.321796       -0.801919       -1.387247       -3.689474       -1.372026  \n DESTIN_hex562   DESTIN_hex564   DESTIN_hex565   DESTIN_hex566   DESTIN_hex567  \n     -0.295076       -1.595243       -1.406230       -0.934724       -2.354647  \n DESTIN_hex568   DESTIN_hex569   DESTIN_hex577   DESTIN_hex579   DESTIN_hex580  \n     -1.701569       -2.933371       -0.505739       -0.658553        0.949814  \n DESTIN_hex581   DESTIN_hex582   DESTIN_hex583   DESTIN_hex584   DESTIN_hex585  \n     -0.888170       -1.588609       -4.040085       -1.435376       -1.136580  \n DESTIN_hex586   DESTIN_hex588   DESTIN_hex589   DESTIN_hex593   DESTIN_hex594  \n     -2.049028       -0.398305        0.177980       -1.433955       -0.914339  \n DESTIN_hex595   DESTIN_hex596   DESTIN_hex597   DESTIN_hex598   DESTIN_hex599  \n      0.297195       -1.180197       -0.492082       -1.294776       -1.480038  \n DESTIN_hex600   DESTIN_hex601   DESTIN_hex603   DESTIN_hex604   DESTIN_hex609  \n      0.615211       -0.022594       -1.641160       -1.001708       -0.139007  \n DESTIN_hex610   DESTIN_hex611   DESTIN_hex612   DESTIN_hex613   DESTIN_hex614  \n      0.456000       -1.342586       -0.167488       -1.327837        0.081230  \n DESTIN_hex615   DESTIN_hex616   DESTIN_hex617   DESTIN_hex618   DESTIN_hex619  \n     -1.484294       -0.778361       -2.515123       -1.500650       -0.064653  \n DESTIN_hex620   DESTIN_hex625   DESTIN_hex626   DESTIN_hex627   DESTIN_hex628  \n     -0.947770       -0.626885       -0.785043        0.593832        0.696592  \n DESTIN_hex629   DESTIN_hex630   DESTIN_hex631   DESTIN_hex632   DESTIN_hex633  \n      0.172336       -2.930094       -1.042349       -3.160708       -2.513428  \n DESTIN_hex634   DESTIN_hex635   DESTIN_hex636   DESTIN_hex643   DESTIN_hex644  \n      0.423642       -3.733362       -1.270835        0.213558       -1.735984  \n DESTIN_hex645   DESTIN_hex646   DESTIN_hex649   DESTIN_hex650   DESTIN_hex651  \n      0.160192       -0.358780       -1.226304       -2.651879        0.640828  \n DESTIN_hex652   DESTIN_hex653   DESTIN_hex654   DESTIN_hex659   DESTIN_hex660  \n     -3.256495       -1.617044       -1.778259       -0.301402       -0.741540  \n DESTIN_hex661   DESTIN_hex662   DESTIN_hex663   DESTIN_hex665   DESTIN_hex666  \n     -0.252528        1.039575       -1.263705       -2.995773       -1.372388  \n DESTIN_hex668   DESTIN_hex669   DESTIN_hex670   DESTIN_hex676   DESTIN_hex677  \n     -2.468633       -1.483162       -4.785086       -0.444806       -1.568342  \n DESTIN_hex678   DESTIN_hex680   DESTIN_hex681   DESTIN_hex682   DESTIN_hex683  \n     -0.852887       -2.141149       -0.650363       -0.840485        0.886908  \n DESTIN_hex687   DESTIN_hex688   DESTIN_hex693   DESTIN_hex694   DESTIN_hex695  \n      1.755743       -1.882406       -0.537605        0.172666        1.731014  \n DESTIN_hex696   DESTIN_hex697   DESTIN_hex698   DESTIN_hex699   DESTIN_hex700  \n     -0.794165       -0.988788       -2.321381        0.317259       -2.510337  \n DESTIN_hex701   DESTIN_hex703   DESTIN_hex706   DESTIN_hex711   DESTIN_hex712  \n     -1.252065       -0.440214        1.595070        0.649327       -0.419697  \n DESTIN_hex713   DESTIN_hex715   DESTIN_hex716   DESTIN_hex718   DESTIN_hex722  \n     -1.101686       -0.554939       -2.578140       -1.663164       -1.947222  \n DESTIN_hex723   DESTIN_hex729   DESTIN_hex730   DESTIN_hex731   DESTIN_hex732  \n     -1.940415       -0.028209       -0.118156       -1.914752        1.749286  \n DESTIN_hex733   DESTIN_hex735   DESTIN_hex736   DESTIN_hex737   DESTIN_hex739  \n      0.958338       -1.376483       -0.056777       -1.999456       -0.850940  \n DESTIN_hex741   DESTIN_hex742   DESTIN_hex746   DESTIN_hex747   DESTIN_hex748  \n      0.300068       -1.486463        0.892643        0.530864        0.219221  \n DESTIN_hex749   DESTIN_hex750   DESTIN_hex751   DESTIN_hex753   DESTIN_hex754  \n     -1.407708        1.075039       -0.913603       -2.123474       -4.621326  \n DESTIN_hex758   DESTIN_hex759   DESTIN_hex760   DESTIN_hex764   DESTIN_hex766  \n     -1.568644       -1.367470       -5.081470        1.108043        0.065792  \n DESTIN_hex767   DESTIN_hex769   DESTIN_hex775   DESTIN_hex777   DESTIN_hex778  \n     -1.471439       -0.454027        0.217420       -1.462736       -0.728886  \n DESTIN_hex782   DESTIN_hex783   DESTIN_hex784   DESTIN_hex785   DESTIN_hex786  \n      0.297740        0.560241       -0.697202       -2.268227       -1.120786  \n DESTIN_hex794   DESTIN_hex795   DESTIN_hex796   DESTIN_hex799   DESTIN_hex800  \n     -4.140801        1.826242       -0.758004       -1.100986        0.688775  \n DESTIN_hex801   DESTIN_hex802   DESTIN_hex810   DESTIN_hex812   DESTIN_hex813  \n     -1.427141       -1.263053       -3.601536       -0.647000       -1.209652  \n DESTIN_hex816   DESTIN_hex817   DESTIN_hex818   DESTIN_hex819   DESTIN_hex820  \n     -0.108042        0.232969        0.467277        0.948886       -2.393455  \n DESTIN_hex821   DESTIN_hex827   DESTIN_hex829   DESTIN_hex830   DESTIN_hex831  \n     -0.840801        0.885495       -0.208606       -1.478141        0.483431  \n DESTIN_hex835   DESTIN_hex836   DESTIN_hex837   DESTIN_hex838   DESTIN_hex847  \n      0.175686       -1.457707       -1.288210       -1.763097       -1.372625  \n DESTIN_hex848   DESTIN_hex849   DESTIN_hex851   DESTIN_hex853   DESTIN_hex854  \n     -1.258366        0.118397        1.399892       -0.362203       -0.973155  \n DESTIN_hex856   DESTIN_hex863   DESTIN_hex864   DESTIN_hex865   DESTIN_hex866  \n     -0.280662       -1.368552       -2.614512        0.790513        0.167787  \n DESTIN_hex867   DESTIN_hex868   DESTIN_hex869   DESTIN_hex870   DESTIN_hex871  \n     -0.336224       -0.554061       -0.348172       -0.604915        0.993697  \n DESTIN_hex873   DESTIN_hex882   DESTIN_hex883   DESTIN_hex884   DESTIN_hex885  \n     -0.632120       -0.235719       -1.159151        0.275172       -1.141521  \n DESTIN_hex887   DESTIN_hex888   DESTIN_hex889   DESTIN_hex890   DESTIN_hex899  \n      1.474487        0.599324       -0.833864       -1.323280       -2.312346  \n DESTIN_hex901   DESTIN_hex902   DESTIN_hex903   DESTIN_hex905   DESTIN_hex906  \n     -0.709827       -0.295724        0.071707       -0.072823       -0.347068  \n DESTIN_hex907   DESTIN_hex909   DESTIN_hex910   DESTIN_hex917   DESTIN_hex920  \n      0.141356       -0.131289       -1.066922       -4.400286       -0.804284  \n DESTIN_hex922   DESTIN_hex925   DESTIN_hex926   DESTIN_hex927   DESTIN_hex928  \n      0.391778       -0.718079        0.809534       -0.993910       -1.335858  \n DESTIN_hex929   DESTIN_hex936   DESTIN_hex937   DESTIN_hex940   DESTIN_hex941  \n     -0.759869       -2.373193       -1.556308        0.610909        0.461015  \n DESTIN_hex944   DESTIN_hex945   DESTIN_hex946   DESTIN_hex947   DESTIN_hex949  \n     -0.624114        0.976707        0.253136       -1.554973       -0.792826  \n DESTIN_hex955   DESTIN_hex959   DESTIN_hex960   DESTIN_hex962   DESTIN_hex963  \n     -4.903935       -0.570538       -1.334225       -0.654570       -0.355887  \n DESTIN_hex964   DESTIN_hex965   DESTIN_hex966   DESTIN_hex968   DESTIN_hex969  \n     -0.120152        0.848161       -0.681642        0.066104       -1.941383  \n DESTIN_hex974   DESTIN_hex975   DESTIN_hex978   DESTIN_hex979   DESTIN_hex980  \n     -5.873880       -0.858957       -1.628532        0.691112       -1.087186  \n DESTIN_hex982   DESTIN_hex983   DESTIN_hex984   DESTIN_hex985   DESTIN_hex986  \n     -1.473992        1.132879        1.239657       -1.004800        0.116353  \n DESTIN_hex989   DESTIN_hex993   DESTIN_hex994   DESTIN_hex995   DESTIN_hex996  \n     -3.396900       -1.110005       -0.189623       -2.883963       -1.822898  \n DESTIN_hex998   DESTIN_hex999  DESTIN_hex1002  DESTIN_hex1004  DESTIN_hex1005  \n     -1.372265       -1.827336        0.279787       -0.772409        0.981871  \nDESTIN_hex1006  DESTIN_hex1008  DESTIN_hex1013  DESTIN_hex1015  DESTIN_hex1016  \n     -1.718996        1.109555        0.854025       -1.389096       -2.545126  \nDESTIN_hex1017  DESTIN_hex1018  DESTIN_hex1019  DESTIN_hex1023  DESTIN_hex1024  \n     -2.130731       -1.523610       -2.163008       -0.748781       -0.671741  \nDESTIN_hex1025  DESTIN_hex1026  DESTIN_hex1027  DESTIN_hex1028  DESTIN_hex1029  \n     -0.416575       -0.256674       -0.610806       -1.780683       -4.337019  \nDESTIN_hex1031  DESTIN_hex1032  DESTIN_hex1034  DESTIN_hex1035  DESTIN_hex1036  \n     -1.205166       -0.770476       -3.891320       -3.860456       -3.410768  \nDESTIN_hex1037  DESTIN_hex1038  DESTIN_hex1039  DESTIN_hex1044  DESTIN_hex1045  \n      0.147540        0.097448       -2.530230       -0.937574       -0.532820  \nDESTIN_hex1046  DESTIN_hex1047  DESTIN_hex1048  DESTIN_hex1051  DESTIN_hex1052  \n     -0.603058       -1.805604       -1.577971       -0.095131       -2.087544  \nDESTIN_hex1053  DESTIN_hex1056  DESTIN_hex1057  DESTIN_hex1058  DESTIN_hex1059  \n     -1.795931        1.236606       -2.003475       -1.769629       -0.956578  \nDESTIN_hex1063  DESTIN_hex1065  DESTIN_hex1067  DESTIN_hex1068  DESTIN_hex1069  \n     -4.213931       -2.148724        0.004580       -0.358382       -0.480451  \nDESTIN_hex1070  DESTIN_hex1071  DESTIN_hex1072  DESTIN_hex1073  DESTIN_hex1075  \n     -0.191333        0.063898       -0.170140       -1.741346       -4.276327  \nDESTIN_hex1076  DESTIN_hex1077  DESTIN_hex1078  DESTIN_hex1079  DESTIN_hex1083  \n     -0.923426       -1.069158       -2.101923       -3.185406       -0.008527  \nDESTIN_hex1085  DESTIN_hex1086  DESTIN_hex1088  DESTIN_hex1089  DESTIN_hex1090  \n      0.052267        0.444581       -0.187942       -1.635748       -1.031417  \nDESTIN_hex1091  DESTIN_hex1092  DESTIN_hex1093  DESTIN_hex1095  DESTIN_hex1096  \n      0.023734       -0.770180       -1.777684       -0.799744        1.152111  \nDESTIN_hex1097  DESTIN_hex1099  DESTIN_hex1103  DESTIN_hex1104  DESTIN_hex1106  \n     -0.859875       -1.451295        0.889314       -0.316063       -0.093124  \nDESTIN_hex1107  DESTIN_hex1108  DESTIN_hex1109  DESTIN_hex1110  DESTIN_hex1111  \n      0.429063       -0.916129       -0.698803       -0.474979       -2.196245  \nDESTIN_hex1112  DESTIN_hex1115  DESTIN_hex1116  DESTIN_hex1124  DESTIN_hex1125  \n     -0.906949       -1.464549       -0.354671        0.088051        0.307905  \nDESTIN_hex1126  DESTIN_hex1127  DESTIN_hex1128  DESTIN_hex1129  DESTIN_hex1130  \n      1.605676       -1.409942       -0.496332       -0.179479        0.380567  \nDESTIN_hex1131  DESTIN_hex1133  DESTIN_hex1134  DESTIN_hex1142  DESTIN_hex1143  \n      0.261169       -1.669553       -1.358062        0.753845       -0.055452  \nDESTIN_hex1144  DESTIN_hex1145  DESTIN_hex1146  DESTIN_hex1147  DESTIN_hex1148  \n     -0.450346        0.021174        1.395264       -1.389281        0.356906  \nDESTIN_hex1149  DESTIN_hex1152  DESTIN_hex1153  DESTIN_hex1161  DESTIN_hex1162  \n      0.414702       -2.108574       -2.779994       -0.688409        0.056049  \nDESTIN_hex1163  DESTIN_hex1164  DESTIN_hex1165  DESTIN_hex1166  DESTIN_hex1168  \n     -0.136670       -0.627496       -1.262064        1.111341        0.986058  \nDESTIN_hex1171  DESTIN_hex1178  DESTIN_hex1179  DESTIN_hex1180  DESTIN_hex1181  \n     -1.498131       -1.426528       -1.016944       -0.747723       -0.936741  \nDESTIN_hex1182  DESTIN_hex1183  DESTIN_hex1187  DESTIN_hex1194  DESTIN_hex1196  \n     -0.005293        0.481341       -2.250765        0.965399       -1.301274  \nDESTIN_hex1197  DESTIN_hex1198  DESTIN_hex1199  DESTIN_hex1200  DESTIN_hex1202  \n      0.355002       -0.259856       -2.080257       -0.504153       -1.699738  \nDESTIN_hex1203  DESTIN_hex1205  DESTIN_hex1212  DESTIN_hex1213  DESTIN_hex1214  \n     -1.555304       -4.145981        0.067330       -0.535484       -1.963146  \nDESTIN_hex1215  DESTIN_hex1216  DESTIN_hex1217  DESTIN_hex1218  DESTIN_hex1220  \n     -1.288826       -1.602329       -0.624388       -4.886168       -3.638423  \nDESTIN_hex1221  DESTIN_hex1226  DESTIN_hex1227  DESTIN_hex1228  DESTIN_hex1229  \n     -1.691882       -0.055065       -1.333599        0.496416        0.353840  \nDESTIN_hex1230  DESTIN_hex1231  DESTIN_hex1232  DESTIN_hex1233  DESTIN_hex1234  \n     -1.971482       -1.624366        0.224762        0.308868       -2.270214  \nDESTIN_hex1235  DESTIN_hex1236  DESTIN_hex1243  DESTIN_hex1244  DESTIN_hex1245  \n     -2.094781       -3.531048        0.654340        1.159343       -0.039985  \nDESTIN_hex1246  DESTIN_hex1247  DESTIN_hex1248  DESTIN_hex1249  DESTIN_hex1251  \n      0.114568       -0.686034       -0.498346       -1.207343       -2.468917  \nDESTIN_hex1252  DESTIN_hex1257  DESTIN_hex1258  DESTIN_hex1259  DESTIN_hex1260  \n     -2.942012       -0.408880        0.374564        0.024602       -1.646119  \nDESTIN_hex1261  DESTIN_hex1262  DESTIN_hex1263  DESTIN_hex1264  DESTIN_hex1265  \n     -1.841832       -0.875779        0.324804        0.035140       -1.172555  \nDESTIN_hex1266  DESTIN_hex1267  DESTIN_hex1272  DESTIN_hex1273  DESTIN_hex1274  \n     -0.377488       -2.643385       -0.687452        0.131725        0.833503  \nDESTIN_hex1275  DESTIN_hex1276  DESTIN_hex1277  DESTIN_hex1278  DESTIN_hex1279  \n     -2.034147        1.385630       -1.487058        0.294141       -2.805710  \nDESTIN_hex1280  DESTIN_hex1281  DESTIN_hex1285  DESTIN_hex1286  DESTIN_hex1287  \n     -1.829471       -3.185007       -2.427344        0.126983       -0.403375  \nDESTIN_hex1288  DESTIN_hex1289  DESTIN_hex1290  DESTIN_hex1291  DESTIN_hex1292  \n      0.376118       -0.835152       -1.311672       -0.868330       -1.352454  \nDESTIN_hex1293  DESTIN_hex1299  DESTIN_hex1300  DESTIN_hex1301  DESTIN_hex1302  \n     -1.502707       -0.888111        0.431569        0.511868        0.050869  \nDESTIN_hex1303  DESTIN_hex1305  DESTIN_hex1306  DESTIN_hex1307  DESTIN_hex1312  \n     -1.259388       -0.323203       -0.886757       -2.916696       -0.097902  \nDESTIN_hex1313  DESTIN_hex1314  DESTIN_hex1315  DESTIN_hex1316  DESTIN_hex1317  \n      0.457769       -0.510225        0.177524       -2.274774        1.017828  \nDESTIN_hex1318  DESTIN_hex1319  DESTIN_hex1320  DESTIN_hex1326  DESTIN_hex1327  \n     -1.531133       -0.612148       -1.618577       -1.072228        0.531660  \nDESTIN_hex1328  DESTIN_hex1329  DESTIN_hex1330  DESTIN_hex1331  DESTIN_hex1332  \n      0.354887        0.137880       -1.432078       -0.440864       -2.298769  \nDESTIN_hex1333  DESTIN_hex1336  DESTIN_hex1337  DESTIN_hex1338  DESTIN_hex1339  \n     -2.313755       -0.593614       -0.611424        0.470173        1.209330  \nDESTIN_hex1340  DESTIN_hex1341  DESTIN_hex1342  DESTIN_hex1343  DESTIN_hex1344  \n     -1.194138       -0.455336        0.847071        0.289346       -0.026378  \nDESTIN_hex1345  DESTIN_hex1348  DESTIN_hex1349  DESTIN_hex1350  DESTIN_hex1351  \n     -1.873380       -0.311508        0.969602        1.053143       -0.117189  \nDESTIN_hex1352  DESTIN_hex1353  DESTIN_hex1354  DESTIN_hex1355  DESTIN_hex1356  \n      0.464980       -0.445511       -1.260269       -1.548138       -1.376470  \nDESTIN_hex1357  DESTIN_hex1359  DESTIN_hex1360  DESTIN_hex1362  DESTIN_hex1363  \n     -2.700981       -1.702041       -0.762592        1.075248        1.190088  \nDESTIN_hex1365  DESTIN_hex1366  DESTIN_hex1367  DESTIN_hex1368  DESTIN_hex1369  \n     -1.870546       -1.757798        0.161190       -0.963849       -2.805688  \nDESTIN_hex1370  DESTIN_hex1371  DESTIN_hex1372  DESTIN_hex1373  DESTIN_hex1374  \n      0.520915        0.797673        0.686915       -0.108649       -0.961940  \nDESTIN_hex1375  DESTIN_hex1376  DESTIN_hex1377  DESTIN_hex1378  DESTIN_hex1379  \n     -2.904064       -2.120654       -1.436504       -1.136107       -3.279069  \nDESTIN_hex1382  DESTIN_hex1383  DESTIN_hex1384  DESTIN_hex1388  DESTIN_hex1389  \n     -0.406285        0.564112        0.551784       -2.267988        0.236197  \nDESTIN_hex1390  DESTIN_hex1391  DESTIN_hex1392  DESTIN_hex1393  DESTIN_hex1394  \n      0.315804       -1.922500        1.003571       -0.432362       -3.103383  \nDESTIN_hex1395  DESTIN_hex1397  DESTIN_hex1399  DESTIN_hex1400  DESTIN_hex1401  \n     -1.709672       -0.003143       -1.959478       -2.042575       -2.545437  \nDESTIN_hex1402  DESTIN_hex1404  DESTIN_hex1405  DESTIN_hex1406  DESTIN_hex1409  \n     -1.658344       -1.213940        0.385384        0.861235        0.628025  \nDESTIN_hex1410  DESTIN_hex1411  DESTIN_hex1412  DESTIN_hex1413  DESTIN_hex1414  \n     -3.124612       -0.747080       -1.772746       -5.735420       -0.260439  \nDESTIN_hex1415  DESTIN_hex1416  DESTIN_hex1417  DESTIN_hex1420  DESTIN_hex1422  \n     -2.217155       -0.370310       -0.325202       -3.600508       -2.203593  \nDESTIN_hex1426  DESTIN_hex1427  DESTIN_hex1428  DESTIN_hex1429  DESTIN_hex1432  \n     -0.538531        0.192950        0.113722       -1.402282       -0.660051  \nDESTIN_hex1433  DESTIN_hex1434  DESTIN_hex1437  DESTIN_hex1438  DESTIN_hex1439  \n     -2.578219       -2.032080       -0.227039       -0.192414       -0.175418  \nDESTIN_hex1442  DESTIN_hex1444  DESTIN_hex1446  DESTIN_hex1447  DESTIN_hex1448  \n      0.218280       -2.113933       -0.444001        0.858567       -0.618548  \nDESTIN_hex1450  DESTIN_hex1451  DESTIN_hex1456  DESTIN_hex1457  DESTIN_hex1458  \n     -3.167342       -2.084260       -1.004591        1.083493       -1.926783  \nDESTIN_hex1459  DESTIN_hex1460  DESTIN_hex1461  DESTIN_hex1465  DESTIN_hex1466  \n     -0.096277       -1.189633        0.765159       -0.843354        0.679310  \nDESTIN_hex1467  DESTIN_hex1468  DESTIN_hex1469  DESTIN_hex1470  DESTIN_hex1471  \n      0.346611        0.862811       -0.492275       -0.297030        0.758185  \nDESTIN_hex1475  DESTIN_hex1476  DESTIN_hex1477  DESTIN_hex1478  DESTIN_hex1479  \n     -0.680713       -0.317021       -1.136572       -0.422127       -0.768994  \nDESTIN_hex1480  DESTIN_hex1481  DESTIN_hex1484  DESTIN_hex1485  DESTIN_hex1486  \n     -0.810611       -2.587971       -0.934893       -0.617190        0.708782  \nDESTIN_hex1487  DESTIN_hex1488  DESTIN_hex1489  DESTIN_hex1491  DESTIN_hex1492  \n     -0.945054       -1.931375       -0.546072       -1.396215       -0.393619  \nDESTIN_hex1493  DESTIN_hex1494  DESTIN_hex1495  DESTIN_hex1496  DESTIN_hex1497  \n     -1.127898        1.858745       -1.147087       -0.745489       -1.659461  \nDESTIN_hex1499  DESTIN_hex1500  DESTIN_hex1501  DESTIN_hex1502  DESTIN_hex1503  \n     -1.121160       -1.508867       -0.902250        0.640477       -2.739814  \nDESTIN_hex1504  DESTIN_hex1506  DESTIN_hex1507  DESTIN_hex1508  DESTIN_hex1509  \n     -0.381068       -1.048943        1.228320        0.876308       -1.292845  \nDESTIN_hex1510  DESTIN_hex1511  DESTIN_hex1513  DESTIN_hex1514  DESTIN_hex1515  \n     -1.298504        1.667095       -0.823532       -1.033630        1.040498  \nDESTIN_hex1516  DESTIN_hex1517  DESTIN_hex1518  DESTIN_hex1522  DESTIN_hex1523  \n      0.032889        0.791694       -1.338887       -1.826134       -0.491926  \nDESTIN_hex1524  DESTIN_hex1525  DESTIN_hex1528  DESTIN_hex1529  DESTIN_hex1530  \n     -1.145420       -0.654614        0.180800       -0.962139       -0.101159  \nDESTIN_hex1531  DESTIN_hex1532  DESTIN_hex1534  DESTIN_hex1535  DESTIN_hex1536  \n     -0.328821       -1.498652       -1.381550        0.627064       -0.502396  \nDESTIN_hex1537  DESTIN_hex1538  DESTIN_hex1540  DESTIN_hex1541  DESTIN_hex1542  \n     -0.831358       -0.825101        1.111700       -0.708465       -0.341168  \nDESTIN_hex1543  DESTIN_hex1544  DESTIN_hex1546  DESTIN_hex1547  DESTIN_hex1548  \n     -0.606940       -2.000755        1.147585       -1.858561        0.706459  \nDESTIN_hex1549  DESTIN_hex1550  DESTIN_hex1551  DESTIN_hex1553  DESTIN_hex1554  \n     -1.935275        0.256976       -0.912716        1.359094        0.975123  \nDESTIN_hex1555  DESTIN_hex1556  DESTIN_hex1557  DESTIN_hex1563  DESTIN_hex1564  \n     -0.974662       -1.431716        0.907818       -1.635899       -0.909504  \nDESTIN_hex1565  DESTIN_hex1570  DESTIN_hex1571  DESTIN_hex1572  DESTIN_hex1575  \n     -0.350175       -0.187507       -1.566796       -1.021091        0.949570  \nDESTIN_hex1579  DESTIN_hex1583  DESTIN_hex1584  DESTIN_hex1587  DESTIN_hex1588  \n     -0.755174       -1.509938        1.255164       -0.149198        0.490300  \nDESTIN_hex1592  DESTIN_hex1594  DESTIN_hex1602  DESTIN_hex1603  DESTIN_hex1608  \n      1.482876       -0.729626       -2.850976       -1.101575        1.970379  \nDESTIN_hex1609  DESTIN_hex1616  DESTIN_hex1623  DESTIN_hex1630  DESTIN_hex1643  \n      1.545360        0.222512        1.862799       -1.140616       -2.955193  \nDESTIN_hex1644  DESTIN_hex1665            dist  \n      1.474527        0.574490       -1.719430  \n\nDegrees of Freedom: 51925 Total (i.e. Null);  50333 Residual\nNull Deviance:      73540000 \nResidual Deviance: 19840000     AIC: 20110000\n\n\nNow that we have fit three different spatial interaction models, we will now save the fitted values resulted from each model into a new dataset called fitted_flow_weekday_morn. These fitted values represent the estimated flows from each model.\n\norcSIM_fitted &lt;- as.data.frame(orcSIM_weekday_morn$fitted.values)\ncolnames(orcSIM_fitted) &lt;- \"ORCEstimatedFlow\"\nfitted_flow_weekday_morn &lt;- cbind(flow_data_weekday_morn,orcSIM_fitted)\n\ndesSIM_fitted &lt;- as.data.frame(desSIM_weekday_morn$fitted.values)\ncolnames(desSIM_fitted) &lt;- \"DESEstimatedFlow\"\nfitted_flow_weekday_morn &lt;- cbind(fitted_flow_weekday_morn,desSIM_fitted)\n\ndbcSIM_fitted &lt;- as.data.frame(dbcSIM_weekday_morn$fitted.values)\ncolnames(dbcSIM_fitted) &lt;- \"DBCEstimatedFlow\"\nfitted_flow_weekday_morn &lt;- cbind(fitted_flow_weekday_morn,dbcSIM_fitted)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#doubly-constraint-flow-estimation-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#doubly-constraint-flow-estimation-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.12 Doubly Constraint Flow Estimation Map",
    "text": "4.12 Doubly Constraint Flow Estimation Map\nSimilar to what we did with origin constrained SIM model, we will follow the same procedure for plotting doubly constrained SIM model results.\n\ndbc_flowLine &lt;- flowLine %&gt;% filter(DBCEstimatedFlow &gt; 5000)\n\n\ndbc_fitted_inflow &lt;- aggregate(fitted_flow_weekday_morn$DBCEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$DESTIN_hex), FUN=sum)\ncolnames(dbc_fitted_inflow) &lt;- c(\"index\", \"Estimated_InFlow\")\ndbc_fitted_inflow_hex &lt;- left_join(hex_grid_pa_sz, orc_fitted_inflow, by = 'index')\ndbc_fitted_inflow_hex$Estimated_InFlow &lt;- ifelse(is.na(dbc_fitted_inflow_hex$Estimated_InFlow), 0, dbc_fitted_inflow_hex$Estimated_InFlow)\n\ndbc_fitted_outflow &lt;- aggregate(fitted_flow_weekday_morn$DBCEstimatedFlow, by=list(Category=fitted_flow_weekday_morn$ORIGIN_hex), FUN=sum)\ncolnames(dbc_fitted_outflow) &lt;- c(\"index\", \"Estimated_OutFlow\")\ndbc_fitted_outflow_hex &lt;- left_join(hex_grid_pa_sz, orc_fitted_outflow, by = 'index')\ndbc_fitted_outflow_hex$Estimated_OutFlow &lt;- ifelse(is.na(dbc_fitted_outflow_hex$Estimated_OutFlow), 0, dbc_fitted_outflow_hex$Estimated_OutFlow)\n\n\ntmap_mode(\"view\")\ntm_shape(hex_grid_pa_sz) +\n  tm_fill(col=\"#f2ffff\",\n          id = \"PLN_AREA_N\") +\n  tm_borders(col = \"grey\") +\ntm_shape(dbc_fitted_inflow_hex) +\n  tm_fill(col = \"Estimated_InFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Inflow Volume\",\n          id = \"Estimated_InFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(dbc_fitted_outflow_hex) +\n  tm_fill(col = \"Estimated_OutFlow\",\n          palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"fixed\",\n          breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n          title = \"Estimated Outflow Volume\",\n          id = \"Estimated_OutFlow\")+\n  tm_borders(col = \"grey\") +\ntm_shape(orc_flowLine) +\n  tm_lines(lwd = \"DBCEstimatedFlow\",\n           col = \"DBCEstimatedFlow\",\n           palette = c(\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n          style = \"pretty\",\n          scale = c(1,2,3,4,5,7,9),\n          n = 6,\n          title.lwd = \"Destination Constrained Flow\",\n          id = \"DBCEstimatedFlow\")\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nDuring my prototyping process, I spent some time to think about the organization of the models. Initially, I considered having a separate tabset for each model type. However, upon reflection, I believe it might be more beneficial to have the model type as one of the calibration inputs and combine everything on one page. The advantage of this approach is that it allows users to stack layers not just from one model, but across different models as well. For instance, they could stack desire flows from all three models and analyze the differences in the results. This would be challenging if we have each tabset for each model than the user will have to keep swtiching from one tab to another just to compare the results."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-to-specific-planning-area-subzones",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#zooming-to-specific-planning-area-subzones",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.13 Zooming to Specific Planning Area & Subzones",
    "text": "4.13 Zooming to Specific Planning Area & Subzones\nSimilar to what we did in earlier sections, we can also create planning area and subzone specific maps. In the following example, I tried to prototype using destination constrained SIM model.\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nFor the purpose of prototyping, I’ve been using the destination constrained Spatial Interaction Model (SIM). However, in the actual Shiny application, I plan to give users the flexibility to choose both the model type and the planning/area or subzone they want to use for their analysis.\nAdditionally, I’m considering allowing users to specify the time period they’re interested in - whether it’s the morning or evening peak, or weekday or weekend/holiday. However, in this exercise we are only using weekday morning dataset, it is not possible to do it here. But the actual application will be designed to handle different time periods.\n\n\n\ndes_flowLine_map_pa &lt;- function(pa_input){\n  des_flowLine_temp &lt;- flowLine %&gt;% filter(PLN_AREA_N == pa_input, DBCEstimatedFlow &gt; 1000)\n  des_fitted_inflow_hex &lt;- des_fitted_inflow_hex %&gt;% filter(PLN_AREA_N == pa_input)\n  des_fitted_outflow_hex &lt;- des_fitted_outflow_hex %&gt;% filter(PLN_AREA_N == pa_input)\n\n  \n  tmap_mode(\"view\")\n  tm_shape(hex_grid_pa_sz) +\n    tm_fill(col=\"#f2ffff\") +\n    tm_borders(col = \"grey\") +\n  tm_shape(des_fitted_inflow_hex) +\n    tm_fill(col = \"Estimated_InFlow\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"fixed\",\n            breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n            title = \"Estimated Inflow Volume\",\n          id = \"Estimated_InFlow\")+\n    tm_borders(col = \"grey\") +\n  tm_shape(des_fitted_outflow_hex) +\n    tm_fill(col = \"Estimated_OutFlow\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"fixed\",\n            breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n            title = \"Estimated Outflow Volume\",\n          id = \"Estimated_OutFlow\")+\n    tm_borders(col = \"grey\") +\n  tm_shape(des_flowLine_temp) +\n    tm_lines(lwd = \"DESEstimatedFlow\",\n             col = \"DESEstimatedFlow\",\n             palette = c(\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"pretty\",\n            scale = c(1,2,3,4,5,7,9),\n            n = 6,\n            title.lwd = \"Destination Constrained Flow\",\n            id = \"DESEstimatedFlow\")\n}\n\n\ndes_flowLine_map_sz &lt;- function(sz_input){\n  des_flowLine_temp &lt;- flowLine %&gt;% filter(SUBZONE_N == sz_input, DBCEstimatedFlow &gt; 1000)\n  des_fitted_inflow_hex &lt;- des_fitted_inflow_hex %&gt;% filter(SUBZONE_N == sz_input)\n  des_fitted_outflow_hex &lt;- des_fitted_outflow_hex %&gt;% filter(SUBZONE_N == sz_input)\n  \n  tmap_mode(\"view\")\n  tm_shape(hex_grid_pa_sz) +\n    tm_fill(col=\"#f2ffff\") +\n    tm_borders(col = \"grey\") +\n  tm_shape(des_fitted_inflow_hex) +\n    tm_fill(col = \"Estimated_InFlow\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"fixed\",\n            breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n            title = \"Estimated Inflow Volume\",\n          id = \"Estimated_InFlow\")+\n    tm_borders(col = \"grey\") +\n  tm_shape(des_fitted_outflow_hex) +\n    tm_fill(col = \"Estimated_OutFlow\",\n            palette = c(\"#f2ffff\",\"#f9f777\",\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"fixed\",\n            breaks = c(0,1,100,1000,10000,100000,500000,1000000,5000000),\n            title = \"Estimated Outflow Volume\",\n          id = \"Estimated_OutFlow\")+\n    tm_borders(col = \"grey\") +\n  tm_shape(des_flowLine_temp) +\n    tm_lines(lwd = \"DESEstimatedFlow\",\n             col = \"DESEstimatedFlow\",\n             palette = c(\"#f8d673\",\"#f89974\",\"#D66779\",\"#b977cb\",\"#7977f3\",\"#57bfc0\"),\n            style = \"pretty\",\n            scale = c(1,2,3,4,5,7,9),\n            n = 6,\n            title.lwd = \"Destination Constrained Flow\",\n            id = \"DESEstimatedFlow\")\n}\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nDuring my prototyping process, I’ve made an adjustment to the filtering based on flow volume. Initially, I was filtering out data with a flow volume less than 5000. However, I realized that some subzones and planning areas might have a lower flow volume, which could lead to errors if the maximum flow is less than this threshold filter.\nTo avoid such errors, I decided to reduce the threshold to 1000. This allows for a more inclusive analysis, capturing data from areas with lower flow volumes. However, I’m aware that there could still be instances where the flow is even less than 1000, which could still potentially lead to errors.\nWhile I haven’t addressed this issue in the current stage, I’m actively thinking about ways to handle this in the final Shiny app.\n\n\nAs a demonstration, we call the des_flowLine_map_pa() function with “WOODLANDS” and the des_flowLine_map_sz() function with “NATURE RESERVE” as inputs. to test whether the functions works well and produce desired outcomes.\n\ndes_flowLine_map_pa(\"WOODLANDS\")\n\n\n\n\n\ndes_flowLine_map_sz(\"NATURE RESERVE\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.14 Push-Pull Factors",
    "text": "4.14 Push-Pull Factors\nBeyond looking at the estimated flow volumes, spatial interaction modelling also offers valuable insights by providing coefficient estimates for explanatory variables, both at the origin and destination locations.\n\nIn particular, the origin-constrained SIM is instrumental in identifying the ‘pull’ factors of the destination. These factors represent conditions or circumstances that draw people from multiple origins towards the destination.\nConversely, the destination-constrained SIM sheds light on the ‘push’ factors at the origin. These factors represent conditions or circumstances that encourage people to leave the origin and move to multiple destinations.\n\nLooking at the coefficient estimates of these push-pull factors, we can find correlation between the flow volume and the increase or decrease in the value of these factors.\nTo do visualisations and analysis with push-pull factors, we will first extract fitted coefficient values from origin-constrained and destination-constrained SIM models first.\n\npull_factors &lt;- tibble::rownames_to_column(as.data.frame(orcSIM_weekday_morn$coefficients), \"factors\")\ncolnames(pull_factors) &lt;- c(\"factors\", \"coefficients\")\npull_factors &lt;- pull_factors[grep(\"^(d_)\", pull_factors$factors), ]\n\n\npush_factors &lt;- tibble::rownames_to_column(as.data.frame(desSIM_weekday_morn$coefficients), \"factors\")\ncolnames(push_factors) &lt;- c(\"factors\", \"coefficients\")\npush_factors &lt;- push_factors[grep(\"^(o_)\", push_factors$factors), ]\n\nLet’s look at the data a bit.\n\npull_factors$factors\n\n [1] \"d_biz_count\"           \"d_school_count\"        \"d_fin_count\"          \n [4] \"d_hc_count\"            \"d_busstop_count\"       \"d_housing_count\"      \n [7] \"d_leisure_recre_count\" \"d_retail_count\"        \"d_entertn_count\"      \n[10] \"d_food_bev_count\"     \n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nDuring my prototyping process, I’ve come to realize the importance of clarity and interpretability. It’s not just about presenting the data, but also about making it understandable and meaningful to the user.\nWhile I understand what each of these factor values means, it may not be as clear for everyone else. Especially without the context of the data, terms like d_hc_count can be confusing - and we cannot assume that they will know hc refers to healthcare!\nWith this in mind, I’ve decided to update these values to be more explicit and straightforward. Instead of using abbreviations, I’ll use clear, descriptive names that accurately represent what each value stands for. This way, users can easily understand what they’re looking at, making the application more user-friendly and informative.\n\n\nWe will now update the factors values in pull_factors dataset from abbreviations to long form as below.\n\npull_factors$factors &lt;- c(\"Business\", \"School\", \"Financial Institute\",\"Healthcare\",\"Bus Stop\", \"Housing\", \"Leisure/Recreation\", \"Retail\", \"Entertainment\", \"Food & Beverages\")\n\n\npush_factors$factors\n\n [1] \"o_biz_count\"           \"o_school_count\"        \"o_fin_count\"          \n [4] \"o_hc_count\"            \"o_busstop_count\"       \"o_housing_count\"      \n [7] \"o_leisure_recre_count\" \"o_retail_count\"        \"o_entertn_count\"      \n[10] \"o_food_bev_count\"     \n\n\nWe will do the same for factors values in push_factors dataset.\n\npush_factors$factors &lt;- c(\"Business\", \"School\", \"Financial Institute\",\"Healthcare\",\"Bus Stop\", \"Housing\", \"Leisure/Recreation\", \"Retail\", \"Entertainment\", \"Food & Beverages\")\n\nNow that we have prepared all the data, we will plot bar graphs using relevant ggplot2 functions.\n\nggplot(data = pull_factors,\n       aes(x = coefficients,\n           y = reorder(factors, coefficients),\n       fill = factors)) +\n  geom_bar(stat=\"identity\", show.legend = FALSE) +\n  xlab(\"Coefficient Estimate\") +\n  ylab(\"Pull Factors of Destination\")\n\n\n\n\n\nggplot(data = push_factors,\n       aes(x = coefficients,\n           y = reorder(factors, coefficients),\n       fill = factors)) +\n  geom_bar(stat=\"identity\", show.legend = FALSE) +\n  xlab(\"Coefficient Estimate\") +\n  ylab(\"Push Factors of Origin\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nVisualizing the coefficient estimates using bar graphs, instead of simple tables, indeed enhances interpretability. It provides a clear picture of not only the magnitude (absolute value) but also the direction (positive or negative) of each coefficient estimate.\nFor example, in push factor graph above, we can clearly see that an increase in business establishments corresponds to a decrease in pushing people away, indicating that areas with more businesses tend to attract more people. On the other hand, an increase in financial institutes corresponds to an increase in pushing people away, suggesting that areas with more financial institutes might be less attractive for some reason.\nThis kind of visual representation makes it much easier to understand and interpret the results of the analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-user-input-options-for-shiny-application-1",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-user-input-options-for-shiny-application-1",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.15 Exploring User Input Options for Shiny Application",
    "text": "4.15 Exploring User Input Options for Shiny Application\nReflection on what we explored in the exercise above, we will use two tabset - Flow Estimation Map and Push-Pull Factors to visualise the results.\n\n4.15.1 Flow Estimation Map\nFor Flow Estimation Map tabset, we may include the following user specification and model calibration options for our Shiny application.\n\n\n\n\n\n\n\nInput\nOptions\n\n\n\n\nModel Type\nOrigin-Constrained, Destination-Constrained, Doubly Constrained\n\n\nTime Period\nWeekday - Morning , Weekday - Evening, Weekend/Holiday - Morning, Weekend/Holiday - Evening\n\n\nResult Layer\nO-D Desire Line, Fitted Inflow Map, Fitted Outflow Map\n\n\nZooming Level\nOverall (Singapore) , Planning Area, Subzone\n\n\nArea of Interest\n\nNA if Overall (Singapore) is selected for Zooming Level\nPlanning Area Name if Planning Area is selected for Zooming Level\nSubzone Name if Subzone is selected for Zooming Level\n\n\n\n\nBelow is a rough outline of how the user interface for this tabset might look like.\n\n\n\n\n\n\n\n4.15.1 Push-Pull Factors\nFor Push-Pull Factors tabset, we may include the following user specification and model calibration options for our Shiny application. Note that we remove the specification for Model Type as push-pull factors cannot be derived from a single SIM model.\n\n\n\n\n\n\n\nInput\nOptions\n\n\n\n\nTime Period\nWeekday - Morning , Weekday - Evening, Weekend/Holiday - Morning, Weekend/Holiday - Evening\n\n\nZooming Level\nOverall (Singapore) , Planning Area, Subzone\n\n\nArea of Interest\n\nNA if Overall (Singapore) is selected for Zooming Level\nPlanning Area Name if Planning Area is selected for Zooming Level\nSubzone Name if Subzone is selected for Zooming Level\n\n\n\n\nBelow is a rough outline of how the user interface for this tabset might look like."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#inflowoutflow-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#inflowoutflow-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "5.1 Inflow/Outflow Map",
    "text": "5.1 Inflow/Outflow Map\n\n5.1.1 Inflow/Outflow Map for Singapore Overall\n\n\n\nInflow/Outflow Map - (Flow Type) Incoming Flow, (Time Period) Weekday - Morning, (Zooming Level) - Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nInflow/Outflow Map - (Flow Type) Outgoing Flow, (Time Period) Weekday - Morning, (Zooming Level) - Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.1.2 Inflow/Outflow Map for Planning Area/Subzone Zooming\n\n\n\nInflow/Outflow Map - (Flow Type) Incoming Flow, (Time Period) Weekday - Morning, (Zooming Level) - Planning Area, (Area of Interest) Bukit Panjang\n\n\n\n\n\nInflow/Outflow Map - (Flow Type) Incoming Flow, (Time Period) Weekday - Morning, (Zooming Level) - Subzone, (Area of Interest) Changi Airport\n\n\n\n\n\nInflow/Outflow Map - (Flow Type) Outgoing Flow, (Time Period) Weekday - Morning, (Zooming Level) - Subzone, (Area of Interest) Tuas North"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#flow-estimation-map",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#flow-estimation-map",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "5.2 Flow Estimation Map",
    "text": "5.2 Flow Estimation Map\n\n5.2.1 Origin-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.2 Destination-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.3 Doubly-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.4 Flow Estimation Maps for Planning Area/Subzone Zooming\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map, Fitted Outflow Map], (Zooming Level) Planning Area, (Area of Interest) Woodlands\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Inflow Map], (Zooming Level) Planning Area, (Area of Interest) Woodlands\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Subzone, (Area of Interest) Tuas North"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors-1",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors-1",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "5.3 Push-Pull Factors",
    "text": "5.3 Push-Pull Factors\n\n\n\nPush-Pull Factors - (Time Period) Weekday - Morning, (Zooming Level) Overall Singapore, (Area of Interest) NA"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparing-estimated-flows-from-different-models",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#comparing-estimated-flows-from-different-models",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "4.16 Comparing Estimated Flows from Different Models",
    "text": "4.16 Comparing Estimated Flows from Different Models\nIn this section, we are comparing the performance of the three Spatial Interaction Models (SIMs) we have fitted above: the origin constrained SIM, the destination constrained SIM, and the doubly constrained SIM. To achieve it, we will explore an R packaged called performance which provides functions for assessment of model quality and performance.\n\npacman::p_load(performance)\n\nFirstly, we will a list of the three models we have fitted: the origin constrained SIM (orcSIM_weekday_morn), the destination constrained SIM (desSIM_weekday_morn), and the doubly constrained SIM (dbcSIM_weekday_morn).\nWe will then use the compare_performance function to compare the models based on the RMSE. The RMSE is a measure of the differences between the values predicted by a model and the values actually observed.\n\nmodel_list &lt;- list(originConstrained=orcSIM_weekday_morn,\n                   destinConstrained=desSIM_weekday_morn,\n                   doublyConstrained=dbcSIM_weekday_morn)\n\ncompare_performance(model_list, metrics = \"RMSE\")\n\n# Comparison of Model Performance Indices\n\nName              | Model |     RMSE\n------------------------------------\noriginConstrained |   glm | 1498.442\ndestinConstrained |   glm | 1468.283\ndoublyConstrained |   glm | 1137.973\n\n\nNext, we will create scatter plots to compare the observed flows against the estimated flows from each model.\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = ORCEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Origin Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")\n\n\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = DESEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Destination Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")\n\n\n\nggplot(data = fitted_flow_weekday_morn,\n                aes(x = DBCEstimatedFlow,\n                    y = TOTAL_TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  coord_cartesian(xlim=c(0,100000),\n                  ylim=c(0,100000)) + \n  labs(title = \"Observed vs. Fitted Values for Doubly Constrained SIM\",\n       x = \"Fitted Values\", y = \"Observed Values\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nPrototyping Thoughts\nAs I’ve been working on the prototyping process, I’ve had some thoughts on whether or not to add the section on model comparison. While observed vs fitted value plots are essential in research for validating models, I realized they might not be as relevant or intuitive for everyone when used in a geospatial application. Given that the users of our Shiny application may not have a research background, such plots could potentially confuse them. Therefore, I’ve decided to focus on features that enhance the interpretability and usability of the application."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#flow-estimation-map-1",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#flow-estimation-map-1",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "5.2 Flow Estimation Map",
    "text": "5.2 Flow Estimation Map\n\n5.2.1 Origin-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Origin-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.2 Destination-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.3 Doubly-Constrained Flow Estimation Maps\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n\nFlow Estimation Map - (Model Type) Doubly-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Outflow Map], (Zooming Level) Overall Singapore, (Area of Interest) NA\n\n\n\n\n5.2.4 Flow Estimation Maps for Planning Area/Subzone Zooming\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line, Fitted Inflow Map, Fitted Outflow Map], (Zooming Level) Planning Area, (Area of Interest) Woodlands\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [Fitted Inflow Map], (Zooming Level) Planning Area, (Area of Interest) Woodlands\n\n\n\n\n\nFlow Estimation Map - (Model Type) Destination-Constrained, (Time Period) Weekday - Morning, (Result Layer) [O-D Desire Line], (Zooming Level) Subzone, (Area of Interest) Tuas North"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors-2",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#push-pull-factors-2",
    "title": "Take-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "5.3 Push-Pull Factors",
    "text": "5.3 Push-Pull Factors\n\n\n\nPush-Pull Factors - (Time Period) Weekday - Morning, (Zooming Level) Overall Singapore, (Area of Interest) NA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10.html",
    "href": "In-class_Ex/In-class_Ex10.html",
    "title": "In-Class Exercise 10",
    "section": "",
    "text": "Modelling Geographical Accessibility"
  }
]