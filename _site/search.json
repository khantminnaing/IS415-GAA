[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHands-On Exercise 01\n\n\n\n\n\nGeospatial Data Wrangling with R!\n\n\n\n\n\n\nJan 7, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 02\n\n\n\n\n\nThematic Mapping and GeoVisualisation with R\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 03\n\n\n\n\n\nSpatial Point Pattern Analysis\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 08\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nJan 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exercise 08\n\n\n\n\n\nGeographically Weighted Regression\n\n\n\n\n\n\nJan 10, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nIn-Class Exercise 02\n\n\n\n\n\nR for Geospatial Data Science\n\n\n\n\n\n\nJan 12, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\n  \n\n\n\n\nTake-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore\n\n\n\n\n\nApplication of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore, implemented using spatstat package (Baddeley, Turner, and Rubak 2022) in R environment.\n\n\n\n\n\n\nJan 15, 2024\n\n\nKhant Min Naing\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html",
    "href": "Hands-on_Ex/hands_on02.html",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#overview",
    "href": "Hands-on_Ex/hands_on02.html#overview",
    "title": "Hands-On Exercise 02",
    "section": "",
    "text": "The main learning topics of today’s hands-on exercise are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package.\nThe output of this exercise is to create maps like this."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-packages",
    "href": "Hands-on_Ex/hands_on02.html#importing-packages",
    "title": "Hands-On Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nSince readr, tidyr and dplyr are part of tidyverse package, we will only need to install and import tidyverse.\n\npacman::p_load(sf, tmap, tidyverse)\ndevtools::install_github(\"thomasp85/patchwork\")\n\nSkipping install of 'patchwork' from a github remote, the SHA1 (d9437579) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(patchwork)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "href": "Hands-on_Ex/hands_on02.html#importing-datasets-into-r-environment",
    "title": "Hands-On Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use two datasets as follows:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\nFor geospatial data, we will use st_read() function of sf package to import shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"~/IS415-GAA/data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/hands_on02.html#data-preparation-and-wrangling",
    "title": "Hands-On Exercise 02",
    "section": "4.0 Data Preparation and Wrangling",
    "text": "4.0 Data Preparation and Wrangling\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n4.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n4.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nHence, we will standard the data values in these two fields.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/hands_on02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 02",
    "section": "5.0 Choropleth Mapping Geospatial Data Using tmap",
    "text": "5.0 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n5.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\",\n    fill.palette =\"plasma\")\n\n\n\n\n\n\n5.2 Plotting a choropleth map quickly by using qtm()\nHowever, in real-life application, the quick choropleth map produced in the previous session may not be sufficient enough to properly visualize geospatial data. However, tmap packages allow us to customise and control how we design our choropleth maps. We will exploit tmap’s drawing elements to create a high quality cartographic choropleth map that includes more accurate and informative information.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nNext, we will breakdown the different tmpa functions used to plot the additional elements in the map above.\n\n\n5.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e.DEPENDENCY)\n\n\n\n\n\n5.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n5.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/hands_on02.html#data-classification-methods-of-tmap",
    "title": "Hands-On Exercise 02",
    "section": "6.0 Data Classification Methods of tmap",
    "text": "6.0 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher, and\njenks.\n\n\n6.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNext, we will try equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nNext, we will try other data classification methods.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\nAlso, we can try exploring using the same classification methods, but with different numbers of classes. As an example, we will use kmeans clustering method with different class sizes (2,6,10,20)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          palette = \"plasma\",\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\noriginal &lt;- ggplot(data=mpsz_pop2020, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\n#Try to remove outliers\nmpsz_pop2020_no_outlier &lt;- subset(mpsz_pop2020, mpsz_pop2020$DEPENDENCY &lt;3)\n\nfiltered &lt;- ggplot(data=mpsz_pop2020_no_outlier, aes(x=`DEPENDENCY`)) +\n  geom_bar(color=\"black\", fill=\"#e9531e\")+\n  scale_x_binned(n.breaks=10)\n\noriginal + filtered\n\n\n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 1.00. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\n\n\n\n\n\nReflection\n\n\n\nWhy do we use the above-mentioned breaks?\nThe reason behind choosing those break points is mainly stemmed from the 1st quantile and 3rd quantile of the datasets. While the minimum value is 0.10 and maximum value is 19.0, the 1st quantile (the value under which 25% of data points are found) is 0.7147 and the 3rd quantile (the value under which 75% of data points are found) is 0.8763. Using these two values, we may assume that the dataset might have outliers on the right end, and the majority of the dataset might be scattered in the range of 0.7147 and 0.8763. Hence, we use the mentioned break points.\nOtherwise, we can use non-heuristic approach in this case as well. We can easily plot the data to see the distribution first (see above). As we assumed earlier, you can clearly see the outliers on the right-side of the histogram. After removing the outliers (temporarily), we can see the new plot (see above). Majority of the datasets are scattered within the range of 0.6 - 1.0. This is why we break the datasets into 0.6, 0.7, 0.8 and 0.9 respectively so that there is balanced quantity of data points in each break.\n\n\nUsing this information, we will now proceed to plot the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "href": "Hands-on_Ex/hands_on02.html#controlling-and-customizing-map-layots",
    "title": "Hands-On Exercise 02",
    "section": "7.0 Controlling and Customizing Map Layots",
    "text": "7.0 Controlling and Customizing Map Layots\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n7.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/hands_on02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-On Exercise 02",
    "section": "8.0 Drawing Small Multiple Choropleth Maps",
    "text": "8.0 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"plasma\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/hands_on02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-On Exercise 02",
    "section": "9.0 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "9.0 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\nFor example, we have select the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/hands_on02.html#references",
    "href": "Hands-on_Ex/hands_on02.html#references",
    "title": "Hands-On Exercise 02",
    "section": "10. References",
    "text": "10. References\nTutorial provided by Professor Kam Tin Seong©, Singapore Management University\nReference: https://r4gdsa.netlify.app/chap02.html\n\n10.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n10.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n10.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html",
    "href": "Hands-on_Ex/hands_on08.html",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#overview",
    "href": "Hands-on_Ex/hands_on08.html#overview",
    "title": "Hands-On Exercise 08",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we explore how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "href": "Hands-on_Ex/hands_on08.html#importing-datasets-and-packages",
    "title": "Hands-On Exercise 08",
    "section": "2.0 Importing Datasets and Packages",
    "text": "2.0 Importing Datasets and Packages\nFirstly, we will install and import necessary R-packages for this modelling exercise. The R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML)\n\nNext, two data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\nmpsz = st_read(dsn = \"~/IS415-GAA/data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncondo_resale = read_csv(\"~/IS415-GAA/data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "href": "Hands-on_Ex/hands_on08.html#data-wrangling",
    "title": "Hands-On Exercise 08",
    "section": "3.0 Data Wrangling",
    "text": "3.0 Data Wrangling\n\n3.1 Geospatial Data Wrangling\nWe use st_transform() to update the imported mpsz with the correct ESPG code (i.e. 3414). Then, we use st_bbox() to view the extent of mpsz_svy21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n3.1 Aspatial Data Wrangling\nWe use glimpse() to have a quick overview of the data structure of condo_resale data.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nTo calculate the summary statistics of condo_resale data frame, we use st().\n\nst(condo_resale)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nLATITUDE\n1436\n1.3\n0.038\n1.2\n1.3\n1.4\n1.5\n\n\nLONGITUDE\n1436\n104\n0.067\n104\n104\n104\n104\n\n\nPOSTCODE\n1436\n440439\n201080\n18965\n259849\n589486\n828833\n\n\nSELLING_PRICE\n1436\n1751211\n1272778\n540000\n1100000\n1950000\n18000000\n\n\nAREA_SQM\n1436\n137\n58\n34\n103\n156\n619\n\n\nAGE\n1436\n12\n8.6\n0\n5\n18\n37\n\n\nPROX_CBD\n1436\n9.3\n4.3\n0.39\n5.6\n13\n19\n\n\nPROX_CHILDCARE\n1436\n0.33\n0.33\n0.0049\n0.17\n0.37\n3.5\n\n\nPROX_ELDERLYCARE\n1436\n1.1\n0.62\n0.055\n0.61\n1.4\n3.9\n\n\nPROX_URA_GROWTH_AREA\n1436\n4.6\n2\n0.21\n3.2\n5.8\n9.2\n\n\nPROX_HAWKER_MARKET\n1436\n1.3\n1\n0.052\n0.55\n1.7\n5.4\n\n\nPROX_KINDERGARTEN\n1436\n0.46\n0.26\n0.0049\n0.28\n0.58\n2.2\n\n\nPROX_MRT\n1436\n0.67\n0.48\n0.053\n0.35\n0.85\n3.5\n\n\nPROX_PARK\n1436\n0.5\n0.33\n0.029\n0.26\n0.66\n2.2\n\n\nPROX_PRIMARY_SCH\n1436\n0.75\n0.49\n0.077\n0.44\n0.95\n3.9\n\n\nPROX_TOP_PRIMARY_SCH\n1436\n2.3\n1.4\n0.077\n1.3\n2.9\n6.7\n\n\nPROX_SHOPPING_MALL\n1436\n1\n0.66\n0\n0.53\n1.4\n3.5\n\n\nPROX_SUPERMARKET\n1436\n0.61\n0.33\n0\n0.37\n0.79\n2.2\n\n\nPROX_BUS_STOP\n1436\n0.19\n0.25\n0.0016\n0.098\n0.22\n2.5\n\n\nNO_Of_UNITS\n1436\n409\n273\n18\n189\n590\n1703\n\n\nFAMILY_FRIENDLY\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\nFREEHOLD\n1436\n0.42\n0.49\n0\n0\n1\n1\n\n\nLEASEHOLD_99YR\n1436\n0.49\n0.5\n0\n0\n1\n1\n\n\n\n\n\n\n\nFinally, we will convert this aspatial data frame into a sf object. To do so, we will use st_as_sf() of sf package.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;% st_transform(crs=3414)\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/hands_on08.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 08",
    "section": "4.0 Exploratory Data Analysis (EDA)",
    "text": "4.0 Exploratory Data Analysis (EDA)\n\n4.1 EDA Using Statistical Graphics\nWe can plot the distribution of different data columns by using appropriate Exploratory Data Analysis (EDA). As an example, we will plot SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\nFrom the figure above, it seems like there is a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\n\n\n\n\n\n4.2 EDA Using Multiple Histogram Plots Distribution of Variables\nIn previous section, we specify a varible to plot. In this section, we will instead draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package. In this way, we can see the distribution plots of different variables at the same time.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#e9531e\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#DC375E\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#AE4285\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"#71508F\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n4.3 Drawing Statistical Point Map\nNext, we will learn how to reveal the geospatial distribution condominium resale prices in Singapore using statistical point maps. To plot such maps, we will prepare using tmap package.\n\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\nThen, we will create an interactive point symbol map using the data values from SELLING_PRICE column.\nNext, we will turn R display into plot mode.\n\n\n#tmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          palette = \"plasma\",\n          alpha = 1,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/hands_on08.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 08",
    "section": "5.0 Hedonic Pricing Modelling in R",
    "text": "5.0 Hedonic Pricing Modelling in R\nIn this section, we will explore how to build a hedonic pricing model for condominium resale units using lm() of R.\n\n5.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\ntab_model(condo.slr)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-258121.06\n-382717.70 – -133524.43\n&lt;0.001\n\n\nAREA SQM\n14719.03\n13879.23 – 15558.83\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.452 / 0.451\n\n\n\n\n\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\(y = -258121.1 + 14719x1\\)\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n5.2 Multiple Linear Regression Method\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. In this section, the corrplot package will be used to display the correlation matrix of the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         col=colorRampPalette(c(\"#E9531E\",\"#F4E8EC\",\"#B445B8\"))(10),\n         tl.pos = \"td\", tl.cex = 0.5,tl.col = \"black\", number.cex = 0.5, method = \"ellipse\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n5.3 Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nNow, we will build a hedonic pricing model of SELLING_PRICE using multiple linear regression method that we explored in previous section.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\nWith reference to the table above, it is clear that not all the independent variables are statistically significant (i.e. some variables resulted in p-value &gt; 0.05). We will revised the model by removing those variables which are not statistically significant.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\n\ntab_model(condo.mlr, show.fstat = TRUE,\n  show.aic = TRUE,show.aicc = TRUE)\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n481728.40\n243504.91 – 719951.90\n&lt;0.001\n\n\nAREA SQM\n12708.32\n11983.32 – 13433.33\n&lt;0.001\n\n\nAGE\n-24440.82\n-29861.15 – -19020.48\n&lt;0.001\n\n\nPROX CBD\n-78669.78\n-91948.06 – -65391.50\n&lt;0.001\n\n\nPROX CHILDCARE\n-351617.91\n-566353.20 – -136882.62\n0.001\n\n\nPROX ELDERLYCARE\n171029.42\n88423.78 – 253635.05\n&lt;0.001\n\n\nPROX URA GROWTH AREA\n38474.53\n13907.81 – 63041.26\n0.002\n\n\nPROX HAWKER MARKET\n23746.10\n-33729.46 – 81221.66\n0.418\n\n\nPROX KINDERGARTEN\n147468.99\n-14697.53 – 309635.51\n0.075\n\n\nPROX MRT\n-314599.68\n-428271.67 – -200927.69\n&lt;0.001\n\n\nPROX PARK\n563280.50\n432730.10 – 693830.90\n&lt;0.001\n\n\nPROX PRIMARY SCH\n180186.08\n52212.74 – 308159.42\n0.006\n\n\nPROX TOP PRIMARY SCH\n2280.04\n-37757.88 – 42317.95\n0.911\n\n\nPROX SHOPPING MALL\n-206604.06\n-290641.86 – -122566.25\n&lt;0.001\n\n\nPROX SUPERMARKET\n-44991.80\n-196200.15 – 106216.54\n0.560\n\n\nPROX BUS STOP\n683121.35\n411722.09 – 954520.61\n&lt;0.001\n\n\nNO Of UNITS\n-231.18\n-405.83 – -56.53\n0.010\n\n\nFAMILY FRIENDLY\n140340.77\n48103.40 – 232578.14\n0.003\n\n\nFREEHOLD\n359913.01\n263360.67 – 456465.35\n&lt;0.001\n\n\nObservations\n1436\n\n\nR2 / R2 adjusted\n0.652 / 0.647\n\n\nAIC\n42970.175\n\n\nAICc\n42970.769\n\n\n\n\n\n\n\n\n\n5.4 Checking for Multicollinearity\nIn this section, we will explore a R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nNow that we have built a multiple linear regression in previous session, we will now use ols_vif_tol() of olsrr package to test if there are sign of multicollinearity.\n\nmulticol_stats &lt;- ols_vif_tol(condo.mlr1)\ntableHTML(multicol_stats)\n\n\n\n\n\n\n\nVariables\nTolerance\nVIF\n\n\n\n\n1\nAREA_SQM\n0.872855423242667\n1.14566510486352\n\n\n2\nAGE\n0.707127520156393\n1.41417208564989\n\n\n3\nPROX_CBD\n0.635614652878236\n1.57328028149088\n\n\n4\nPROX_CHILDCARE\n0.306601856967953\n3.26155884993391\n\n\n5\nPROX_ELDERLYCARE\n0.659847919847265\n1.51550072360836\n\n\n6\nPROX_URA_GROWTH_AREA\n0.751031083374135\n1.33150281278283\n\n\n7\nPROX_MRT\n0.523608983366243\n1.90982208435592\n\n\n8\nPROX_PARK\n0.827926085868263\n1.20783729015046\n\n\n9\nPROX_PRIMARY_SCH\n0.452462836020451\n2.21012626980661\n\n\n10\nPROX_SHOPPING_MALL\n0.673879496684337\n1.48394483720051\n\n\n11\nPROX_BUS_STOP\n0.351411792499116\n2.84566432130337\n\n\n12\nNO_Of_UNITS\n0.690103613311802\n1.44905776568972\n\n\n13\nFAMILY_FRIENDLY\n0.724415713651706\n1.38042284444535\n\n\n14\nFREEHOLD\n0.693116329580593\n1.44275925601854\n\n\n\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n5.5 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nWe will use ols_plot_resid_fit() of olsrr package to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n5.6 Test for Normality Assumption\nLastly, we will use ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\nnormality_stats &lt;- ols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the Kolmogorov-Smirnov test\n\nnormality_stats\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n5.7 Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nNext, we will join the newly created data frame with condo_resale.sf object.\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\n\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          palette = \"plasma\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\nThe figure above seems to indicate that there is sign of spatial autocorrelation. However, to prove that our observation is indeed true, the Moran’s I test will be performed.\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_summary &lt;- summary(nb)\nnb_summary\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "href": "Hands-on_Ex/hands_on08.html#building-hedonic-pricing-model-using-gwmodel",
    "title": "Hands-On Exercise 08",
    "section": "6.0 Building Hedonic Pricing Model using GWmodel",
    "text": "6.0 Building Hedonic Pricing Model using GWmodel\nAfter exploring the use of linear regression and multiple linear regression in previous sessions, we will now explore how to model hedonic pricing using both the fixed and adaptive bandwidth schemes.\nGWR is an outgrowth of ordinary least squares regression (OLS); and adds a level of modeling sophistication by allowing the relationships between the independent and dependent variables to vary by locality. Note that the basic OLS regression model above is just a special case of the GWR model where the coefficients are constant over space. The parameters in the GWR are estimated by weighted least squares. The weighting matrix is a diagonal matrix, with each diagonal element wij being a function of the location of the observation. The role of the weight matrix is to give more value to observations that are close to i, as it is assumed that observations that are close will influence each other more than those that are far away (Tobler’s Law).\nThere are three major decisions to make when running a GWR: (1) the bandwidth h of the function, which determines the degree of distance decay, (2) the kernel density function assigning weights wij ,and (3) who to count as neighbors.\n\n6.1 Computing Bandwidth\nTo calculate the optimal bandwidth to use in the model, bw.gwr() of GWModel package can be used, with both fixed and adapative mode. Also, There are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=TRUE, \n                   longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n6.2 Building Basic GWModel with Fixed and Adaptive Bandwidth\nNow we can use the fixed and adaptive bandwidth values above to calibrate the gwr model using gaussian kernel (which is the default kernel density function).\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA +\n      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n      PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                      FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-01-22 13:55:14.368297 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-01-22 13:55:15.832465 \n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-01-22 13:55:15.833146 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-01-22 13:55:17.48637 \n\n\nBased on the results, two conclusions can be made as below.\n\nThe AICc of the fixed-bandwidth GWR model is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\nThe AICc the adaptive-bandwidth GWR model is 41982.22 which is even smaller than the AICc of the fixed-bandwidth GWR model, which is 42263.61.\n\n\n\n6.3 Visualisaing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nTo visualise the fields in SDF, we need to first covert it into sf data frame.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\nWe will now visualise the local R2 value as below.\n\n#tmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\n\n\n\n\n\n#tmap_mode(\"view\")\n\n{= 1) +}   tm_view(set.zoom.limits = c(11,14))\nNext, we will visualise the coefficient estimates\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          palette=\"plasma\",\n          size = 0.2,\n          alpha = 0.5)\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid\n\nWarning: The shape mpsz_svy21 is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01.html#introduction",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "",
    "text": "Human mobility, the spatial-temporal dynamics of human movements, serves as a critical reflection of societal patterns and human behaviors. With the advancement and pervasiveness of Information and Communication Technologies (ICT) in our daily life, especially smart phone, a large volume of data related to human mobility have been collected. These data provide valuable insights into understanding how individuals and populations move within and between different geographical locations. By using appropriate GIS analysis methods, these data can turn into valuable inisghts for predicting future mobility trrends and developing more efficient and sustainable strategies for managing urban mobility.\nIn this study, I will apply Spatial Point Patterns Analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore. In order to determine the geographical and spatio-temporal patterns of the Grab hailing services, I will develop traditional Kernel Density Estimation (KDE) and Temporal Network Kernel Density Estimation (TNKDE). KDE layers will help identify the areas with high concentration of Grab hailing services, providing insights into the demand and popularity of these services in different parts of Singapore. TNKDE, on the other hand, will allow for analysis of how the distribution of Grab hailing services changes over time, revealing temporal patterns and trends in their usage. These spatial and spatio-temporal analyses will contribute to a better understanding of the dynamics and effectiveness of Grab’s mobility services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#literature-review-of-spatial-point-pattern-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "2.0 Literature Review of Spatial Point Pattern Analysis",
    "text": "2.0 Literature Review of Spatial Point Pattern Analysis\nSpatial point pattern analysis is concerned with description, statistical characterization, modeling and visulisation of point patterns over space and making inference about the process that could have generated an observed pattern (Boots & Getis, 1988 ,Rey et al., 2023; Pebesma & Bivand, 2023). According to this theory, empirical spatial distribution of points in our daily life are not controlled by sampling, but a result of an underlying geographically-continuous process (Rey et al., 2023). For example, an COVID-19 cluster did not happen by chance, but due to a spatial process of close-contact infection.\nWhen analysing real-world spatial points, it is important to analyse whether the observed spatial points are randomly distributed or patterned due to a process or interaction (Floch et al., 2018). In “complete random” distribution, points are located everywhere with the same probability and independently of each other. On the other hand, the spatial points can be clustered or dispersed due to an underlying point process. However, it is challenging to use heuristic observation and intuitive interpretation to detect whether a spatial point pattern exists (Baddeley et al., 2015; Floch et al., 2018). Hence, spatial point pattern analysis can be used to detect the spatial concentration or dispersion phenomena.\n\nWhen analysing and interpreting the properties of a point pattern, these properties can be categorized into two: (a) first-order properties and (b) second-order properties (Yuan et al., 2020; Gimond, 2023). First-order properties concern with the characteristics of individual point locations and their variations of their density across space (Gimond, 2023). Under this conception, observations vary from point to point due to changes in the underlying property. Second-order properties focus on not only individual points, but also the interactions between points and their influences on one another (Gimond, 2023). Under this conception, observations vary from place to place due to interaction effects between observations. First-order properties of point patterns are mostly addressed by density-based techniques, such as quadrat analysis and kernel density estimation, whereas, distance-based techniques, such nearest neighbour index and K-functions, are often used to analyse second-order properties since they take into account the distance between point pairs (Yuan et al., 2020; Gimond, 2023)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-packages",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "3.0 Importing Packages",
    "text": "3.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow : for reading and writing Apache Parquet files\nlubridate : for tackling with temporal data (dates and times)\nspatstat: A package for statistical analysis of spatial data, specifically Spatial Point Pattern Analysis. This package was provided by Baddeley, Turner and Ruback (2015) and gives a comprehensive list of functions to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nrgdal: Used to import geospatial data and output as spatial class objects from sp package\nraster : reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools, ggplot2, ggthemes, plotly: Packages used to plot interactive visualisations summary statistics and KDE layers\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf,st,spatstat,patchwork)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "href": "Take-home_Ex/Take-home_Ex01.html#importing-datasets-into-r-environment",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "4.0 Importing Datasets into R Environment",
    "text": "4.0 Importing Datasets into R Environment\n\n4.1 Datasets\nIn this study, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia. Apart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\nMoreover, we will also use OpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nTo extract the Singapore boundary, we will use Master Plan 2019 Subzone Boundary (No Sea), provided by data.gov.sg.\n\n\n4.2 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format. The Singapore portion of the dataset is packaged into a total of 10 Parquet files.\nFirstly, we will use read_parquet function from arrow package, which allows us to read Parquet files into R environment as a data frame (more specifically, a tibble).\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet',as_data_frame = TRUE)\ndf1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet',as_data_frame = TRUE)\ndf2 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00002.snappy.parquet',as_data_frame = TRUE)\ndf3 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00003.snappy.parquet',as_data_frame = TRUE)\ndf4 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00004.snappy.parquet',as_data_frame = TRUE)\ndf5 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00005.snappy.parquet',as_data_frame = TRUE)\ndf6 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00006.snappy.parquet',as_data_frame = TRUE)\ndf7 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00007.snappy.parquet',as_data_frame = TRUE)\ndf8 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00008.snappy.parquet',as_data_frame = TRUE)\ndf9 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00009.snappy.parquet',as_data_frame = TRUE)\n\nTo consolidate all trajectory instances into a single dataframe, we’ll vertically bind all 10 imported dataframes using bind_rows() function from tidyverse package.\n\ndf_trajectories &lt;- bind_rows(df,df1,df2,df3,df4,df5,df6,df7,df8,df9)\n\nTo get a quick overview of the dataset, we’ll first check the number of trajectory instances using dim() function. Then, we’ll use head() function to quickly scan through the data columns and values\n\ndim(df_trajectories)\n\n[1] 30329685        9\n\nhead(df_trajectories)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 30329685 trajectory instances, each with a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf_trajectories$pingtimestamp &lt;- as_datetime(df_trajectories$pingtimestamp)\n\n\n\n4.3 Importing OpenStreetMap road data for Malaysia, Singapore and Brunei\nThe gis_osm_roads_free_1 dataset, which we downloaded from OpenStreetMap, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nosm_road_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"gis_osm_roads_free_1\") %&gt;% st_transform(crs = 3414)\n\n\n\n4.4 Importing Singapore Master Plan Planning Subzone boundary data\nThe MP14_SUBZONE_WEB_PL dataset, which we downloaded from data.gov.sg, is in ESRI shapefile format. To use this data in an R-environment, we need to import it as an sf object. We can do this using the st_read() function of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\nmpsz_sf &lt;- st_read(dsn = \"~/IS415-GAA/data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we use %&gt;% operator is used to pipe the output of st_read() to the st_transform() function. Since the dataset we are using is the Singapore boundary, we need to assign the standard coordinate reference system for Singapore, which is SVY21 (EPSG:3414). st_transform() function transforms the coordinate reference system of the sf object to 3414.\n\n\nAfter importing the dataset, we will plot it to see how it looks. The plot() function is used to plot the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object.\n\nplot(st_geometry(mpsz_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "5.0 Data Wrangling",
    "text": "5.0 Data Wrangling\nData wrangling is the process of converting and transforming raw data into a usable form and is carried out prior to conducting any data analysis.\n\n5.1 Extracting Trip Starting Locations and Temporal Data Values from Grab-Posisi dataset\nFirstly, we will extract trip starting locations for all unqiue trajectories in the dataset and store them to a new df named origin_df. We are also interested in obtaining valuable temporal data such as the day of the week, the hour, and the date (yy-mm-dd). To do so, we will use the following functions from lubridate package, and add the newly derived values as columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n5.2 Extracting Trip Ending Locations and Temporal Data Values from Grab-Posisi dataset\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df_trajectories %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, this default order is applied to origin_df. However, for destination_df, the arrange(desc()) argument is used to modify the default order to descending.\n\n\n\n\n5.3 Converting to sf tibble data.frame\n\norigin_sf &lt;- st_as_sf(origin_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\ndestination_sf &lt;- st_as_sf(destination_df,\n                      coords = c(\"rawlng\", \"rawlat\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n5.4 Creating a CoastalLine of Singapore\nThe original mpsz_sf dataset we imported include information of all URA Master Plan planning area boundaries. However, for this analysis, we only need the national-level boundary of Singapore. Hence, we will need to union all the subzone boundaries to one single polygon boundary. Also, Grab ride-hailing service is only available on the main Singapore islands. Hence, we will need to remove outer islands which Grab service is not available. In particular, we will remove the following planning subzones: NORTH-EASTERN ISLANDS, SOUTHERN GROUP, SUDONG & SEMAKAU.\nWe can remove these subzones using the subset() function. The subset() function is used to extract rows from a data frame that meet certain conditions.\n\nnortheasten.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouthern.islands &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\")\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\")\n\nouterislands &lt;- dplyr::bind_rows(list(northeasten.islands,southern.islands,sudong,semakau))\n\n\n\n\n\n\n\nReflection\n\n\n\nIn the code chunk above, we first created four new data frames called northeasten.islands, southern.islands, sudong, and semakau by selecting rows from mpsz_sf where the value in the SUBZONE_N column matches the corresponding value.\nAfter that, we used bind_rows() function from the dplyr package to combine these four data frames into a single data frame called outerislands.\n\n\nAfter importing the dataset, we will plot it to see how it looks.\n\nplot(st_geometry(outerislands))\n\n\n\n\nAs mentioned earlier, we only need to get national-level boundary of Singapore, without outer islands. To do so, we will need to process the mpsz_sf layer to achieve the outcome. - We will first use st_union() function from the sf package to combine the geometries of mpsz_sf and outerislands sf objects into a single geometry each. - Next, we will use st_difference() function then removes the overlapping areas between the two geometries. - Finally, we will store the non-overlapping areas into a new sf objected called sg_sf.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outerislands))\n\nTo assess whether the geometry of the newly created sg_sf matches our intended outcome, we will plot it out.\n\nplot(st_geometry(sg_sf))\n\n\n\n\n\n\n5.5 Extracting Road Layers within Singapore\nAs we have seen in Section 4.3., osm_road_sf dataset includes road networks from not only Singapore, but also Malaysia and Brunei. However, our analysis is focused on Singapore. Hence, we will need to remove unecessary data rows. To do so, we will\n\nsg_road_sf &lt;- st_intersection(osm_road_sf,sg_sf)\n\nNext, we will look at the classification of road networks as provided by OpenStreetMap.\n\nunique(sg_road_sf$fclass)\n\n [1] \"primary\"        \"residential\"    \"tertiary\"       \"footway\"       \n [5] \"service\"        \"secondary\"      \"motorway\"       \"motorway_link\" \n [9] \"trunk\"          \"trunk_link\"     \"primary_link\"   \"pedestrian\"    \n[13] \"living_street\"  \"unclassified\"   \"steps\"          \"track_grade2\"  \n[17] \"track\"          \"secondary_link\" \"cycleway\"       \"path\"          \n[21] \"tertiary_link\"  \"track_grade1\"   \"track_grade3\"   \"unknown\"       \n[25] \"track_grade5\"   \"bridleway\"      \"track_grade4\"  \n\n\n\ntm_shape(sg_sf) +\n  tm_polygons() +\ntm_shape(sg_road_sf) +\n  tm_lines(col=\"fclass\", palette =\"viridis\") +\n  tm_layout(main.title = \"Road Network in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.outside = TRUE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.5 Converting the Simple Features to Planar Point Pattern Object\nIn order to use the capabilities of spatstat packahe, a spatial dataset should be converted into an object of class planar point pattern ppp (Baddeley et al., 2015). A point pattern object contains the spatial coordinates of the points, the marks attached to the points (if any), the window in which the points were observed, and the name of the unit of length for the spatial coordinates. s. Thus, a single object of class ppp contains all the information required to perform spatial point pattern analysis.\nIn previous section, we have created sf objects of Grab trajectory origin and destination points. Now, we will convert them into ppp objects using as.ppp() function from spatstat package.\n\norigin_ppp &lt;- as.ppp(st_coordinates(origin_sf), st_bbox(origin_sf))\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe code chunk above converts the origin_sf object to a point pattern object of class ppp. st_coordinates() function is used to extract the coordinates of the origin_sf object and st_bbox() function is used to extract the bounding box of the origin_sf object. The resulting object origin_ppp is a point pattern object of class ppp.\n\n\n\ndestination_ppp &lt;- as.ppp(st_coordinates(destination_sf), st_bbox(destination_sf))\n\npar(mar = c(0,0,1,0))\nplot(destination_ppp)\n\n\n\n\n\n\n5.6 Handling Data Errors\nBefore going striaght into analysis, we will need to a quick look at the summary statistics of the newly created ppp objects. This is an important step to ensure that the data is free of errors and that a reliable analysis can be performed.\n\n5.6.1 Data Error Handling for origin_ppp\nWe will use summary() function to get summary information of origin_ppp object.\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\nWe can also check if there is any duplicated points in origin_ppp object using any(duplicated() function.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordaintes in the origin_ppp object.\n\nWhy do we need to check duplication?\nWhen analyzing spatial point processes, it is important to avoid duplication of points. This is because statistical methodology for spatial point processes is based largely on the assumption that processes are simple, i.e., that points of the process can never be coincident. When the data have coincident points, some statistical procedures designed for simple point processes will be severely affected (Baddeley et al., 2015).\n\n\n\n5.6.2 Data Error Handling for destination_ppp\nWe will use summary() function to get summary information of destination_ppp object.\n\nsummary(destination_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\nWe can also check if there is any duplicated points in destination_ppp object using any(duplicated() function.\n\nany(duplicated(destination_ppp))\n\n[1] FALSE\n\n\nThe code output is FALSE, which means there are no duplication of point coordinates in the destination_ppp object.\n\n\n\n5.7 Creating Observation Windows\nMany data types in spatstat require us to specify the region of space inside which the data were observed. This is the observation window and it is represented by an object of class owin. In this analysis, our study area is Singapore, hence we will use Singapore boundary as the observation window for spatial point pattern analysis.\nIn Section 5.4, we have already created the sg_sf object, which represents the Singapore boundary (without outer islands). To convert this sf object to owin object, we will use as.owin() function from spatstat package.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot.owin(sg_owin)\n\n\n\n\nWe will use summary() function to get summary information of sg_owin object.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459\n\n\n\n\n5.8 Combining ppp objects and owin object\nIn section 5.5, we have created two ppp objects - origin_ppp and destination_ppp, each representing the spatial points of Grab trajectory origin and destination. In section 5.7, we have created a owin object called sg_owin, which represent the observation window of our analysis.\nThe observation window sg_owin and the point pattern origin_ppp or destination_ppp can be combined, so that the custom window replaces the default ractangular extent (as seen in section 5.5).\n\norigin_ppp_sg = origin_ppp[sg_owin]\ndestination_ppp_sg = destination_ppp[sg_owin]\n\npar(mar = c(0,0,1,0))\nplot(origin_ppp_sg)\n\n\n\nplot(destination_ppp_sg)\n\n\n\n\nWe will use summary() function to get summary information of the newly created origin_ppp_sg object and destination_ppp_sg object.\n\nsummary(origin_ppp_sg)\n\nPlanar point pattern:  28000 points\nAverage intensity 3.981136e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459\n\nsummary(destination_ppp_sg)\n\nPlanar point pattern:  27997 points\nAverage intensity 3.980709e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n44 separate polygons (24 holes)\n                  vertices         area relative.area\npolygon 1            14651  6.97996e+08      9.92e-01\npolygon 2 (hole)         3 -2.21090e+00     -3.14e-09\npolygon 3              285  1.61128e+06      2.29e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.93e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.26e-11\npolygon 6               27  1.50315e+04      2.14e-05\npolygon 7 (hole)        36 -4.01660e+04     -5.71e-05\npolygon 8 (hole)       317 -5.11280e+04     -7.27e-05\npolygon 9 (hole)         3 -2.89050e-05     -4.11e-14\npolygon 10              30  2.80002e+04      3.98e-05\npolygon 11 (hole)        3 -2.83151e-01     -4.03e-10\npolygon 12              71  8.18750e+03      1.16e-05\npolygon 13 (hole)        3 -1.68316e-04     -2.39e-13\npolygon 14 (hole)       36 -7.79904e+03     -1.11e-05\npolygon 15 (hole)        4 -2.05611e-02     -2.92e-11\npolygon 16 (hole)        3 -2.18000e-06     -3.10e-15\npolygon 17 (hole)        3 -3.65501e-03     -5.20e-12\npolygon 18 (hole)        3 -4.95057e-02     -7.04e-11\npolygon 19 (hole)        3 -3.99521e-02     -5.68e-11\npolygon 20 (hole)        3 -6.62377e-01     -9.42e-10\npolygon 21 (hole)        3 -2.09065e-03     -2.97e-12\npolygon 22              91  1.49663e+04      2.13e-05\npolygon 23 (hole)       26 -1.25665e+03     -1.79e-06\npolygon 24 (hole)      349 -1.21433e+03     -1.73e-06\npolygon 25 (hole)       20 -4.39069e+00     -6.24e-09\npolygon 26 (hole)       48 -1.38338e+02     -1.97e-07\npolygon 27 (hole)       28 -1.99862e+01     -2.84e-08\npolygon 28              40  1.38607e+04      1.97e-05\npolygon 29 (hole)       40 -6.00381e+03     -8.54e-06\npolygon 30 (hole)        7 -1.40545e-01     -2.00e-10\npolygon 31 (hole)       12 -8.36709e+01     -1.19e-07\npolygon 32              45  2.51218e+03      3.57e-06\npolygon 33             142  3.22293e+03      4.58e-06\npolygon 34             148  3.10395e+03      4.41e-06\npolygon 35              75  1.73526e+04      2.47e-05\npolygon 36              83  5.28920e+03      7.52e-06\npolygon 37             106  3.04104e+03      4.32e-06\npolygon 38             266  1.50631e+06      2.14e-03\npolygon 39              71  5.63061e+03      8.01e-06\npolygon 40              10  1.99717e+02      2.84e-07\npolygon 41             478  2.06120e+06      2.93e-03\npolygon 42              65  8.42861e+04      1.20e-04\npolygon 43              47  3.82087e+04      5.43e-05\npolygon 44              22  6.74651e+03      9.59e-06\nenclosing rectangle: [2667.54, 55941.94] x [21494.3, 50256.33] units\n                     (53270 x 28760 units)\nWindow area = 703317000 square units\nFraction of frame area: 0.459"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-spatial-data-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "6.0 Exploratory Spatial Data Analysis",
    "text": "6.0 Exploratory Spatial Data Analysis\n\n6.1 Visualising Frequency Distribution\n\norigin_day &lt;- ggplot(data=origin_df, \n              aes(x=weekday)) + \n              geom_bar()\n\ndestination_day &lt;-  ggplot(data=destination_sf, \n                    aes(x=weekday)) + \n                    geom_bar()\n\norigin_day + destination_day\n\n\n\n\n\n\n6.2 Creating Point Symbol Maps\n\ntmap_mode(\"view\")\ntm_shape(origin_sf) +\n    tm_dots(alpha=0.4, \n          size=0.05)\n\ntm_shape(destination_sf) +\n    tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n6.2 Measuring Central Tendency\nDescriptive statistics are used in point pattern analysis to summarise a point pattern’s basic properties, such as its central tendency and dispersion. The mean centre and the median centre are two often employed metrics for central tendency (Gimond, 2019).\n\n6.2.1 Mean Center\nMean center is the arithmetic average of the (x, y) coordinates of all point in the study area. Similar to mean in statistical analysis, mean center is influenced to a greater degree by the outliers. (Yuan et al.,2020)\n\norigin_xy &lt;- st_coordinates(origin_sf)\norigin_mc &lt;- apply(origin_xy, 2, mean)\n\ndestination_xy &lt;- st_coordinates(destination_sf)\ndestination_mc &lt;- apply(destination_xy, 2, mean)\n\norigin_mc\n\n       X        Y \n28490.57 36939.04 \n\ndestination_mc\n\n       X        Y \n28870.96 36590.49 \n\n\nThe results show that the origin and destination mean centres are, respectively, (28490.57, 36939.04) and (28870.96, 36590.49). The two mean centres appear to be situated in close proximity to one another.\n\n\n6.2.2 Median Center\nMedian center is the location that minimizes the sum of distances required to travel to all points within an observation window. It can be calculated using an iterative procedure first presented by Kulin and Kuenne (1962). The procedure begins at a predetermined point, such as the median center, as the initial point. Then, the algorithm updates the median center’s new coordinates (x’, y’) continually until the optimal value is reached. The median center, as opposed to the mean center, offers a more reliable indicator of central tendency as it is unaffected by outliers (Yuan et al., 2020).\n\norigin_medc &lt;- apply(origin_xy, 2, median)\n\ndestination_medc &lt;- apply(destination_xy, 2, median)\n\norigin_medc\n\n       X        Y \n28553.17 36179.05 \n\ndestination_medc\n\n       X        Y \n28855.04 35883.86 \n\n\nBased on the results, the median centres of origin and destination are, respectively, (28553.17, 36179.05) and (28855.04, 35883.86). The two median centres appear to be situated in close proximity to one another.\nMoreover, mean centers and median centers for each origin and destination points are similar. This may imply that the distribution of the data is relatively balanced and there is not a significant difference in the spatial patterns between the origin and destination points. Additionally, this indicates that both the mean center and median center are effective measures for analyzing the central tendency of the data in this context.\n\n\n6.2.3 Plotting Mean and Median Centers\nWe can try to plot both results obtained from previous section on the same plane for better comparison of the mean center and median center.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(origin_medc[1], origin_medc[2]), pch='*', col='purple', cex=3)\n\n\n\n\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey', main=\"mean and median centers of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='yellow', cex=3)\npoints(cbind(destination_medc[1], destination_medc[2]), pch='*', col='orange', cex=3)\n\n\n\n\n\n\n\n6.2 Measuring Dispersion\n\n6.2.1 Standard Distance\nStandard distances are defined similarly to standard deviations. This indicator measures how dispersed a group of points is around its mean center (Gimond, 2023).\n\norigin_sd &lt;- sqrt(sum((origin_xy[,1] - origin_mc[1])^2 + (origin_xy[,2] - origin_mc[2])^2) / nrow(origin_xy))\n\ndestination_sd &lt;- sqrt(sum((destination_xy[,1] - destination_mc[1])^2 + (destination_xy[,2] - destination_mc[2])^2) / nrow(destination_xy))\n\norigin_sd\n\n[1] 10187.88\n\ndestination_sd\n\n[1] 9545.69\n\n\nFrom the results, the origin and destination standard distances are 10187.88 and 9545.69, respectively. Hence, it appears that origin points are more dispersed than the origin points.\n\n\n\n\n\n\nReflection\n\n\n\nHowever, it would be challenging to discern why the origin points are more dispersed without further analysis. Further analysis would be needed to determine the factors contributing to the increased dispersion of destination points. Since it is out of scope for this exercise, we will not explore any further.\n\n\n\n\n6.2.3 Plotting Standard Distances\nIn this section, we will create bearing circles of origin and destination points using the standard distance values we have calculated earlier. This can provide visual representation of their dispersion and make intuitive comparison between them.\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey', main=\"standard distance of origin_sf\")\npoints(origin_xy, cex=.5)\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- origin_mc[1] + origin_sd * cos(bearing)\ncy &lt;- origin_mc[2] + origin_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='red', lwd=2)\n\n\n\n\n\npar(mar = c(0,0,1,0))\nplot(sg_sf, col='light grey',main=\"standard distance of destination_sf\")\npoints(destination_xy, cex=.5)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\ncx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ncy &lt;- destination_mc[2] + destination_sd * sin(bearing)\ncircle &lt;- cbind(cx, cy)\nlines(circle, col='purple', lwd=2)\n\n\n\n\nA better comparison of the standard distances between origin and destination points can also be achieved by trying to plot both results on the same plane.\n\npar(mar = c(0,0,1,0))\n\nplot(sg_sf, col='light grey',main=\"standard distances of origin_sf & destination_sf\")\npoints(cbind(origin_mc[1], origin_mc[2]), pch='*', col='red', cex=3)\npoints(cbind(destination_mc[1], destination_mc[2]), pch='*', col='purple', cex=3)\n\nbearing &lt;- 1:360 * pi/180\n\norigin_cx &lt;- origin_mc[1] + origin_sd * cos(bearing)\norigin_cy &lt;- origin_mc[2] + origin_sd * sin(bearing)\n\ndestination_cx &lt;- destination_mc[1] + destination_sd * cos(bearing)\ndestination_cy &lt;- destination_mc[2] + destination_sd * sin(bearing)\n\norigin_circle &lt;- cbind(origin_cx, origin_cy)\ndestination_circle &lt;- cbind(destination_cx, destination_cy)\n\nlines(origin_circle, col='red', lwd=2)\nlines(destination_circle, col='purple', lwd=2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "7.0 First-Order Spatial Point Patterns Analysis",
    "text": "7.0 First-Order Spatial Point Patterns Analysis\nAfter data wrangling is complete, we will start to perform first-order spatial point pattern analysis using functions from spatstat package. As we have discussed in Section 2.0., First-order properties concern the characteristics of individual point locations and their variations of their density across space and are mostly addressed by density-based techniques, such as quadrant analysis and kernel density estimation.\nInvestigation of the intensity of a point pattern is one of the first and most important steps in point pattern analysis (Baddeley et al., 2015). If the point process has an intensity function λ(u), this function can be estimated non-parametrically by kernel estimation (Baddeley et al., 2015). Kernel estimation allows for smoothing of the probability density estimation of a random variable (in this analysis a point event) based on kernels as weights.\n\n7.1 Kernel Density Estimation\nKernel Destiny Estimation (KDE) generates a surface (raster) representing the estimated distribution of point events over the observation window. Each cell in the KDE layer carries a value representing the estimated density of that location (Wilkin, 2020). Hence, this approach is also known as local density approach.\nTo build the KDE layer, a localised density is calculated for multiple small subsets of the observation window. However, these subsets overlap throughout each iteration, resulting in a moving window defined by a kernel (Wilkin, 2020; Gimond, 2023).\nKernel estimation is implemented in spatstat by the function density.ppp(), a method for the generic command density.\n\npar(mar = c(0,0,1,0))\nden &lt;- density(origin_ppp_sg)\nplot(den,main = \"default density\")\ncontour(den, add=TRUE)\n\n\n\n\nHowever, the KDE given by default argument may not be what we aim to achieve. In this regards, we can specify smoothing bandwidth through the argument sigma or kernel function through the argument kernel.\n\n\n7.2 Bandwidth Selection\nThe kernel bandwidth sigma controls the degree of smoothing. In general, a small value of sigma produces an irregular intensity surface, while a large value of sigma appears to oversmooth the intensity.\n\n\n\nDensity Estimates with Different Smoothing Bandwidth (Ref, Baddeley et al., 2015)\n\n\n\n7.2.1 Fixed Bandwidth\ndensity() function of spatstat allows us to compute a kernel density for a given set of point events.\n\nbw.diggle() (Cross Validated)\nbw.CvL() (Cronie and van Lieshout’s Criterion)\nbw.scott() (Scott’s Rule)\nbw.ppl() (Likelihood Cross Validation)can also be used.\n\n\nbw_diggle &lt;- bw.diggle(origin_ppp_sg)\nbw_diggle\n\n   sigma \n8.300264 \n\nbw_CvL &lt;- bw.CvL(origin_ppp_sg)\nbw_CvL\n\n   sigma \n3147.562 \n\nbw_scott &lt;- bw.scott(origin_ppp_sg)\nbw_scott\n\n  sigma.x   sigma.y \n1592.6707  938.9324 \n\nbw_ppl &lt;- bw.ppl(origin_ppp_sg)\nbw_ppl\n\n   sigma \n123.8744 \n\n\nThe commands used for generating bandwidth return a numerical value, the optimised bandwidth, which also belongs to the special class bw.optim. The plot method for this class shows the objective function for the optimisation that leads to the result.\n\npar(mfrow = c(1,2))\nplot(bw_diggle, xlim=c(0,40), ylim=c(-70,200))\nplot(bw_CvL)\n\n\n\npar(mfrow = c(1,2))\nplot(bw_scott)\nplot(bw_ppl, xlim=c(-1000,6000), ylim=c(-320000,-260000))\n\n\n\n\n\n\n7.2.2 Adaptive Bandwidth\nA fixed smoothing bandwidth is unsatisfactory if the true intensity varies greatly across the spatial domain, because it is likely to cause over-smoothing in the high-intensity areas and under-smoothing in the low intensity areas (Baddeley et al., 2015)\nThe fixed bandwidth method is very sensitive to highly skewed distribution of spatial point patterns over geographical units.\n\nkde_origin &lt;- adaptive.density(origin_ppp_sg, method = \"kernel\")\n\n\npar(mar = c(0,0,1,0))\nplot(kde_origin,main = \"adapative density\")\n\n\n\n\n\n\n\n7.4 Rescaling KDE Values\n\norigin_ppp_sg.km &lt;- rescale(origin_ppp_sg, 1000, \"km\")\ndestination_ppp_sg.km &lt;- rescale(destination_ppp_sg, 1000, \"km\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "8.0 Network Constrained Kernel Density Estimation (NKDE)",
    "text": "8.0 Network Constrained Kernel Density Estimation (NKDE)\nMany real world point event are not randomly distributed. Their distribution, on the other hand, are constrained by network such as roads, rivers, and fault lines just to name a few of them.\n\npacman::p_load(dodgr)\nsg_network &lt;- weight_streetnet(dodgr_streetnet(\"singapore\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "href": "Take-home_Ex/Take-home_Ex01.html#temporal-network-kernel-density-estimation-tnkde",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "9.0 Temporal Network Kernel Density Estimation (TNKDE)",
    "text": "9.0 Temporal Network Kernel Density Estimation (TNKDE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01.html#references",
    "title": "Take-Home Exercise 01: Spatial Point Patterns Analysis of Grab Trajectories in Singapore",
    "section": "References",
    "text": "References\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial Point Patterns: Methodology and Applications with R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b19708.\nBoots, B.N., & Getis, A. (1988). Point Pattern Analysis. Reprint. Edited by G.I. Thrall. WVU Research Repository.\nFloch, J.-M., Marcon, E., Puech, F. (n.d.). Spatial distribution of points. In M.-P. de Bellefon (Ed.), Handbook of Spatial Analysis : Theory and Application with R (pp. 72–111). Insee-Eurostat.\nGimond (2023). Chapter 11 Point Pattern Analysis. Retrieved from https://mgimond.github.io/Spatial/index.html.\nPebesma, E.; Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\nRey, S.J., Arribas-Bel, D., Wolf, L.J. (2023). Point Pattern Analysis. In: Geographic Data Science with python. CRC Press.\nWilkin, J. (2020). Geocomputation 2020-2021 Work Book. University College London. Retrieved from https://jo-wilkin.github.io/GEOG0030/coursebook/analysing-spatial-patterns-iii-point-pattern-analysis.html.\nYuan, Y., Qiang, Y., Bin Asad, K., and Chow, T. E. (2020). Point Pattern Analysis. In J.P. Wilson (Ed.), The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition). DOI: 10.22224/gistbok/2020.1.13.\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app/"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#overview",
    "title": "In-Class Exercise 03 NKDE",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network constrained kernel density estimation (NetKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#installing-and-launching-the-r-package",
    "title": "In-Class Exercise 03 NKDE",
    "section": "2.0 Installing and launching the R package",
    "text": "2.0 Installing and launching the R package\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nrgdal, which provides bindings to the ‘Geospatial’ Data Abstraction Library (GDAL) (&gt;= 1.11.4) and access to projection/transformation operations from the PROJ library. In this exercise, rgdal will be used to import geospatial data in R and store as sp objects.\nsp, which provides classes and methods for dealing with spatial data in R. In this exercise, it will be used to manage SpatialPointsDataFrame and SpatiaLinesDataFrame, and for performing projection transformation.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, spNetwork, tmap, classInt, virdis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#importing-data-to-r-environment",
    "title": "In-Class Exercise 03 NKDE",
    "section": "3.0 Importing Data to R Environment",
    "text": "3.0 Importing Data to R Environment\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format.\n\nnetwork &lt;- st_read(dsn=\"../data/geospatial\", \n                   layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\nchildcare &lt;- st_read(dsn=\"../data/geospatial\",\n                     layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\nNext, we will plot the childcare locations (as points) and road network (as lines) as follows.\n\ntmap_mode('view')\ntm_shape(childcare)+\n  tm_dots(col='orange')+\n  tm_shape(network)+\n  tm_lines()\n\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "href": "In-class_Ex/In-class_Ex03-NKDE.html#network-constrained-kde-netkde-analysis",
    "title": "In-Class Exercise 03 NKDE",
    "section": "4.0 Network Constrained KDE (NetKDE) Analysis",
    "text": "4.0 Network Constrained KDE (NetKDE) Analysis\n\n4.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nlixels &lt;- lixelize_lines(network, 750, mindist = 375)\n\n\n\n4.2 Generating Line Centers\nNext, we will used lines_center() of spNetwork to generate a SpatialPointsDataFrame (i.e. samples) with line centre points.\n\nsamples &lt;- lines_center(lixels)\n\n\n\n4.3 Performing NKDE\nOnce we have obtained all the datasets required, we can perform NKDE by using nkde() function of spNetwork.\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1,nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n4.4 Visualisation\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nNext, we will rescale the density values to help with better mapping results\n\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\", palette=\"plasma\")+\ntm_shape(childcare)+\n  tm_dots()\n\n\n\n\n\ntmap_mode('plot')\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)\nIn practical use, we can use these results to effectively identify areas where new pedestrian roads can be built."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03.html",
    "title": "In-Class Exercise 03",
    "section": "",
    "text": "pacman::p_load(arrow,lubridate,tidyverse,tmap,sf)\n\n\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "href": "In-class_Ex/In-class_Ex03.html#importing-datasets-to-r-environment",
    "title": "In-Class Exercise 03",
    "section": "3.0 Importing Datasets to R Environment",
    "text": "3.0 Importing Datasets to R Environment\nIn this exercise, we will use the following datasets:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n3.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import the three geospatial data sets mentioned in previous section into R environment.\n\nchildcare_sf &lt;- st_read(\"../data/aspatial/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/khantminnaing/IS415-GAA/data/aspatial/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nmpsz_sf &lt;- st_read(dsn = \"../data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/khantminnaing/IS415-GAA/data/geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex03.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 03",
    "section": "4.0 Geospatial Data Wrangling",
    "text": "4.0 Geospatial Data Wrangling\n\n4.1. Assigning Stanadrd Coordinate Systems\n\nchildcare_sf &lt;- st_transform(childcare_sf, crs = 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, crs = 3414)\n\n\n\n4.2 Creating Coastal Outline\n\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\nplot(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#overview",
    "href": "In-class_Ex/In-class_Ex02.html#overview",
    "title": "In-Class Exercise 02",
    "section": "",
    "text": "In this exercise, we will explore how to process and wrangle Grab Posisi dataset."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "href": "In-class_Ex/In-class_Ex02.html#importing-packages",
    "title": "In-Class Exercise 02",
    "section": "2.0 Importing Packages",
    "text": "2.0 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\narrow for reading and writing Apache Parquet files\nlubridate for tackling with temporal data (dates and times)\ntidyverse for manipulating and wrangling data, as well as, implementing data science functions\ntmap for creating and visualizing thematic maps\nsf for handling geospatial data.\n\n\npacman::p_load(arrow,lubridate,tidyverse,tmap,sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "href": "In-class_Ex/In-class_Ex02.html#importing-datasets-into-r-environment",
    "title": "In-Class Exercise 02",
    "section": "3.0 Importing Datasets into R Environment",
    "text": "3.0 Importing Datasets into R Environment\n\n3.1 Datasets\nIn this exercise, we will use Grab-Posisi dataset, which is a comprehensive GPS trajectory dataset for car-hailing services in Southeast Asia.\n\nApart from the time and location of the object, GPS trajectories are also characterised by other parameters such as speed, the headed direction, the area and distance covered during its travel, and the travelled time. Thus, the trajectory patterns from users GPS data are a valuable source of information for a wide range of urban applications, such as solving transportation problems, traffic prediction, and developing reasonable urban planning.\n\n\n3.1 Importing Grab-Posisi Dataset\nEach trajectory in Grab-Posisi dataset is serialised in a file in Apache Parquet format.\n\nFirstly, we will use read_parquet function from arrow package\n\n\ndf &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00000.snappy.parquet')\ndf_1 &lt;- read_parquet('~/IS415-GAA/data/GrabPosisi/part-00001.snappy.parquet')\n\n\nNext, we will use head() function to quickly scan through the data columns and values.\n\n\nhead(df)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nFrom the result above, we can see that the dataset includes a total of 9 columns as follows:\n\n\n\nColumn Name\nData Type\nRemark\n\n\n\n\ntrj_id\nchr\nTrajectory ID\n\n\ndriving_mode\nchr\nMode of Driving\n\n\nosname\nchr\n\n\n\npingtimestamp\nint\nData Recording Timestamp\n\n\nrawlat\nnum\nLatitude Value (WGS-84)\n\n\nrawlng\nnum\nLongitude Value (WGS-84)\n\n\nspeed\nnum\nSpeed\n\n\nbearing\nint\nBearing\n\n\naccuracy\nnum\nAccuracy\n\n\n\nFrom the above table, it is seen that the pingtimestamp is recorded as int. We need to convert this data to proper datetime format to derive meaningful temporal insights of the data. To do so, we will use as_datetime() function from lubridate package.\n\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)\n\n\n\n3.2 Extracting Trip Starting Locations and Temporal Data Values\nAfter loading the Grab-Posisi dataset, we will extract features that we want to use for analysis. Firstly, we will extract trip starting locations for all trajectories in the dataset and save it into a new df called origin_df.\nAlso, we are interested to derive useful temporal data such as day of the week, hour, and yy-mm-dd. To do so, we will use the following functions from lubridate package, and add the newly derived values as new columns to origin_df.\n\nwday: allows us to get days component of a date-time\nhour: allows us to get hours component of a date-time\nmday: allows us to parse dates with year, month, and day components\n\n\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n3.3 Extracting Trip Ending Locations and Temporal Data Values\nSimilar to what we did in previous session, we are also interested to extract trip ending locations and associated temporal data into a new df called destination_df. We will use the same functions from previous session here.\n\ndestination_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number()==1) %&gt;% \n  mutate(weekday = wday(pingtimestamp,\n                       label=TRUE,\n                       abbr=TRUE),\n         starting_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\n\n\n\n\nReflection\n\n\n\narrange() function sort the timestamps in ascending order by default. Hence, for destination_df, we use arrange(desc()) argument to sort the timestamps in descending order\n\n\n\n\n3.4 Saving R Objects in RDS Format\nRDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow us to preserve the state of an object between R sessions. Saving R object as an RDS file in R can be useful for sharing our work with others, replicating our analysis, or simply storing our work for later use.\n\nwrite_rds(origin_df, \"../data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"../data/rds/destination_df.rds\")\n\n\n\n3.4 Importing RDS Objects\n\norigin_df &lt;- read_rds(\"../data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"../data/rds/destination_df.rds\")"
  }
]